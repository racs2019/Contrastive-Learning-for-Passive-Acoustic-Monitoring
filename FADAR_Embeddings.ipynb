{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be89e24-29f7-4a74-a604-341094b48474",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea68641-8577-456b-b86e-f2d181f6b8cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T18:28:51.865183Z",
     "iopub.status.busy": "2025-08-28T18:28:51.864739Z",
     "iopub.status.idle": "2025-08-28T18:29:04.337546Z",
     "shell.execute_reply": "2025-08-28T18:29:04.337011Z",
     "shell.execute_reply.started": "2025-08-28T18:28:51.865162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.22.1)\n",
      "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (4.23.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gammatone in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from gammatone) (1.26.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gammatone) (1.11.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from gammatone) (3.7.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->gammatone) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->gammatone) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2020.6.20)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (6.3.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from plotly) (2.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (23.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.6 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.7.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.66.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6->umap-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime onnxruntime-gpu\n",
    "!pip install gammatone\n",
    "!pip install librosa\n",
    "!pip install plotly\n",
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4affd8-dc4c-4598-8be0-ccb56423040b",
   "metadata": {},
   "source": [
    "Clustering + Report\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a111462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T18:29:39.714921Z",
     "iopub.status.busy": "2025-08-28T18:29:39.714586Z",
     "iopub.status.idle": "2025-08-28T18:29:39.737496Z",
     "shell.execute_reply": "2025-08-28T18:29:39.737089Z",
     "shell.execute_reply.started": "2025-08-28T18:29:39.714889Z"
    }
   },
   "outputs": [],
   "source": [
    "# onnx_cnn_latent_pipeline.py\n",
    "#!wget -O net_eff_with_latent.onnx \"https://www.dropbox.com/scl/fi/ur5n2co355tus6kg3qp1v/net_eff_with_latent.onnx?rlkey=ecfzipwdtbxpc2b79782hathl&st=aap83t7o&dl=1\"\n",
    "#!pip install onnxruntime onnxruntime-gpu\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import onnxruntime as ort\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import Iterable, Tuple, Dict, Any, Optional\n",
    "\n",
    "# CHANGED: safe UMAP import & plotting up front\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸŽ¯ Metrics\n",
    "# ----------------------------\n",
    "def hungarian_accuracy(y_true, y_pred) -> float:\n",
    "    D = int(max(y_pred.max(), y_true.max())) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(len(y_pred)):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return float(sum(w[i, j] for i, j in zip(row_ind, col_ind)) / len(y_pred))\n",
    "\n",
    "def _internal_indices(X: np.ndarray, labels: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute Silhouette, Daviesâ€“Bouldin, Calinskiâ€“Harabasz on X with given labels.\n",
    "       Returns NaN for invalid cases (e.g., 1 cluster).\"\"\"\n",
    "    out = {\n",
    "        \"silhouette\": float(\"nan\"),\n",
    "        \"davies_bouldin\": float(\"nan\"),\n",
    "        \"calinski_harabasz\": float(\"nan\"),\n",
    "    }\n",
    "    uniq = np.unique(labels)\n",
    "    if len(uniq) <= 1 or len(uniq) >= len(labels):\n",
    "        return out\n",
    "    try:\n",
    "        out[\"silhouette\"] = float(silhouette_score(X, labels))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        out[\"davies_bouldin\"] = float(davies_bouldin_score(X, labels))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        out[\"calinski_harabasz\"] = float(calinski_harabasz_score(X, labels))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸŽšï¸ Features (Log-Mel)\n",
    "# ----------------------------\n",
    "def extract_logmel(\n",
    "    file_path: str,\n",
    "    sr: int = 16000,\n",
    "    n_mels: int = 64,\n",
    "    target_shape: Tuple[int, int] = (224, 224),\n",
    "):\n",
    "    try:\n",
    "        audio = np.load(file_path).astype(np.float32)\n",
    "        maxabs = np.max(np.abs(audio))\n",
    "        if maxabs > 0:\n",
    "            audio = audio / maxabs\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=sr, n_fft=1024, hop_length=512, n_mels=n_mels\n",
    "        )\n",
    "        logmel = librosa.power_to_db(mel + 1e-8)\n",
    "\n",
    "        padded = np.zeros(target_shape, dtype=np.float32)\n",
    "        h, w = min(logmel.shape[0], target_shape[0]), min(logmel.shape[1], target_shape[1])\n",
    "        padded[:h, :w] = logmel[:h, :w]\n",
    "        return padded\n",
    "    except Exception as e:\n",
    "        print(f\"Failed processing {file_path}: {e}\", flush=True)\n",
    "        return None\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸ§  ONNX Inference â†’ Latents\n",
    "# ----------------------------\n",
    "def extract_cnn_latents(\n",
    "    X: np.ndarray,\n",
    "    model_path: str = \"net_eff_with_latent.onnx\",\n",
    "    output_name: str = \"new_fc\",\n",
    "    providers: Iterable[str] = (\"CUDAExecutionProvider\", \"CPUExecutionProvider\"),\n",
    ") -> np.ndarray:\n",
    "    session = ort.InferenceSession(model_path, providers=list(providers))\n",
    "    input_name = session.get_inputs()[0].name\n",
    "\n",
    "    latents = []\n",
    "    for idx, x in enumerate(X):\n",
    "        # Expect [H, W] -> [1, 3, H, W]\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = np.repeat(x[np.newaxis], 3, axis=1)\n",
    "        try:\n",
    "            out = session.run([output_name], {input_name: x.astype(np.float32)})\n",
    "            latents.append(out[0].squeeze())\n",
    "        except Exception as e:\n",
    "            print(f\"ONNX failed on index {idx}: {e}\", flush=True)\n",
    "\n",
    "    print(f\"âœ… Extracted {len(latents)} latent vectors\", flush=True)\n",
    "    return np.asarray(latents)\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸ”— Clustering (+ optional UMAP)\n",
    "# ----------------------------\n",
    "def run_cnn_clustering(\n",
    "    X_latent: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    kmeans_k: Optional[int] = None,        # NEW: choose k; if None, infer from y\n",
    "    do_umap: bool = True,\n",
    "    umap_components: int = 3,\n",
    "    umap_html_path: str = \"cnn_latent_umap.html\",\n",
    "    random_state: int = 42,\n",
    ") -> Dict[str, Any]:\n",
    "    n_clusters = int(kmeans_k) if kmeans_k is not None else len(np.unique(y))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=random_state)\n",
    "    preds = kmeans.fit_predict(X_latent)\n",
    "\n",
    "    # External metrics\n",
    "    ari = adjusted_rand_score(y, preds)\n",
    "    ami = adjusted_mutual_info_score(y, preds)\n",
    "    hacc = hungarian_accuracy(y, preds)\n",
    "\n",
    "    # Internal indices (on latent space)\n",
    "    extra = _internal_indices(X_latent, preds)\n",
    "\n",
    "    print(f\"\\nðŸ“Š CNN Latent Clustering (k={n_clusters})\", flush=True)\n",
    "    print(f\"ARI: {ari:.4f} | AMI: {ami:.4f} | H-Acc: {hacc:.4f}\", flush=True)\n",
    "    print(f\"Silhouette: {extra['silhouette']:.4f} | \"\n",
    "          f\"Daviesâ€“Bouldin: {extra['davies_bouldin']:.4f} | \"\n",
    "          f\"Calinskiâ€“Harabasz: {extra['calinski_harabasz']:.2f}\", flush=True)\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"ari\": float(ari),\n",
    "        \"ami\": float(ami),\n",
    "        \"hungarian_accuracy\": float(hacc),\n",
    "        \"silhouette\": float(extra[\"silhouette\"]),\n",
    "        \"davies_bouldin\": float(extra[\"davies_bouldin\"]),\n",
    "        \"calinski_harabasz\": float(extra[\"calinski_harabasz\"]),\n",
    "        \"kmeans_k\": int(n_clusters),\n",
    "    }\n",
    "\n",
    "    # Optional UMAP viz on the same latent space\n",
    "    if do_umap:\n",
    "        if umap_components not in (2, 3):\n",
    "            raise ValueError(\"umap_components must be 2 or 3\")\n",
    "        reducer = umap.UMAP(n_components=umap_components, random_state=random_state)\n",
    "        X_umap = reducer.fit_transform(X_latent)\n",
    "\n",
    "        if umap_components == 3:\n",
    "            fig = px.scatter_3d(\n",
    "                x=X_umap[:, 0], y=X_umap[:, 1], z=X_umap[:, 2],\n",
    "                color=y.astype(str),\n",
    "                title=(f\"CNN Latent Clustering (k={n_clusters}) | \"\n",
    "                       f\"ARI {ari:.4f}, AMI {ami:.4f}, H-Acc {hacc:.4f}\")\n",
    "            )\n",
    "        else:\n",
    "            fig = px.scatter(\n",
    "                x=X_umap[:, 0], y=X_umap[:, 1],\n",
    "                color=y.astype(str),\n",
    "                title=(f\"CNN Latent Clustering (k={n_clusters}) | \"\n",
    "                       f\"ARI {ari:.4f}, AMI {ami:.4f}\")\n",
    "            )\n",
    "\n",
    "        fig.write_html(umap_html_path)\n",
    "        print(f\"Saved UMAP visualization to '{umap_html_path}'\", flush=True)\n",
    "        results[\"umap_html_path\"] = umap_html_path\n",
    "\n",
    "    return results\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸš€ Runner (TEST-ONLY, 20%)\n",
    "# ----------------------------\n",
    "def run_onnx_pipeline(\n",
    "    dataset_path: str,\n",
    "    do_clustering: int = 1,\n",
    "    *,\n",
    "    model_path: str = \"net_eff_with_latent.onnx\",\n",
    "    output_name: str = \"new_fc\",\n",
    "    providers: Iterable[str] = (\"CUDAExecutionProvider\", \"CPUExecutionProvider\"),\n",
    "    # NOTE: enforced to 20% test set, as requested\n",
    "    random_state: int = 42,\n",
    "    sr: int = 16000,\n",
    "    n_mels: int = 64,\n",
    "    target_shape: Tuple[int, int] = (224, 224),\n",
    "    do_umap: bool = True,\n",
    "    umap_components: int = 3,\n",
    "    umap_html_path: str = \"cnn_latent_umap.html\",\n",
    "    kmeans_k: Optional[int] = None,  # NEW: pass k through to clustering\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    End-to-end (TEST-ONLY, 20%):\n",
    "      - Loads labels + .npy waveforms from `dataset_path`\n",
    "      - Splits out a fixed 20% TEST set (stratified)\n",
    "      - Extracts log-mel inputs for TEST\n",
    "      - Runs ONNX to get CNN latent vectors for TEST\n",
    "      - Optionally runs KMeans (+UMAP) on TEST latents and reports ARI/AMI/H-Acc + Silhouette/DB/CH\n",
    "    \"\"\"\n",
    "    label_path = os.path.join(dataset_path, \"labels.npy\")\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {dataset_path}\")\n",
    "    if not os.path.isfile(label_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    all_files = sorted(\n",
    "        [f for f in os.listdir(dataset_path) if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "        key=lambda x: int(os.path.splitext(x)[0])\n",
    "    )\n",
    "    labels = np.load(label_path)\n",
    "    assert len(labels) == len(all_files), \"Mismatch between files and labels!\"\n",
    "\n",
    "    full_paths = [os.path.join(dataset_path, f) for f in all_files]\n",
    "    indices = np.arange(len(full_paths))\n",
    "\n",
    "    # FIXED 20% TEST SPLIT (stratified)\n",
    "    _, test_idx = train_test_split(\n",
    "        indices, test_size=0.2, stratify=labels, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Build TEST inputs\n",
    "    X_test, y_test = [], []\n",
    "    for idx in test_idx:\n",
    "        logmel = extract_logmel(full_paths[idx], sr=sr, n_mels=n_mels, target_shape=target_shape)\n",
    "        if logmel is not None:\n",
    "            X_test.append(logmel)\n",
    "            y_test.append(labels[idx])\n",
    "\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "    print(f\"âœ… Loaded TEST set (20%): X={X_test.shape}, y={y_test.shape}\", flush=True)\n",
    "\n",
    "    # ONNX â†’ latents (TEST)\n",
    "    X_latent_test = extract_cnn_latents(\n",
    "        X_test, model_path=model_path, output_name=output_name, providers=providers\n",
    "    )\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"test_samples\": int(len(X_test)),\n",
    "        \"latents_shape\": tuple(X_latent_test.shape),\n",
    "        \"split\": \"test (20%)\",\n",
    "        \"model_path\": model_path,\n",
    "        \"output_name\": output_name,\n",
    "    }\n",
    "\n",
    "    # Optional clustering + UMAP on TEST latents\n",
    "    if do_clustering:\n",
    "        metrics = run_cnn_clustering(\n",
    "            X_latent=X_latent_test,\n",
    "            y=y_test,\n",
    "            kmeans_k=kmeans_k,             # NEW\n",
    "            do_umap=do_umap,\n",
    "            umap_components=umap_components,\n",
    "            umap_html_path=umap_html_path,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        results[\"metrics\"] = metrics\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7394ce39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T18:30:52.591481Z",
     "iopub.status.busy": "2025-08-28T18:30:52.590836Z",
     "iopub.status.idle": "2025-08-28T19:06:13.882738Z",
     "shell.execute_reply": "2025-08-28T19:06:13.882139Z",
     "shell.execute_reply.started": "2025-08-28T18:30:52.591461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded TEST set (20%): X=(82655, 224, 224), y=(82655,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning:\n",
      "\n",
      "Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted 82655 latent vectors\n",
      "\n",
      "ðŸ“Š CNN Latent Clustering (k=6)\n",
      "ARI: 0.0821 | AMI: 0.1021 | H-Acc: 0.3051\n",
      "Silhouette: 0.2338 | Daviesâ€“Bouldin: 1.2509 | Calinskiâ€“Harabasz: 28019.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved UMAP visualization to 'cnn_latent_umap.html'\n",
      "{'ari': 0.08212392486495881, 'ami': 0.10211889006053312, 'hungarian_accuracy': 0.30507531304821245, 'silhouette': 0.23382499814033508, 'davies_bouldin': 1.250871491520521, 'calinski_harabasz': 28019.17804328841, 'kmeans_k': 6, 'umap_html_path': 'cnn_latent_umap.html'}\n"
     ]
    }
   ],
   "source": [
    "res = run_onnx_pipeline(\n",
    "    dataset_path=\"/notebooks/dataset_preprocessed\",\n",
    "    do_clustering=1,\n",
    "    model_path=\"net_eff_with_latent.onnx\",\n",
    "    output_name=\"new_fc\",\n",
    "    kmeans_k=6,                    # set k explicitly (or None to infer)\n",
    "    do_umap=True,\n",
    "    umap_html_path=\"cnn_latent_umap.html\",\n",
    ")\n",
    "print(res.get(\"metrics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf2b2c-aba5-4e6b-a28d-ce5ffe75c079",
   "metadata": {},
   "source": [
    "Clustering + Report\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e6b460-e384-4975-86b8-f7e7dcc21a3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:09:25.564758Z",
     "iopub.status.busy": "2025-08-28T19:09:25.564501Z",
     "iopub.status.idle": "2025-08-28T19:09:25.607473Z",
     "shell.execute_reply": "2025-08-28T19:09:25.606873Z",
     "shell.execute_reply.started": "2025-08-28T19:09:25.564742Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ===== Report Add-On =====\n",
    "# =========================\n",
    "import math, base64, time, contextlib\n",
    "from io import BytesIO\n",
    "from collections import Counter, OrderedDict\n",
    "from typing import Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.signal import spectrogram, get_window\n",
    "import plotly.io as pio\n",
    "\n",
    "# ---------- File utilities & spectrogram helpers ----------\n",
    "def _list_audio_npy_files(folder):\n",
    "    files = []\n",
    "    for f in os.listdir(folder):\n",
    "        if f.endswith(\".npy\") and f != \"labels.npy\":\n",
    "            stem = os.path.splitext(f)[0]\n",
    "            try:\n",
    "                key = int(stem)\n",
    "            except ValueError:\n",
    "                key = stem\n",
    "            files.append((key, os.path.join(folder, f)))\n",
    "    files.sort(key=lambda t: t[0])\n",
    "    return [p for _, p in files]\n",
    "\n",
    "def _load_class_labels_if_any(folder, count):\n",
    "    lbl_path = os.path.join(folder, \"labels.npy\")\n",
    "    if os.path.isfile(lbl_path):\n",
    "        try:\n",
    "            arr = np.load(lbl_path, allow_pickle=True)\n",
    "            if len(arr) < count:\n",
    "                pad = np.array([\"Unknown\"] * (count - len(arr)), dtype=object)\n",
    "                arr = np.concatenate([arr, pad], axis=0)\n",
    "            elif len(arr) > count:\n",
    "                arr = arr[:count]\n",
    "            return arr\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.array([\"Unknown\"] * count, dtype=object)\n",
    "\n",
    "def _ali_spec(x, fs):\n",
    "    Lframe2 = 1000\n",
    "    po = 80\n",
    "    lov = int(np.ceil((po / 100) * Lframe2))\n",
    "    taper = get_window('hann', Lframe2)\n",
    "    Nfft = 2 ** (int(np.floor(np.log2(Lframe2))) + 2)\n",
    "    f, t, s = spectrogram(x, fs=fs, window=taper, noverlap=lov, nfft=Nfft, mode='complex')\n",
    "    as_ = np.abs(s)\n",
    "    as_max = np.max(as_) if np.isfinite(np.max(as_)) and np.max(as_) > 0 else 1.0\n",
    "    sdb = 10 * np.log10(100 * as_ / as_max + 1e-10)\n",
    "    min_inx = np.argmin(np.abs(f - 0))\n",
    "    max_inx = np.argmin(np.abs(f - 800))\n",
    "    return sdb[min_inx:max_inx+1, :], f[min_inx:max_inx+1], t\n",
    "\n",
    "def _spec_img_base64_from_wave(audio, fs=10000, title=\"Spectrogram\"):\n",
    "    spec, f_axis, t_axis = _ali_spec(audio, fs)\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.imshow(spec, aspect='auto', origin='lower',\n",
    "              extent=[t_axis[0], t_axis[-1], f_axis[0], f_axis[-1]], cmap='hsv')\n",
    "    ax.set_title(title); ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    fig.tight_layout()\n",
    "    buf = BytesIO(); fig.savefig(buf, format='png'); plt.close(fig); buf.seek(0)\n",
    "    return base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "\n",
    "def _spec_img_base64_from_matrix(S, title=\"Mel-Spectrogram\"):\n",
    "    # S assumed ~ (F, T) or (1, F, T)\n",
    "    if S.ndim == 3:\n",
    "        S = S.squeeze(0)\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.imshow(S, aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax.set_title(title); ax.set_xlabel(\"Frames\"); ax.set_ylabel(\"Mel bins\")\n",
    "    fig.tight_layout()\n",
    "    buf = BytesIO(); fig.savefig(buf, format='png'); plt.close(fig); buf.seek(0)\n",
    "    return base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "\n",
    "def _carousel_html(cluster_id, scope, base64_imgs):\n",
    "    cid = f\"carousel_{scope}_{cluster_id}\"\n",
    "    indicators = \"\".join(\n",
    "        f'<button type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide-to=\"{i}\" {\"class=active\" if i==0 else \"\"} aria-current=\"true\" aria-label=\"Slide {i+1}\"></button>'\n",
    "        for i in range(len(base64_imgs))\n",
    "    )\n",
    "    items = \"\".join(\n",
    "        f'<div class=\"carousel-item {\"active\" if i==0 else \"\"}\"><div class=\"d-flex justify-content-center\"><img class=\"d-block w-100\" src=\"data:image/png;base64,{img}\"></div></div>'\n",
    "        for i, img in enumerate(base64_imgs)\n",
    "    )\n",
    "    return f\"\"\"\n",
    "    <div id=\"{cid}\" class=\"carousel slide\" data-bs-interval=\"false\" data-bs-touch=\"false\">\n",
    "      <div class=\"carousel-indicators\">{indicators}</div>\n",
    "      <div class=\"carousel-inner\">{items}</div>\n",
    "      <button class=\"carousel-control-prev\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"prev\">\n",
    "        <span class=\"carousel-control-prev-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Previous</span>\n",
    "      </button>\n",
    "      <button class=\"carousel-control-next\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"next\">\n",
    "        <span class=\"carousel-control-next-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Next</span>\n",
    "      </button>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "# ---------- Clustering choices & per-cluster metrics ----------\n",
    "def _choose_clusterer(algorithm: str, embeddings: np.ndarray, n_clusters: int):\n",
    "    if algorithm == 'kmeans':\n",
    "        model = KMeans(n_clusters=n_clusters, n_init=10, random_state=42).fit(embeddings)\n",
    "        return model.labels_, None\n",
    "    elif algorithm == 'agglomerative':\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters).fit(embeddings)\n",
    "        return model.labels_, None\n",
    "    elif algorithm == 'gmm':\n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        gm = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=42).fit(embeddings)\n",
    "        return gm.predict(embeddings), gm.predict_proba(embeddings)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported clustering algorithm: {algorithm}\")\n",
    "\n",
    "def _per_cluster_stats(embeddings, cluster_labels, top_n=10, samples_per_cluster=10, cosine_eps=1e-8):\n",
    "    \"\"\"Returns list of dicts for top-N clusters by size with metrics & exemplar indices.\"\"\"\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)\n",
    "    cluster_labels = np.asarray(cluster_labels)\n",
    "    uniq, counts = np.unique(cluster_labels, return_counts=True)\n",
    "\n",
    "    # Silhouette per-sample (if valid), then mean per cluster\n",
    "    per_sample_sil = None\n",
    "    if np.all(counts >= 2) and len(uniq) > 1 and embeddings.shape[0] > len(uniq):\n",
    "        try:\n",
    "            per_sample_sil = silhouette_samples(embeddings, cluster_labels)\n",
    "        except Exception:\n",
    "            per_sample_sil = None\n",
    "\n",
    "    clusters = []\n",
    "    for c in uniq:\n",
    "        idxs = np.where(cluster_labels == c)[0]\n",
    "        Xi = embeddings[idxs]\n",
    "        size = len(idxs)\n",
    "        ctr = Xi.mean(axis=0, keepdims=True)\n",
    "        # Intra-cluster variance (mean L2^2 to centroid)\n",
    "        d2 = np.sum((Xi - ctr) ** 2, axis=1)\n",
    "        intra_var = float(np.mean(d2)) if size > 0 else 0.0\n",
    "        # Mean cosine similarity to centroid\n",
    "        Xi_n = Xi / (np.linalg.norm(Xi, axis=1, keepdims=True) + cosine_eps)\n",
    "        ctr_n = ctr / (np.linalg.norm(ctr, axis=1, keepdims=True) + cosine_eps)\n",
    "        mean_cos = float(np.mean((Xi_n @ ctr_n.T).squeeze())) if size > 0 else 1.0\n",
    "        # Per-cluster silhouette\n",
    "        sil = float(np.mean(per_sample_sil[idxs])) if per_sample_sil is not None else float(\"nan\")\n",
    "        # pick K nearest to centroid\n",
    "        order = np.argsort(d2)\n",
    "        exemplars = idxs[order[: min(samples_per_cluster, size)]]\n",
    "        clusters.append({\n",
    "            \"cluster_id\": int(c),\n",
    "            \"size\": int(size),\n",
    "            \"silhouette\": sil,\n",
    "            \"intra_variance\": intra_var,\n",
    "            \"mean_cosine_to_centroid\": mean_cos,\n",
    "            \"exemplar_indices\": exemplars.tolist(),\n",
    "        })\n",
    "\n",
    "    clusters_sorted = sorted(clusters, key=lambda z: z[\"size\"], reverse=True)[: top_n]\n",
    "    return clusters_sorted\n",
    "\n",
    "# ---------- Embedding extraction via your existing ONNX path ----------\n",
    "def _extract_onnx_embeddings_subset(\n",
    "    dataset_path: str,\n",
    "    *,\n",
    "    model_path: str,\n",
    "    output_name: str,\n",
    "    providers: Iterable[str],\n",
    "    subset_fraction: float = 0.02,\n",
    "    subset_seed: int = 42,\n",
    "    subset_strategy: str = \"random\",  # \"random\" | \"head\" | \"tail\"\n",
    "    sr: int = 16000,\n",
    "    n_mels: int = 64,\n",
    "    target_shape: Tuple[int, int] = (224, 224),\n",
    "):\n",
    "    \"\"\"Select a subset of files from dataset_path, build log-mels, run ONNX -> latents.\"\"\"\n",
    "    all_files = _list_audio_npy_files(dataset_path)\n",
    "    if len(all_files) == 0:\n",
    "        raise RuntimeError(f\"No .npy files found in {dataset_path}\")\n",
    "    n_sub = max(1, int(math.ceil(len(all_files) * subset_fraction)))\n",
    "    if subset_strategy == \"random\":\n",
    "        rng = np.random.RandomState(subset_seed)\n",
    "        chosen_idx = np.sort(rng.choice(len(all_files), size=n_sub, replace=False))\n",
    "    elif subset_strategy == \"head\":\n",
    "        chosen_idx = np.arange(0, n_sub)\n",
    "    elif subset_strategy == \"tail\":\n",
    "        chosen_idx = np.arange(len(all_files) - n_sub, len(all_files))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown subset_strategy: {subset_strategy}\")\n",
    "\n",
    "    chosen_files = [all_files[i] for i in chosen_idx]\n",
    "\n",
    "    # Build log-mels\n",
    "    X = []\n",
    "    valid_files = []\n",
    "    for fp in chosen_files:\n",
    "        logmel = extract_logmel(fp, sr=sr, n_mels=n_mels, target_shape=target_shape)\n",
    "        if logmel is not None:\n",
    "            X.append(logmel); valid_files.append(fp)\n",
    "    if not X:\n",
    "        raise RuntimeError(\"No valid samples after log-mel extraction.\")\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "\n",
    "    # ONNX -> latents\n",
    "    latents = extract_cnn_latents(\n",
    "        X, model_path=model_path, output_name=output_name, providers=providers\n",
    "    )\n",
    "    return latents, valid_files\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def evaluate_cluster_metrics(embeddings, idxs, location_labels, location_entropy_base=None):\n",
    "    X = embeddings[idxs]\n",
    "    if X.shape[0] == 0:\n",
    "        return {'variance': 0.0, 'mean_sim': 1.0, 'entropy': 0.0, 'quality': 0.0, 'novelty': 0.0}\n",
    "    center = np.mean(X, axis=0, keepdims=True)\n",
    "    variance = float(np.mean(np.sum((X - center) ** 2, axis=1)))\n",
    "\n",
    "    if len(X) > 1:\n",
    "        cos_sim = cosine_similarity(X)\n",
    "        iu = np.triu_indices_from(cos_sim, k=1)\n",
    "        mean_sim = float(np.mean(cos_sim[iu])) if iu[0].size > 0 else 1.0\n",
    "    else:\n",
    "        mean_sim = 1.0\n",
    "\n",
    "    loc_counts = Counter(location_labels[idxs])\n",
    "    loc_probs = np.array(list(loc_counts.values()), dtype=np.float32)\n",
    "    loc_probs /= max(loc_probs.sum(), 1e-8)\n",
    "    base = int(location_entropy_base or len(set(location_labels)))\n",
    "    loc_entropy = float(entropy(loc_probs, base=base)) if base > 1 else 0.0\n",
    "    max_ent = np.log2(base) if base > 1 else 1.0\n",
    "    entropy_score = 1.0 - (loc_entropy / max_ent) if base > 1 else 1.0\n",
    "    quality = float((mean_sim / (variance + 1e-8)) * entropy_score)\n",
    "    novelty = float((loc_entropy / max_ent) * variance) if base > 1 else 0.0\n",
    "\n",
    "    return {'variance': variance, 'mean_sim': mean_sim, 'entropy': loc_entropy,\n",
    "            'quality': quality, 'novelty': novelty}\n",
    "\n",
    "# ---------- Main analysis to HTML (mirrors your SimCLR script) ----------\n",
    "def analyze_onnx_unsupervised_to_html(\n",
    "    model_path: str,\n",
    "    dataset_paths: Sequence[str],\n",
    "    labels_list: Sequence[str],\n",
    "    *,\n",
    "    output_name: str = \"new_fc\",\n",
    "    providers: Iterable[str] = (\"CUDAExecutionProvider\", \"CPUExecutionProvider\"),\n",
    "    cluster_method: str = \"kmeans\",  # 'kmeans' | 'gmm' | 'agglomerative'\n",
    "    n_clusters: int = 60,\n",
    "    subset_fraction: float = 0.02,\n",
    "    subset_seed: int = 42,\n",
    "    subset_strategy: str = \"random\",\n",
    "    sr: int = 16000,\n",
    "    n_mels: int = 64,\n",
    "    target_shape: Tuple[int, int] = (224, 224),\n",
    "    top_n_clusters: int = 10,\n",
    "    samples_per_cluster: int = 10,\n",
    "):\n",
    "    # 1) Collect embeddings + metadata across locations\n",
    "    embeddings_all, loc_labels_all, class_labels_all, file_paths_all = [], [], [], []\n",
    "\n",
    "    for ds_path, loc_label in zip(dataset_paths, labels_list):\n",
    "        print(f\"[INFO] Extracting ONNX latents from {ds_path}\")\n",
    "        H, chosen_files = _extract_onnx_embeddings_subset(\n",
    "            ds_path,\n",
    "            model_path=model_path,\n",
    "            output_name=output_name,\n",
    "            providers=providers,\n",
    "            subset_fraction=subset_fraction,\n",
    "            subset_seed=subset_seed,\n",
    "            subset_strategy=subset_strategy,\n",
    "            sr=sr, n_mels=n_mels, target_shape=target_shape,\n",
    "        )\n",
    "        embeddings_all.append(H)\n",
    "        file_paths_all.extend(chosen_files)\n",
    "        loc_labels_all.extend([loc_label] * H.shape[0])\n",
    "\n",
    "        # class labels from labels.npy if available (aligned by filename stem)\n",
    "        full_files_sorted = _list_audio_npy_files(ds_path)\n",
    "        cls_full = _load_class_labels_if_any(ds_path, count=len(full_files_sorted))\n",
    "        name_to_label = {os.path.basename(p): cls_full[i] for i, p in enumerate(full_files_sorted)}\n",
    "        class_labels_all.extend([name_to_label.get(os.path.basename(p), \"Unknown\") for p in chosen_files])\n",
    "\n",
    "    embeddings = np.vstack(embeddings_all).astype(np.float32)\n",
    "    location_labels = np.array(loc_labels_all, dtype=object)\n",
    "    class_labels = np.array(class_labels_all, dtype=object)\n",
    "    original_paths = np.array(file_paths_all, dtype=object)\n",
    "    print(f\"[INFO] Total subset size: {embeddings.shape}\")\n",
    "\n",
    "    # 2) UMAP projection (3D)\n",
    "    print(f\"[INFO] Running UMAP on {embeddings.shape[0]} embeddings (dim={embeddings.shape[1]})\")\n",
    "    reducer = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=42)\n",
    "    t0 = time.perf_counter(); proj_3d = reducer.fit_transform(embeddings); t1 = time.perf_counter()\n",
    "    print(f\"[INFO] UMAP done in {t1 - t0:.2f}s\")\n",
    "\n",
    "    # 3) Clustering\n",
    "    print(f\"[INFO] Clustering with {cluster_method}, k={n_clusters}\")\n",
    "    t2 = time.perf_counter(); cluster_labels, _ = _choose_clusterer(cluster_method, embeddings, n_clusters); t3 = time.perf_counter()\n",
    "    print(f\"[INFO] Clustering done in {t3 - t2:.2f}s\")\n",
    "\n",
    "    # 4) Global metrics (safe)\n",
    "    uniq = np.unique(cluster_labels)\n",
    "    valid = (len(uniq) > 1) and (embeddings.shape[0] > len(uniq))\n",
    "    def _safe(fn, X, y):\n",
    "        try:\n",
    "            return float(fn(X, y)) if valid else float(\"nan\")\n",
    "        except Exception:\n",
    "            return float(\"nan\")\n",
    "    sil = _safe(silhouette_score, embeddings, cluster_labels)\n",
    "    dbi = _safe(davies_bouldin_score, embeddings, cluster_labels)\n",
    "    ch  = _safe(calinski_harabasz_score, embeddings, cluster_labels)\n",
    "    print(f\"[INFO] Silhouette={sil if sil==sil else 'nan'} | DBI={dbi if dbi==dbi else 'nan'} | CH={ch if ch==ch else 'nan'}\")\n",
    "\n",
    "    # 5) UMAP figure\n",
    "    title_txt = f\"{cluster_method.capitalize()} (Sil={sil:.3f} | DBI={dbi:.3f} | CH={ch:.1f})\"\n",
    "    umap_fig = px.scatter_3d(\n",
    "        x=proj_3d[:, 0], y=proj_3d[:, 1], z=proj_3d[:, 2],\n",
    "        color=[str(c) for c in cluster_labels],\n",
    "        symbol=location_labels,\n",
    "        hover_data={\"Cluster\": cluster_labels, \"Class\": class_labels},\n",
    "        title=title_txt, opacity=0.85, height=800\n",
    "    )\n",
    "    umap_html = pio.to_html(umap_fig, include_plotlyjs=\"cdn\", full_html=False)\n",
    "\n",
    "    # 6) Per-cluster details (top-N)\n",
    "    cluster_blocks = []\n",
    "    top_clusters = _per_cluster_stats(\n",
    "        embeddings, cluster_labels,\n",
    "        top_n=top_n_clusters, samples_per_cluster=samples_per_cluster\n",
    "    )\n",
    "\n",
    "    for c in top_clusters:\n",
    "        cid = c[\"cluster_id\"]\n",
    "        idxs = np.where(cluster_labels == cid)[0]\n",
    "        size = len(idxs)\n",
    "\n",
    "        # distributions\n",
    "        loc_counts = Counter(location_labels[idxs])\n",
    "        cls_counts = Counter(class_labels[idxs])\n",
    "\n",
    "        base_for_entropy = len(set(location_labels))\n",
    "        # inside the loop for each cluster `cid`:\n",
    "        metrics = evaluate_cluster_metrics(embeddings, idxs, location_labels,\n",
    "                                           location_entropy_base=base_for_entropy)\n",
    "\n",
    "        meta_html = \"<p><strong>Location Distribution:</strong></p><ul>\" + \"\".join(\n",
    "            f\"<li><b>{loc}</b>: {count} ({count/size:.1%})</li>\" for loc, count in Counter(location_labels[idxs]).items()\n",
    "        ) + \"</ul>\"\n",
    "\n",
    "        meta_html += \"<p><strong>Class Distribution:</strong></p><ul>\" + \"\".join(\n",
    "            f\"<li>{cls}: {count}</li>\" for cls, count in Counter(class_labels[idxs]).items()\n",
    "        ) + \"</ul>\"\n",
    "\n",
    "        # identical to SimCLR report (you can append your per-cluster silhouette line if you like)\n",
    "        meta_html += f\"\"\"\n",
    "        <p><strong>Cluster Metrics:</strong></p>\n",
    "        <ul>\n",
    "          <li>Size: {size}</li>\n",
    "          <li>Intra-Cluster Variance: {metrics['variance']:.4f}</li>\n",
    "          <li>Mean Cosine Similarity: {metrics['mean_sim']:.4f}</li>\n",
    "          <li>Location Entropy: {metrics['entropy']:.3f}</li>\n",
    "          <li>Composite Quality Score: {metrics['quality']:.4f}</li>\n",
    "          <li><strong>Novelty Score:</strong> {metrics['novelty']:.4f}</li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "\n",
    "        # exemplars: nearest to centroid (already chosen)\n",
    "        imgs64 = []\n",
    "        for i, ex_idx in enumerate(c[\"exemplar_indices\"]):\n",
    "            try:\n",
    "                x = np.load(original_paths[ex_idx], mmap_mode=\"r\")\n",
    "                if x.ndim == 1:\n",
    "                    title = f\"#{i+1} | {location_labels[ex_idx]} | Class {class_labels[ex_idx]}\"\n",
    "                    imgs64.append(_spec_img_base64_from_wave(x.astype(np.float32), title=title))\n",
    "                else:\n",
    "                    title = f\"#{i+1} | {location_labels[ex_idx]} | Class {class_labels[ex_idx]}\"\n",
    "                    imgs64.append(_spec_img_base64_from_matrix(x, title=title))\n",
    "            except Exception as e:\n",
    "                imgs64.append(_spec_img_base64_from_matrix(np.zeros((64,64)), title=f\"Error: {e}\"))\n",
    "\n",
    "        carousel_html = _carousel_html(cid, \"onnx\", imgs64)\n",
    "        block = f\"<div class='col-md-6 mb-4'><h4>Cluster {cid} (n={size})</h4>{meta_html}{carousel_html}</div>\"\n",
    "        cluster_blocks.append(block)\n",
    "\n",
    "    cluster_html = \"\"\n",
    "    for i in range(0, len(cluster_blocks), 2):\n",
    "        cluster_html += \"<div class='row'>\" + \"\".join(cluster_blocks[i:i+2]) + \"</div>\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    <div class='section'>\n",
    "      <h2>ONNX Latent Clustering Analysis (subset {int(subset_fraction*100)}%)</h2>\n",
    "      <div class=\"container\">\n",
    "        <div class=\"row justify-content-center mb-4\">\n",
    "          <div class=\"col-md-12 d-flex justify-content-center\">{umap_html}</div>\n",
    "        </div>\n",
    "      </div>\n",
    "      {cluster_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "# ---------- Top-level HTML report ----------\n",
    "def generate_full_onnx_report(\n",
    "    model_path: str,\n",
    "    dataset_paths: Sequence[str],\n",
    "    labels_list: Sequence[str],\n",
    "    *,\n",
    "    output_name: str = \"new_fc\",\n",
    "    providers: Iterable[str] = (\"CUDAExecutionProvider\", \"CPUExecutionProvider\"),\n",
    "    cluster_method: str = \"kmeans\",\n",
    "    n_clusters: int = 60,\n",
    "    subset_fraction: float = 0.02,\n",
    "    subset_seed: int = 42,\n",
    "    subset_strategy: str = \"random\",\n",
    "    sr: int = 16000,\n",
    "    n_mels: int = 64,\n",
    "    target_shape: Tuple[int, int] = (224, 224),\n",
    "    top_n_clusters: int = 10,\n",
    "    samples_per_cluster: int = 10,\n",
    "    out_prefix: str = \"onnx_unsup_report\",\n",
    ") -> str:\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\"><head>\n",
    "      <meta charset=\"UTF-8\">\n",
    "      <title>Unsupervised Clustering Report (ONNX Latents)</title>\n",
    "      <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "      <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "      <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "      <style>\n",
    "        body {{ font-family: Arial, sans-serif; padding: 20px; background-color: #f5f5f5; }}\n",
    "        h1 {{ color: #2c3e50; }}\n",
    "        h2, h4 {{ color: #34495e; }}\n",
    "        hr {{ border-top: 2px solid #bbb; margin-top: 40px; margin-bottom: 40px; }}\n",
    "        .section {{ margin-bottom: 60px; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "      </style>\n",
    "    </head><body>\n",
    "      <h1 class='mb-4'>Unsupervised Clustering Report (ONNX CNN Latents)</h1>\n",
    "      <p><strong>Locations:</strong> {', '.join(labels_list)}</p>\n",
    "      <p><strong>Subset:</strong> {int(subset_fraction*100)}% â€¢ Strategy: {subset_strategy} â€¢ Seed: {subset_seed}</p>\n",
    "      <hr>\n",
    "    \"\"\"\n",
    "    html += analyze_onnx_unsupervised_to_html(\n",
    "        model_path=model_path,\n",
    "        dataset_paths=list(dataset_paths),\n",
    "        labels_list=list(labels_list),\n",
    "        output_name=output_name,\n",
    "        providers=providers,\n",
    "        cluster_method=cluster_method,\n",
    "        n_clusters=int(n_clusters),\n",
    "        subset_fraction=subset_fraction,\n",
    "        subset_seed=subset_seed,\n",
    "        subset_strategy=subset_strategy,\n",
    "        sr=sr, n_mels=n_mels, target_shape=target_shape,\n",
    "        top_n_clusters=top_n_clusters,\n",
    "        samples_per_cluster=samples_per_cluster,\n",
    "    )\n",
    "    html += \"</body></html>\"\n",
    "    out_path = f\"{out_prefix}_{cluster_method}_subset{int(subset_fraction*100)}.html\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83819173-039f-43cb-b9b3-df6fdf3a7e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:09:37.768051Z",
     "iopub.status.busy": "2025-08-28T19:09:37.767391Z",
     "iopub.status.idle": "2025-08-28T19:46:48.282707Z",
     "shell.execute_reply": "2025-08-28T19:46:48.282058Z",
     "shell.execute_reply.started": "2025-08-28T19:09:37.768031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting ONNX latents from /notebooks/dataset_preprocessed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning:\n",
      "\n",
      "Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted 82655 latent vectors\n",
      "[INFO] Total subset size: (82655, 6)\n",
      "[INFO] Running UMAP on 82655 embeddings (dim=6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] UMAP done in 63.73s\n",
      "[INFO] Clustering with kmeans, k=60\n",
      "[INFO] Clustering done in 13.40s\n",
      "[INFO] Silhouette=0.14605672657489777 | DBI=1.3729855360610055 | CH=9004.832101821683\n",
      "âœ… Report saved to: FADAR_unsup_report_k60_kmeans_subset20.html\n"
     ]
    }
   ],
   "source": [
    "report_path = generate_full_onnx_report(\n",
    "    model_path=\"net_eff_with_latent.onnx\",\n",
    "    dataset_paths=[\"/notebooks/dataset_preprocessed\"],   # can pass multiple\n",
    "    labels_list=[\"PR_U1137\"],                             # one label per dataset\n",
    "    cluster_method=\"kmeans\",                              # 'kmeans' | 'gmm' | 'agglomerative'\n",
    "    n_clusters=60,\n",
    "    subset_fraction=0.2,                                 # 2% â€œsafe modeâ€ subset\n",
    "    subset_seed=42,\n",
    "    subset_strategy=\"random\",\n",
    "    top_n_clusters=30,\n",
    "    samples_per_cluster=10,\n",
    "    out_prefix=\"FADAR_unsup_report_k60\"\n",
    ")\n",
    "print(\"âœ… Report saved to:\", report_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
