{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:33:27.221167Z",
     "iopub.status.busy": "2025-10-09T15:33:27.220517Z",
     "iopub.status.idle": "2025-10-09T15:33:49.781757Z",
     "shell.execute_reply": "2025-10-09T15:33:49.781111Z",
     "shell.execute_reply.started": "2025-10-09T15:33:27.221144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.62.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.3)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.45.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2020.6.20)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading msgpack-1.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.62.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.6/242.6 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.45.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soxr, msgpack, llvmlite, audioread, soundfile, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.11.0 llvmlite-0.45.1 msgpack-1.1.2 numba-0.62.1 pooch-1.8.2 soundfile-0.13.1 soxr-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting umap-learn\n",
      "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.11.2)\n",
      "Collecting scikit-learn>=1.6 (from umap-learn)\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.62.1)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.66.1)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.45.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6->umap-learn) (3.2.0)\n",
      "Downloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, pynndescent, umap-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.0\n",
      "    Uninstalling scikit-learn-1.3.0:\n",
      "      Successfully uninstalled scikit-learn-1.3.0\n",
      "Successfully installed pynndescent-0.5.13 scikit-learn-1.7.2 umap-learn-0.5.9.post2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting plotly\n",
      "  Downloading plotly-6.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (23.2)\n",
      "Downloading plotly-6.3.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.7.0-py3-none-any.whl (412 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.8/412.8 kB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-2.7.0 plotly-6.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting hdbscan\n",
      "  Downloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20->hdbscan) (3.2.0)\n",
      "Downloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hdbscan\n",
      "Successfully installed hdbscan-0.8.40\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n",
    "!pip install umap-learn\n",
    "!pip install plotly\n",
    "!pip install hdbscan\n",
    "!pip install --upgrade pip\n",
    "!pip install -U soundfile datasets[audio] librosa\n",
    "# optional but helpful in many audio setups:\n",
    "!pip install -U torchaudio audioread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimCLR trainer \n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:05.677294Z",
     "iopub.status.busy": "2025-10-09T15:34:05.677035Z",
     "iopub.status.idle": "2025-10-09T15:34:09.427355Z",
     "shell.execute_reply": "2025-10-09T15:34:09.426846Z",
     "shell.execute_reply.started": "2025-10-09T15:34:05.677274Z"
    }
   },
   "outputs": [],
   "source": [
    "# === SimCLR (reef acoustics) with Feature Bank + SimSiam + VICReg ============================\n",
    "# NaN-proof + faster:\n",
    "#  - Contrastive logits never use -inf; use large negatives and clamp\n",
    "#  - Explicit sanitization (nan/inf -> 0) on all embeddings\n",
    "#  - Loss computed in fp32 (AMP only for forwards)\n",
    "#  - VICReg made numerically safer\n",
    "#  - SimSiam & VICReg warmup ramp (first 5 epochs)\n",
    "#  - bank_size=8192 (per paper), bank_sample default 2048/4096 for speed\n",
    "#  - Cached MelSpectrograms, channels_last, cuDNN autotune, optional torch.compile\n",
    "# ==============================================================================================\n",
    "\n",
    "import os, math, random, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torchaudio as ta\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "# ----------------------------\n",
    "# Speed/Determinism toggles\n",
    "# ----------------------------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "\n",
    "# ----------------------------\n",
    "# Backbone: ResNet18 (1-channel)\n",
    "# ----------------------------\n",
    "try:\n",
    "    from torchvision.models import resnet18\n",
    "    def _resnet18_1ch():\n",
    "        m = resnet18(weights=None)\n",
    "        m.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        m.fc = nn.Identity()\n",
    "        return m\n",
    "except Exception:\n",
    "    from torchvision.models import resnet18 as _resnet18_old\n",
    "    def _resnet18_1ch():\n",
    "        m = _resnet18_old(pretrained=False)\n",
    "        m.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        m.fc = nn.Identity()\n",
    "        return m\n",
    "\n",
    "# ----------------------------\n",
    "# Encoder + Projector + Predictor (for SimSiam)\n",
    "# ----------------------------\n",
    "class EncoderProj(nn.Module):\n",
    "    def __init__(self, proj_dim=128, use_predictor=True):\n",
    "        super().__init__()\n",
    "        self.backbone = _resnet18_1ch()\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, proj_dim)\n",
    "        )\n",
    "        self.predictor = None\n",
    "        if use_predictor:\n",
    "            self.predictor = nn.Sequential(\n",
    "                nn.Linear(proj_dim, 256), nn.BatchNorm1d(256), nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, proj_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, return_backbone=False, for_predictor=False):\n",
    "        h = self.backbone(x)              # (B, 512)\n",
    "        z = F.normalize(self.projector(h), dim=1)\n",
    "        if for_predictor and (self.predictor is not None):\n",
    "            p = self.predictor(z)\n",
    "            p = F.normalize(p, dim=1)\n",
    "            if return_backbone:\n",
    "                return F.normalize(h, dim=1), z, p\n",
    "            return z, p\n",
    "        if return_backbone:\n",
    "            return F.normalize(h, dim=1), z\n",
    "        return z\n",
    "\n",
    "# ----------------------------\n",
    "# EMA Teacher\n",
    "# ----------------------------\n",
    "class EMATeacher(nn.Module):\n",
    "    def __init__(self, online: EncoderProj, m=0.99, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.teacher = EncoderProj(proj_dim=proj_dim, use_predictor=False)\n",
    "        self.m = m\n",
    "        self._init_from(online)\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.teacher.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _init_from(self, online):\n",
    "        state = online.state_dict()\n",
    "        state = {k: v for k, v in state.items() if not k.startswith(\"predictor.\")}\n",
    "        self.teacher.load_state_dict(state, strict=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, online, m=None):\n",
    "        m = float(self.m if m is None else m)\n",
    "        for p_t, p_o in zip(self.teacher.parameters(), online.parameters()):\n",
    "            p_t.data.mul_(m).add_(p_o.data, alpha=(1.0 - m))\n",
    "        for b_t, b_o in zip(self.teacher.buffers(), online.buffers()):\n",
    "            if b_t.dtype.is_floating_point:\n",
    "                b_t.data.mul_(m).add_(b_o.data, alpha=(1.0 - m))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, return_backbone=False):\n",
    "        return self.teacher(x, return_backbone=return_backbone)\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def _sanitize_(*tensors):\n",
    "    \"\"\"Replace NaN/Inf with 0.0 on every tensor; returns sanitized copies.\"\"\"\n",
    "    out = []\n",
    "    for t in tensors:\n",
    "        if t is None:\n",
    "            out.append(None)\n",
    "        else:\n",
    "            out.append(torch.nan_to_num(t, nan=0.0, posinf=0.0, neginf=0.0))\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# FIFO Feature Bank (teacher embeddings, detached)\n",
    "# ----------------------------\n",
    "class FeatureBank:\n",
    "    def __init__(self, dim, size=8192, device=None):\n",
    "        device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bank = torch.zeros(size, dim, device=device)\n",
    "        self.ptr = 0\n",
    "        self.size = size\n",
    "        self._filled = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def enqueue(self, feats):  # feats: (N, dim), detached\n",
    "        if feats is None or feats.numel() == 0:\n",
    "            return\n",
    "        feats = F.normalize(feats, dim=1)\n",
    "        feats = torch.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        n = feats.size(0)\n",
    "        end = self.ptr + n\n",
    "        if end <= self.size:\n",
    "            self.bank[self.ptr:end] = feats\n",
    "        else:\n",
    "            first = self.size - self.ptr\n",
    "            self.bank[self.ptr:] = feats[:first]\n",
    "            self.bank[:end % self.size] = feats[first:]\n",
    "        self.ptr = (self.ptr + n) % self.size\n",
    "        self._filled = min(self.size, self._filled + n)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get(self):\n",
    "        if self._filled == 0:\n",
    "            return self.bank[:0]\n",
    "        return self.bank[:self._filled].detach()\n",
    "\n",
    "# ----------------------------\n",
    "# Multi-Positive NT-Xent (weighted by teacher) + Memory Bank (NaN-proof)\n",
    "# ----------------------------\n",
    "class MultiPosNTXentSoft(nn.Module):\n",
    "    def __init__(self, temperature=0.1, pos_topk=5, pos_thr=0.7):\n",
    "        super().__init__()\n",
    "        self.tau = float(temperature)\n",
    "        self.pos_topk = int(pos_topk)\n",
    "        self.pos_thr = float(pos_thr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _weights_from_teacher(self, t1, t2):\n",
    "        B = t1.size(0)\n",
    "        T = F.normalize(torch.cat([t1, t2], dim=0), dim=1)\n",
    "        T = torch.nan_to_num(T, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        S = T @ T.T\n",
    "        eye = torch.eye(2 * B, device=T.device, dtype=torch.bool)\n",
    "        S = S.masked_fill(eye, -1e9)  # large negative, not -inf\n",
    "\n",
    "        W = torch.zeros_like(S)\n",
    "        idx = torch.arange(B, device=T.device)\n",
    "        W[idx, idx + B] = 1.0\n",
    "        W[idx + B, idx] = 1.0\n",
    "\n",
    "        S_thr = torch.where(S >= self.pos_thr, S, torch.full_like(S, -1e9))\n",
    "        k = min(self.pos_topk, 2 * B - 2)\n",
    "        if k > 0:\n",
    "            vals, inds = torch.topk(S_thr, k=k, dim=1)\n",
    "            rows = torch.arange(2 * B, device=T.device).unsqueeze(1).expand_as(inds)\n",
    "            keep = torch.isfinite(vals)\n",
    "            weights = torch.clamp((vals + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "            W[rows[keep], inds[keep]] = weights[keep]\n",
    "\n",
    "        W = torch.where(eye, torch.zeros_like(W), W)\n",
    "        row_sum = W.sum(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "        W = W / row_sum\n",
    "        return W\n",
    "\n",
    "    def forward_with_bank(self, z1, z2, t1, t2, bank_feats):\n",
    "        B = z1.size(0)\n",
    "        Q = F.normalize(torch.cat([z1, z2], dim=0), dim=1).float()\n",
    "        Q = torch.nan_to_num(Q, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        cur_keys = F.normalize(torch.cat([z1.detach(), z2.detach()], dim=0), dim=1)\n",
    "        cur_keys = torch.nan_to_num(cur_keys, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if bank_feats is not None and bank_feats.numel() > 0:\n",
    "            bank_feats = torch.nan_to_num(bank_feats, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            K = torch.cat([cur_keys, bank_feats], dim=0)\n",
    "        else:\n",
    "            K = cur_keys\n",
    "\n",
    "        logits = (Q @ K.T) / self.tau\n",
    "        eye = torch.eye(2 * B, device=Q.device, dtype=torch.bool)\n",
    "        mask = torch.zeros_like(logits, dtype=torch.bool)\n",
    "        mask[:, :2 * B] = eye\n",
    "        logits = logits.masked_fill(mask, -1e9)    # never -inf\n",
    "        logits = logits.clamp(min=-30.0, max=30.0) # keep exp stable\n",
    "\n",
    "        log_denom = torch.logsumexp(logits, dim=1, keepdim=True)\n",
    "        log_probs = logits - log_denom\n",
    "\n",
    "        with torch.no_grad():\n",
    "            W = self._weights_from_teacher(t1, t2)\n",
    "        if K.size(0) > 2 * B:\n",
    "            pad_zeros = torch.zeros(Q.size(0), K.size(0) - 2 * B, device=Q.device, dtype=W.dtype)\n",
    "            W_full = torch.cat([W, pad_zeros], dim=1)\n",
    "        else:\n",
    "            W_full = W\n",
    "\n",
    "        loss = -(W_full * log_probs).sum(dim=1).mean()\n",
    "        if not torch.isfinite(loss):\n",
    "            loss = torch.nan_to_num(loss, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return loss\n",
    "\n",
    "# ----------------------------\n",
    "# SimSiam & VICReg (safer)\n",
    "# ----------------------------\n",
    "def simsiam_loss(p, z_target):\n",
    "    p = torch.nan_to_num(F.normalize(p, dim=1), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    zt = torch.nan_to_num(F.normalize(z_target, dim=1), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    out = - (p * zt).sum(dim=1).mean()\n",
    "    return torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def vicreg_loss(z_a, z_b, sim_coeff=25.0, var_coeff=25.0, cov_coeff=1.0, eps=1e-4, gamma=1.0):\n",
    "    z_a = torch.nan_to_num(z_a, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    z_b = torch.nan_to_num(z_b, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    inv = F.mse_loss(z_a, z_b)\n",
    "\n",
    "    def _var(z):\n",
    "        std = torch.sqrt(z.var(dim=0, unbiased=False).clamp_min(0.0) + eps)\n",
    "        return torch.mean(F.relu(gamma - std))\n",
    "\n",
    "    def _cov(z):\n",
    "        N, D = z.size()\n",
    "        zc = z - z.mean(dim=0, keepdim=True)\n",
    "        cov = (zc.T @ zc) / max(1, N - 1)\n",
    "        off_diag = cov - torch.diag(torch.diag(cov))\n",
    "        return (off_diag ** 2).sum() / max(1, D)\n",
    "\n",
    "    z_a = z_a - z_a.mean(dim=0, keepdim=True)\n",
    "    z_b = z_b - z_b.mean(dim=0, keepdim=True)\n",
    "\n",
    "    var = _var(z_a) + _var(z_b)\n",
    "    cov = _cov(z_a) + _cov(z_b)\n",
    "    out = sim_coeff * inv + var_coeff * var + cov_coeff * cov\n",
    "    return torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ----------------------------\n",
    "# Augmentations (spectral notch + others)\n",
    "# ----------------------------\n",
    "class SpectroAug(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_mask_param=32,\n",
    "        freq_mask_param=12,\n",
    "        p_time_mask=0.8,\n",
    "        p_freq_mask=0.8,\n",
    "        p_notch=0.3, notch_width=6,\n",
    "        p_shift=0.8,\n",
    "        p_noise=0.5,\n",
    "        max_shift_frac=0.12,\n",
    "        noise_std=0.01,\n",
    "        p_time_crop=0.5,\n",
    "        crop_frac_range=(0.7, 1.0),\n",
    "        keep_length=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tm = ta.transforms.TimeMasking(time_mask_param=time_mask_param)\n",
    "        self.fm = ta.transforms.FrequencyMasking(freq_mask_param=freq_mask_param)\n",
    "        self.p_time_mask = p_time_mask\n",
    "        self.p_freq_mask = p_freq_mask\n",
    "        self.p_notch = p_notch\n",
    "        self.notch_width = notch_width\n",
    "        self.p_shift = p_shift\n",
    "        self.p_noise = p_noise\n",
    "        self.max_shift_frac = max_shift_frac\n",
    "        self.noise_std = noise_std\n",
    "        self.p_time_crop = p_time_crop\n",
    "        self.crop_lo, self.crop_hi = crop_frac_range\n",
    "        self.keep_length = keep_length\n",
    "\n",
    "    def forward(self, x):  # x: (1, F, T0)\n",
    "        T0 = x.size(2)\n",
    "\n",
    "        if torch.rand(1) < self.p_time_crop and T0 > 16:\n",
    "            frac = float(torch.empty(1).uniform_(self.crop_lo, self.crop_hi))\n",
    "            new_T = max(16, int(T0 * frac))\n",
    "            if new_T < T0:\n",
    "                start = int(torch.randint(0, T0 - new_T + 1, (1,)))\n",
    "                x = x[:, :, start:start + new_T]\n",
    "\n",
    "        if torch.rand(1) < self.p_time_mask: x = self.tm(x)\n",
    "        if torch.rand(1) < self.p_freq_mask: x = self.fm(x)\n",
    "\n",
    "        if torch.rand(1) < self.p_notch:\n",
    "            Fdim = x.size(1)\n",
    "            width = min(self.notch_width, Fdim)\n",
    "            start = int(torch.randint(0, max(1, Fdim - width + 1), (1,)))\n",
    "            x[:, start:start+width, :] = 0.0\n",
    "\n",
    "        if torch.rand(1) < self.p_shift:\n",
    "            T_dim2 = x.size(2)\n",
    "            max_shift = max(1, int(self.max_shift_frac * T_dim2))\n",
    "            shift = int(torch.randint(-max_shift, max_shift + 1, (1,)))\n",
    "            x = torch.roll(x, shifts=shift, dims=2)\n",
    "\n",
    "        if torch.rand(1) < self.p_noise:\n",
    "            x = x + self.noise_std * torch.randn_like(x)\n",
    "\n",
    "        if self.keep_length and x.size(2) != T0:\n",
    "            pad = T0 - x.size(2)\n",
    "            if pad > 0: x = F.pad(x, (0, pad), mode='constant', value=0.0)\n",
    "            else:       x = x[:, :, :T0]\n",
    "        return x.contiguous()\n",
    "\n",
    "# ----------------------------\n",
    "# Multi-Resolution + Local/Global Multi-Crop Dataset (with cached mels)\n",
    "# ----------------------------\n",
    "class MultiResCropDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns list of views per sample: [G1, G2, L1, L2, ...], each (1, F, T).\n",
    "    Caches MelSpectrogram ops per (n_mels, n_fft, hop) for speed.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_dir: str,\n",
    "        sr: int = 10000,\n",
    "        mel_choices: Tuple[Tuple[int,int,int], ...] = ((64,1024,512),(128,1024,256),(128,2048,512)),\n",
    "        n_global: int = 2, global_T: int = 256,\n",
    "        n_local: int = 2,  local_T: int = 96,\n",
    "        event_jitter_T: int = 24,\n",
    "        per_sample_norm: bool = True,\n",
    "        augment_fn: Optional[nn.Module] = None,\n",
    "        resize_to: Optional[Tuple[int,int]] = (128, 256),\n",
    "    ):\n",
    "        self.audio_paths = sorted(\n",
    "            [os.path.join(audio_dir, f) for f in os.listdir(audio_dir)\n",
    "             if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "            key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "        )\n",
    "        self.sr = sr\n",
    "        self.mel_choices = list(mel_choices)\n",
    "        self.n_global, self.global_T = n_global, global_T\n",
    "        self.n_local,  self.local_T  = n_local,  local_T\n",
    "        self.event_jitter_T = event_jitter_T\n",
    "        self.per_sample_norm = per_sample_norm\n",
    "        self.augment_fn = augment_fn\n",
    "        self.resize_to = resize_to\n",
    "\n",
    "        # cache Mel ops\n",
    "        self._mel_cache: Dict[Tuple[int,int,int], ta.transforms.MelSpectrogram] = {}\n",
    "        for n_mels, n_fft, hop in set(mel_choices + ((64,1024,512),)):\n",
    "            self._mel_cache[(n_mels, n_fft, hop)] = ta.transforms.MelSpectrogram(\n",
    "                sample_rate=self.sr, n_fft=n_fft, hop_length=hop, n_mels=n_mels\n",
    "            )\n",
    "\n",
    "    def __len__(self): return len(self.audio_paths)\n",
    "\n",
    "    def _wave_to_logmel(self, y, n_mels, n_fft, hop):\n",
    "        y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "        mx = float(np.max(np.abs(y)));  mx = 1.0 if (not np.isfinite(mx) or mx < 1e-8) else mx\n",
    "        y = (y / mx).astype(np.float32)\n",
    "        y_t = torch.from_numpy(y).unsqueeze(0)  # (1,T)\n",
    "        mel = self._mel_cache[(n_mels, n_fft, hop)](y_t)\n",
    "        mel = torch.nan_to_num(mel, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        logmel = torch.log(torch.clamp(mel, min=1e-5))\n",
    "        logmel = torch.nan_to_num(logmel, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        if self.per_sample_norm:\n",
    "            m, s = logmel.mean(), logmel.std()\n",
    "            if (not torch.isfinite(s)) or (s < 1e-6): s = torch.tensor(1e-6, device=logmel.device)\n",
    "            logmel = (logmel - m) / s\n",
    "        return logmel  # (1, F, T)\n",
    "\n",
    "    def _energy_center(self, logmel):\n",
    "        energy = logmel.mean(dim=1).squeeze(0)  # (T,)\n",
    "        return int(torch.argmax(energy).item())\n",
    "\n",
    "    def _crop_T(self, x: torch.Tensor, T_win: int, center: Optional[int], jitter_T: int) -> torch.Tensor:\n",
    "        T = x.size(2)\n",
    "        if T <= T_win:\n",
    "            return x\n",
    "        if center is None:\n",
    "            start = int(torch.randint(0, T - T_win + 1, (1,)))\n",
    "        else:\n",
    "            start = max(0, min(center - T_win // 2, T - T_win))\n",
    "            j = int(torch.randint(-jitter_T, jitter_T + 1, (1,)))\n",
    "            start = max(0, min(start + j, T - T_win))\n",
    "        return x[:, :, start:start + T_win]\n",
    "\n",
    "    def _maybe_resize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.resize_to is None:\n",
    "            return x\n",
    "        Ft, Tt = self.resize_to\n",
    "        x4 = x.unsqueeze(0)  # (1,1,F,T)\n",
    "        x4 = F.interpolate(x4, size=(Ft, Tt), mode='bilinear', align_corners=False)\n",
    "        return x4.squeeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = np.load(self.audio_paths[idx]).astype(np.float32)\n",
    "\n",
    "        base = self._wave_to_logmel(y, n_mels=64, n_fft=1024, hop=512)\n",
    "        center = self._energy_center(base)\n",
    "\n",
    "        views = []\n",
    "        for _ in range(self.n_global):\n",
    "            n_mels, n_fft, hop = random.choice(self.mel_choices)\n",
    "            lm = self._wave_to_logmel(y, n_mels, n_fft, hop)\n",
    "            crop = self._crop_T(lm, self.global_T, center=center, jitter_T=self.event_jitter_T)\n",
    "            if self.augment_fn is not None: crop = self.augment_fn(crop)\n",
    "            crop = self._maybe_resize(crop)\n",
    "            views.append(crop)\n",
    "\n",
    "        for _ in range(self.n_local):\n",
    "            n_mels, n_fft, hop = random.choice(self.mel_choices)\n",
    "            lm = self._wave_to_logmel(y, n_mels, n_fft, hop)\n",
    "            crop = self._crop_T(lm, self.local_T, center=center, jitter_T=self.event_jitter_T)\n",
    "            if self.augment_fn is not None: crop = self.augment_fn(crop)\n",
    "            crop = self._maybe_resize(crop)\n",
    "            views.append(crop)\n",
    "\n",
    "        return views  # list[(1,Ft,Tt), ...]\n",
    "\n",
    "# ----------------------------\n",
    "# Collate: multi-crop -> list of (B,1,F,T)\n",
    "# ----------------------------\n",
    "def multicrop_collate(batch):\n",
    "    n_views = len(batch[0])\n",
    "    out = []\n",
    "    for v in range(n_views):\n",
    "        V = torch.stack([sample[v] for sample in batch], dim=0)\n",
    "        out.append(V)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# kNN-like sanity (multi-crop loaders)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def knn_sim_eval_multicrop(encoder, loader, device, use_backbone=True, K=20):\n",
    "    encoder.eval()\n",
    "    feats = []\n",
    "    for views in loader:\n",
    "        x = views[0].to(device, non_blocking=True)\n",
    "        h, z = encoder(x, return_backbone=True)\n",
    "        feats.append(h if use_backbone else z)\n",
    "    Fmat = F.normalize(torch.cat(feats, dim=0), dim=1)\n",
    "    sim = Fmat @ Fmat.T\n",
    "    sim.fill_diagonal_(-1)\n",
    "    knn_vals, _ = torch.topk(sim, k=min(K, sim.size(1)-1), dim=1)\n",
    "    return float(knn_vals.mean().item())\n",
    "\n",
    "# ----------------------------\n",
    "# Schedulers\n",
    "# ----------------------------\n",
    "def warmup_cosine_lambda(step, warmup_steps, total_steps):\n",
    "    if step < warmup_steps: return (step + 1) / max(1, warmup_steps)\n",
    "    t = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "    return 0.5 * (1 + math.cos(math.pi * t))\n",
    "\n",
    "def ema_momentum_at(step, total_steps, m_base=0.99, m_final=0.9995):\n",
    "    cos_t = 0.5 * (1 + math.cos(math.pi * step / max(1, total_steps)))\n",
    "    return m_final - (m_final - m_base) * cos_t\n",
    "\n",
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "def train_simclr_multipos(\n",
    "    audio_dir,\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    temperature=0.1,              # hotter by default\n",
    "    accum_steps=2,\n",
    "    ema_m=0.99,\n",
    "    pos_topk=5,\n",
    "    pos_thr=0.7,\n",
    "    save_path=\"simclr_multipos_latest.pth\",\n",
    "    warmup_epochs=10,\n",
    "    num_workers=4,\n",
    "    compile_model=False,\n",
    "    clip_grad_norm=1.0,\n",
    "    seed=42,\n",
    "    resume_path=None,\n",
    "    # dataset knobs\n",
    "    mel_choices=((64,1024,512),(128,1024,256),(128,2048,512)),\n",
    "    n_global=2, global_T=256,\n",
    "    n_local=2,  local_T=96,\n",
    "    resize_to=(128,256),\n",
    "    # bank knobs\n",
    "    bank_size=8192,\n",
    "    bank_sample=2048,             # 2048–4096 is good; lower = faster\n",
    "    # loss weights\n",
    "    alpha_contrast=1.0,\n",
    "    beta_siam=0.1,\n",
    "    gamma_vicreg=0.1,\n",
    "    vic_sim=25.0, vic_var=25.0, vic_cov=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    SimCLR with:\n",
    "      - EMA teacher + soft multi-positive NT-Xent (two global views),\n",
    "      - FIFO Feature Bank (teacher embeddings) with per-step sampling,\n",
    "      - SimSiam invariance,\n",
    "      - VICReg regularization.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ------- data -------\n",
    "    aug = SpectroAug(keep_length=True)\n",
    "\n",
    "    full_train_ds = MultiResCropDataset(\n",
    "        audio_dir,\n",
    "        mel_choices=mel_choices,\n",
    "        n_global=n_global, global_T=global_T,\n",
    "        n_local=n_local,   local_T=local_T,\n",
    "        per_sample_norm=True,\n",
    "        augment_fn=aug,\n",
    "        resize_to=resize_to,\n",
    "    )\n",
    "\n",
    "    full_val_ds = MultiResCropDataset(\n",
    "        audio_dir,\n",
    "        mel_choices=mel_choices,\n",
    "        n_global=1, global_T=global_T,\n",
    "        n_local=0,  local_T=local_T,\n",
    "        per_sample_norm=True,\n",
    "        augment_fn=aug,  # set None for absolutely clean val\n",
    "        resize_to=resize_to,\n",
    "    )\n",
    "\n",
    "    n_total = len(full_train_ds)\n",
    "    train_size = int(0.9 * n_total)\n",
    "    val_size   = n_total - train_size\n",
    "    idx_all = list(range(n_total))\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    train_subset, val_subset = random_split(idx_all, [train_size, val_size], generator=g)\n",
    "    train_ds = Subset(full_train_ds, train_subset.indices)\n",
    "    val_ds   = Subset(full_val_ds,   val_subset.indices)\n",
    "\n",
    "    pin = (device.type == \"cuda\")\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, drop_last=True, pin_memory=pin,\n",
    "        collate_fn=multicrop_collate, persistent_workers=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=max(1, num_workers // 2), drop_last=False, pin_memory=pin,\n",
    "        collate_fn=multicrop_collate, persistent_workers=False\n",
    "    )\n",
    "\n",
    "    # ------- models -------\n",
    "    online = EncoderProj(proj_dim=128, use_predictor=True).to(device).to(memory_format=torch.channels_last)\n",
    "    teacher = EMATeacher(online, m=ema_m, proj_dim=128).to(device).to(memory_format=torch.channels_last)\n",
    "    if compile_model and hasattr(torch, \"compile\"):\n",
    "        online = torch.compile(online)\n",
    "        teacher.teacher = torch.compile(teacher.teacher)\n",
    "\n",
    "    # ------- objective & optimizer -------\n",
    "    ntxent = MultiPosNTXentSoft(temperature=temperature, pos_topk=pos_topk, pos_thr=pos_thr)\n",
    "    optimizer = AdamW(online.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    steps_per_epoch = max(1, math.ceil(len(train_loader) / max(1, accum_steps)))\n",
    "    total_steps = max(1, epochs * steps_per_epoch)\n",
    "    warmup_steps = max(1, warmup_epochs * steps_per_epoch)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: warmup_cosine_lambda(step, warmup_steps, total_steps))\n",
    "\n",
    "    # AMP scaler: we keep AMP on for forwards; loss is computed in fp32\n",
    "    use_cuda = (device.type == \"cuda\")\n",
    "    amp_dtype = torch.bfloat16 if (use_cuda and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    scaler = GradScaler(enabled=(use_cuda and amp_dtype == torch.float16))\n",
    "\n",
    "    # ------- feature bank (teacher embeddings) -------\n",
    "    bank = FeatureBank(dim=128, size=bank_size, device=device)\n",
    "\n",
    "    best_sim = -1e9\n",
    "    global_step = 0\n",
    "    start_epoch = 1\n",
    "\n",
    "    # ------- resume -------\n",
    "    if resume_path is None:\n",
    "        resume_path = save_path.replace(\".pth\", \"_ckpt.pth\")\n",
    "    if resume_path and os.path.isfile(resume_path):\n",
    "        ckpt = torch.load(resume_path, map_location=device)\n",
    "        if isinstance(ckpt, dict) and \"online\" in ckpt and \"optimizer\" in ckpt:\n",
    "            try:\n",
    "                online.load_state_dict(ckpt[\"online\"], strict=True)\n",
    "                teacher.teacher.load_state_dict(ckpt[\"teacher\"], strict=False)\n",
    "            except Exception as e:\n",
    "                print(f\"[Resume] strict load failed: {e} — retrying with strict=False\")\n",
    "                online.load_state_dict(ckpt[\"online\"], strict=False)\n",
    "                teacher.teacher.load_state_dict(ckpt[\"teacher\"], strict=False)\n",
    "            try: optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "            except Exception as e: print(f\"[Resume] optimizer not loaded: {e}\")\n",
    "            try: scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "            except Exception as e: print(f\"[Resume] scheduler not loaded: {e}\")\n",
    "            if scaler.is_enabled() and ckpt.get(\"scaler\") is not None:\n",
    "                try: scaler.load_state_dict(ckpt[\"scaler\"])\n",
    "                except Exception as e: print(f\"[Resume] scaler not loaded: {e}\")\n",
    "            global_step = int(ckpt.get(\"global_step\", 0))\n",
    "            start_epoch = int(ckpt.get(\"epoch\", 0)) + 1\n",
    "            best_sim = float(ckpt.get(\"best_sim\", best_sim))\n",
    "            print(f\"[Resume] FULL checkpoint loaded (epoch={start_epoch-1}, best_sim={best_sim:.4f})\")\n",
    "        else:\n",
    "            print(f\"[Resume] Weights-only file detected; loading encoder only.\")\n",
    "            try:\n",
    "                online.load_state_dict(ckpt, strict=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[Resume] strict load failed: {e} — using strict=False\")\n",
    "                online.load_state_dict(ckpt, strict=False)\n",
    "            with torch.no_grad():\n",
    "                teacher._init_from(online)\n",
    "\n",
    "    # ------- train loop -------\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        online.train()\n",
    "        running = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Warmup ramp for auxiliary losses (prevents early explosions)\n",
    "        aux_ramp = min(1.0, epoch / 5.0)\n",
    "        eff_beta_siam    = beta_siam    * aux_ramp\n",
    "        eff_gamma_vicreg = gamma_vicreg * aux_ramp\n",
    "\n",
    "        for it, views in enumerate(train_loader):\n",
    "            assert len(views) >= 2, \"Need at least two global views\"\n",
    "            v_g1 = views[0].to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "            v_g2 = views[1].to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "            v_l  = views[2].to(device, non_blocking=True).to(memory_format=torch.channels_last) if len(views) > 2 else None\n",
    "\n",
    "            # Online forward under AMP\n",
    "            with autocast(enabled=use_cuda, dtype=amp_dtype):\n",
    "                z_g1 = online(v_g1)                  # (B,d)\n",
    "                z_g2 = online(v_g2)                  # (B,d)\n",
    "                _, z_g1_p = online(v_g1, for_predictor=True)\n",
    "                _, z_g2_p = online(v_g2, for_predictor=True)\n",
    "                if v_l is not None:\n",
    "                    _, z_l_p = online(v_l, for_predictor=True)\n",
    "                    z_l = online(v_l)\n",
    "                else:\n",
    "                    z_l_p, z_l = None, None\n",
    "\n",
    "            # Teacher in fp32\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled=False):\n",
    "                    t_g1 = teacher(v_g1)\n",
    "                    t_g2 = teacher(v_g2)\n",
    "                    t_l  = teacher(v_l) if v_l is not None else None\n",
    "\n",
    "            # Sanitize all embeddings to kill NaNs/Infs\n",
    "            z_g1, z_g2, z_g1_p, z_g2_p, z_l, z_l_p = _sanitize_(z_g1, z_g2, z_g1_p, z_g2_p, z_l, z_l_p)\n",
    "            t_g1, t_g2, t_l = _sanitize_(t_g1, t_g2, t_l)\n",
    "\n",
    "            # ----- Losses computed in fp32 for stability -----\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                # sample a subset of negatives for speed\n",
    "                bank_feats = bank.get()\n",
    "                if bank_feats is not None and bank_feats.numel() > 0 and bank_feats.size(0) > bank_sample:\n",
    "                    idx = torch.randperm(bank_feats.size(0), device=bank_feats.device)[:bank_sample]\n",
    "                    bank_feats = bank_feats[idx]\n",
    "\n",
    "                loss_ctr = ntxent.forward_with_bank(\n",
    "                    z_g1.float(), z_g2.float(), t_g1.float(), t_g2.float(),\n",
    "                    bank_feats.float() if bank_feats is not None else None\n",
    "                )\n",
    "                loss_total = alpha_contrast * loss_ctr\n",
    "\n",
    "                if eff_beta_siam > 0.0:\n",
    "                    if v_l is not None:\n",
    "                        loss_siam = 0.5 * (simsiam_loss(z_g1_p.float(), z_l.detach().float()) +\n",
    "                                           simsiam_loss(z_l_p.float(),  z_g1.detach().float()))\n",
    "                    else:\n",
    "                        loss_siam = 0.5 * (simsiam_loss(z_g1_p.float(), z_g2.detach().float()) +\n",
    "                                           simsiam_loss(z_g2_p.float(), z_g1.detach().float()))\n",
    "                    loss_total = loss_total + eff_beta_siam * loss_siam\n",
    "\n",
    "                if eff_gamma_vicreg > 0.0:\n",
    "                    if v_l is not None:\n",
    "                        loss_vcr = vicreg_loss(z_g1.float(), z_l.float(), vic_sim, vic_var, vic_cov)\n",
    "                    else:\n",
    "                        loss_vcr = vicreg_loss(z_g1.float(), z_g2.float(), vic_sim, vic_var, vic_cov)\n",
    "                    loss_total = loss_total + eff_gamma_vicreg * loss_vcr\n",
    "\n",
    "                loss_total = loss_total / max(1, accum_steps)\n",
    "                if not torch.isfinite(loss_total):\n",
    "                    loss_total = torch.nan_to_num(loss_total, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            scaler.scale(loss_total).backward()\n",
    "            running += float(loss_total.item()) * max(1, accum_steps)\n",
    "\n",
    "            if (it + 1) % max(1, accum_steps) == 0:\n",
    "                if scaler.is_enabled(): scaler.unscale_(optimizer)\n",
    "                if clip_grad_norm and clip_grad_norm > 0:\n",
    "                    nn.utils.clip_grad_norm_(online.parameters(), max_norm=clip_grad_norm)\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                m_now = ema_momentum_at(global_step, total_steps, m_base=ema_m, m_final=0.9995)\n",
    "                with torch.no_grad():\n",
    "                    teacher.update(online, m=m_now)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    to_enqueue = [t_g1, t_g2] + ([t_l] if (t_l is not None) else [])\n",
    "                    cat = torch.cat([t for t in to_enqueue if t is not None], dim=0)\n",
    "                    bank.enqueue(cat)\n",
    "\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            knn_sim = knn_sim_eval_multicrop(online, val_loader, device, use_backbone=True, K=20)\n",
    "\n",
    "        cur_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"[Epoch {epoch:03d}] loss={running/len(train_loader):.4f} | kNN-sim={knn_sim:.4f} | lr={cur_lr:.2e}\")\n",
    "\n",
    "        if knn_sim > best_sim:\n",
    "            best_sim = knn_sim\n",
    "            torch.save(online.state_dict(), save_path)\n",
    "            print(f\"  ✔ Saved BEST encoder weights to {save_path} (kNN-sim={best_sim:.4f})\")\n",
    "\n",
    "        ckpt_path = save_path.replace(\".pth\", \"_ckpt.pth\")\n",
    "        torch.save({\n",
    "            \"online\": online.state_dict(),\n",
    "            \"teacher\": teacher.teacher.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if scaler.is_enabled() else None,\n",
    "            \"epoch\": epoch,\n",
    "            \"global_step\": global_step,\n",
    "            \"best_sim\": best_sim,\n",
    "            \"config\": {\n",
    "                \"audio_dir\": audio_dir, \"epochs\": epochs, \"batch_size\": batch_size, \"lr\": lr,\n",
    "                \"weight_decay\": weight_decay, \"temperature\": temperature, \"accum_steps\": accum_steps,\n",
    "                \"ema_m\": ema_m, \"pos_topk\": pos_topk, \"pos_thr\": pos_thr, \"warmup_epochs\": warmup_epochs,\n",
    "                \"num_workers\": num_workers, \"compile_model\": compile_model, \"clip_grad_norm\": clip_grad_norm,\n",
    "                \"seed\": seed, \"mel_choices\": mel_choices, \"n_global\": n_global, \"global_T\": global_T,\n",
    "                \"n_local\": n_local, \"local_T\": local_T, \"resize_to\": resize_to,\n",
    "                \"bank_size\": bank_size, \"bank_sample\": bank_sample,\n",
    "                \"alpha_contrast\": alpha_contrast, \"beta_siam\": beta_siam, \"gamma_vicreg\": gamma_vicreg,\n",
    "                \"vic_sim\": vic_sim, \"vic_var\": vic_var, \"vic_cov\": vic_cov\n",
    "            }\n",
    "        }, ckpt_path)\n",
    "\n",
    "    last_path = save_path.replace(\".pth\", \"_last.pth\")\n",
    "    torch.save(online.state_dict(), last_path)\n",
    "    print(f\"Final encoder weights saved to {last_path}\")\n",
    "    return {\"best_encoder\": save_path, \"last_encoder\": last_path, \"best_knn_sim\": best_sim}\n",
    "\n",
    "# ----------------------------\n",
    "# Optional: embeddings extraction (one global view)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(encoder_ckpt_path, audio_dir, batch_size=128, n_workers=4, device_str=None,\n",
    "                       resize_to=(128,256), mel_choice=(128,1024,256), T_win=256):\n",
    "    device = torch.device(device_str or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    enc = EncoderProj(proj_dim=128, use_predictor=True).to(device).to(memory_format=torch.channels_last)\n",
    "    enc.load_state_dict(torch.load(encoder_ckpt_path, map_location=device), strict=True)\n",
    "    enc.eval()\n",
    "\n",
    "    class _EvalDS(Dataset):\n",
    "        def __init__(self, audio_dir):\n",
    "            self.paths = sorted(\n",
    "                [os.path.join(audio_dir, f) for f in os.listdir(audio_dir)\n",
    "                 if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "                key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "            )\n",
    "            self.sr=10000; self.resize_to=resize_to\n",
    "            self.mel_choice = mel_choice; self.T_win=T_win\n",
    "        def __len__(self): return len(self.paths)\n",
    "        def _lm(self, y, n_mels,n_fft,hop):\n",
    "            y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "            mx = float(np.max(np.abs(y))); mx = 1.0 if (not np.isfinite(mx) or mx<1e-8) else mx\n",
    "            y = (y/mx).astype(np.float32); y_t=torch.from_numpy(y).unsqueeze(0)\n",
    "            mel = ta.transforms.MelSpectrogram(sample_rate=self.sr,n_fft=n_fft,hop_length=hop,n_mels=n_mels)(y_t)\n",
    "            mel = torch.nan_to_num(mel, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            lm = torch.log(torch.clamp(mel, min=1e-5))\n",
    "            lm = torch.nan_to_num(lm, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            m, s = lm.mean(), lm.std(); s = s if (torch.isfinite(s) and s>=1e-6) else torch.tensor(1e-6, device=lm.device)\n",
    "            return (lm - m)/s\n",
    "        def _center(self, lm):\n",
    "            e = lm.mean(dim=1).squeeze(0); c=int(torch.argmax(e).item())\n",
    "            T=lm.size(2); s=max(0, min(c-self.T_win//2, T-self.T_win))\n",
    "            return lm[:,:,s:s+self.T_win] if T>self.T_win else lm\n",
    "        def __getitem__(self, idx):\n",
    "            y = np.load(self.paths[idx]).astype(np.float32)\n",
    "            _ = self._center(self._lm(y,64,1024,512))\n",
    "            n_mels,n_fft,hop = self.mel_choice\n",
    "            lm = self._lm(y,n_mels,n_fft,hop)\n",
    "            crop = self._center(lm)\n",
    "            if self.resize_to is not None:\n",
    "                Ft,Tt=self.resize_to\n",
    "                crop = F.interpolate(crop.unsqueeze(0), size=(Ft,Tt), mode='bilinear', align_corners=False).squeeze(0)\n",
    "            return crop\n",
    "\n",
    "    ds = _EvalDS(audio_dir)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=n_workers, pin_memory=(device.type==\"cuda\"))\n",
    "\n",
    "    feats = []\n",
    "    for x in loader:\n",
    "        x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        h, _ = enc(x, return_backbone=True)\n",
    "        feats.append(h.cpu())\n",
    "    H = torch.cat(feats, dim=0).numpy().astype(np.float32)\n",
    "    return H\n",
    "\n",
    "# ----------------------------\n",
    "# Example\n",
    "# ----------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Tip for notebooks: set num_workers=0 to avoid teardown spam; in scripts you can use >0.\n",
    "#     train_simclr_multipos(\n",
    "#         audio_dir=\"/notebooks/dataset_preprocessed\",\n",
    "#         epochs=200,\n",
    "#         batch_size=256,\n",
    "#         lr=3e-4,\n",
    "#         weight_decay=1e-4,\n",
    "#         temperature=0.1,\n",
    "#         accum_steps=2,\n",
    "#         ema_m=0.99,\n",
    "#         pos_topk=5,\n",
    "#         pos_thr=0.7,\n",
    "#         save_path=\"simclr_ne.pth\",\n",
    "#         warmup_epochs=10,\n",
    "#         num_workers=4,              # set 0 in notebooks if you see cleanup warnings\n",
    "#         compile_model=False,        # True on PyTorch 2.x if desired (first iter slower)\n",
    "#         clip_grad_norm=1.0,\n",
    "#         seed=42,\n",
    "#         resume_path=None,\n",
    "#         mel_choices=((64,1024,512),(128,1024,256),(128,2048,512)),\n",
    "#         n_global=2, global_T=256,\n",
    "#         n_local=2,  local_T=96,\n",
    "#         resize_to=(128,256),\n",
    "#         bank_size=8192,\n",
    "#         bank_sample=2048,           # try 2048 first; raise to 4096 if still fast\n",
    "#         alpha_contrast=1.0,\n",
    "#         beta_siam=0.1,\n",
    "#         gamma_vicreg=0.1,\n",
    "#         vic_sim=25.0, vic_var=25.0, vic_cov=1.0,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:07:47.376072Z",
     "iopub.status.busy": "2025-09-17T22:07:47.375546Z",
     "iopub.status.idle": "2025-09-18T00:07:34.125961Z",
     "shell.execute_reply": "2025-09-18T00:07:34.125319Z",
     "shell.execute_reply.started": "2025-09-17T22:07:47.376051Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 22:07:56.607681: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-17 22:07:56.607742: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-17 22:07:56.608941: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-17 22:07:56.615586: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-17 22:07:57.886620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Could not import from your_training_file: No module named 'your_training_file'\n",
      "[INFO] Scanning: /notebooks/MX_PA_2022_preprocessed\n",
      "[INFO] Found 211770 candidate .npy files\n",
      "[INFO] Subset size: 211770 (fraction=1)\n",
      "[INFO] Encoder loaded on cuda • CUDA available: True\n",
      "[INFO] DataLoader ready: 211770 samples • batch_size=128 • workers=0\n",
      "[INFO] Starting embedding pass on cuda (AMP=True)\n",
      "[INFO] Batches: 1 • Seen: 128 • 71.0 items/s\n",
      "[INFO] Batches: 10 • Seen: 1280 • 76.5 items/s\n",
      "[INFO] Batches: 20 • Seen: 2560 • 87.7 items/s\n",
      "[INFO] Batches: 30 • Seen: 3840 • 72.8 items/s\n",
      "[INFO] Batches: 40 • Seen: 5120 • 65.9 items/s\n",
      "[INFO] Batches: 50 • Seen: 6400 • 69.5 items/s\n",
      "[INFO] Batches: 60 • Seen: 7680 • 74.9 items/s\n",
      "[INFO] Batches: 70 • Seen: 8960 • 77.7 items/s\n",
      "[INFO] Batches: 80 • Seen: 10240 • 78.3 items/s\n",
      "[INFO] Batches: 90 • Seen: 11520 • 70.6 items/s\n",
      "[INFO] Batches: 100 • Seen: 12800 • 64.3 items/s\n",
      "[INFO] Batches: 110 • Seen: 14080 • 61.3 items/s\n",
      "[INFO] Batches: 120 • Seen: 15360 • 63.2 items/s\n",
      "[INFO] Batches: 130 • Seen: 16640 • 64.8 items/s\n",
      "[INFO] Batches: 140 • Seen: 17920 • 66.7 items/s\n",
      "[INFO] Batches: 150 • Seen: 19200 • 68.5 items/s\n",
      "[INFO] Batches: 160 • Seen: 20480 • 69.2 items/s\n",
      "[INFO] Batches: 170 • Seen: 21760 • 69.8 items/s\n",
      "[INFO] Batches: 180 • Seen: 23040 • 69.1 items/s\n",
      "[INFO] Batches: 190 • Seen: 24320 • 67.9 items/s\n",
      "[INFO] Batches: 200 • Seen: 25600 • 68.7 items/s\n",
      "[INFO] Batches: 210 • Seen: 26880 • 69.7 items/s\n",
      "[INFO] Batches: 220 • Seen: 28160 • 69.5 items/s\n",
      "[INFO] Batches: 230 • Seen: 29440 • 68.6 items/s\n",
      "[INFO] Batches: 240 • Seen: 30720 • 67.7 items/s\n",
      "[INFO] Batches: 250 • Seen: 32000 • 68.1 items/s\n",
      "[INFO] Batches: 260 • Seen: 33280 • 69.2 items/s\n",
      "[INFO] Batches: 270 • Seen: 34560 • 68.4 items/s\n",
      "[INFO] Batches: 280 • Seen: 35840 • 69.2 items/s\n",
      "[INFO] Batches: 290 • Seen: 37120 • 69.2 items/s\n",
      "[INFO] Batches: 300 • Seen: 38400 • 67.5 items/s\n",
      "[INFO] Batches: 310 • Seen: 39680 • 65.9 items/s\n",
      "[INFO] Batches: 320 • Seen: 40960 • 64.2 items/s\n",
      "[INFO] Batches: 330 • Seen: 42240 • 63.2 items/s\n",
      "[INFO] Batches: 340 • Seen: 43520 • 62.7 items/s\n",
      "[INFO] Batches: 350 • Seen: 44800 • 61.2 items/s\n",
      "[INFO] Batches: 360 • Seen: 46080 • 61.4 items/s\n",
      "[INFO] Batches: 370 • Seen: 47360 • 61.7 items/s\n",
      "[INFO] Batches: 380 • Seen: 48640 • 60.8 items/s\n",
      "[INFO] Batches: 390 • Seen: 49920 • 60.0 items/s\n",
      "[INFO] Batches: 400 • Seen: 51200 • 60.5 items/s\n",
      "[INFO] Batches: 410 • Seen: 52480 • 61.1 items/s\n",
      "[INFO] Batches: 420 • Seen: 53760 • 60.7 items/s\n",
      "[INFO] Batches: 430 • Seen: 55040 • 61.0 items/s\n",
      "[INFO] Batches: 440 • Seen: 56320 • 60.3 items/s\n",
      "[INFO] Batches: 450 • Seen: 57600 • 61.0 items/s\n",
      "[INFO] Batches: 460 • Seen: 58880 • 61.4 items/s\n",
      "[INFO] Batches: 470 • Seen: 60160 • 61.5 items/s\n",
      "[INFO] Batches: 480 • Seen: 61440 • 61.6 items/s\n",
      "[INFO] Batches: 490 • Seen: 62720 • 60.2 items/s\n",
      "[INFO] Batches: 500 • Seen: 64000 • 58.6 items/s\n",
      "[INFO] Batches: 510 • Seen: 65280 • 59.1 items/s\n",
      "[INFO] Batches: 520 • Seen: 66560 • 58.8 items/s\n",
      "[INFO] Batches: 530 • Seen: 67840 • 58.4 items/s\n",
      "[INFO] Batches: 540 • Seen: 69120 • 58.4 items/s\n",
      "[INFO] Batches: 550 • Seen: 70400 • 58.6 items/s\n",
      "[INFO] Batches: 560 • Seen: 71680 • 58.0 items/s\n",
      "[INFO] Batches: 570 • Seen: 72960 • 57.2 items/s\n",
      "[INFO] Batches: 580 • Seen: 74240 • 54.4 items/s\n",
      "[INFO] Batches: 590 • Seen: 75520 • 54.7 items/s\n",
      "[INFO] Batches: 600 • Seen: 76800 • 53.6 items/s\n",
      "[INFO] Batches: 610 • Seen: 78080 • 53.3 items/s\n",
      "[INFO] Batches: 620 • Seen: 79360 • 53.3 items/s\n",
      "[INFO] Batches: 630 • Seen: 80640 • 53.4 items/s\n",
      "[INFO] Batches: 640 • Seen: 81920 • 53.4 items/s\n",
      "[INFO] Batches: 650 • Seen: 83200 • 52.8 items/s\n",
      "[INFO] Batches: 660 • Seen: 84480 • 52.3 items/s\n",
      "[INFO] Batches: 670 • Seen: 85760 • 52.5 items/s\n",
      "[INFO] Batches: 680 • Seen: 87040 • 52.8 items/s\n",
      "[INFO] Batches: 690 • Seen: 88320 • 52.9 items/s\n",
      "[INFO] Batches: 700 • Seen: 89600 • 52.8 items/s\n",
      "[INFO] Batches: 710 • Seen: 90880 • 52.9 items/s\n",
      "[INFO] Batches: 720 • Seen: 92160 • 53.0 items/s\n",
      "[INFO] Batches: 730 • Seen: 93440 • 51.9 items/s\n",
      "[INFO] Batches: 740 • Seen: 94720 • 51.9 items/s\n",
      "[INFO] Batches: 750 • Seen: 96000 • 52.0 items/s\n",
      "[INFO] Batches: 760 • Seen: 97280 • 52.3 items/s\n",
      "[INFO] Batches: 770 • Seen: 98560 • 52.3 items/s\n",
      "[INFO] Batches: 780 • Seen: 99840 • 52.0 items/s\n",
      "[INFO] Batches: 790 • Seen: 101120 • 51.1 items/s\n",
      "[INFO] Batches: 800 • Seen: 102400 • 51.4 items/s\n",
      "[INFO] Batches: 810 • Seen: 103680 • 50.6 items/s\n",
      "[INFO] Batches: 820 • Seen: 104960 • 50.4 items/s\n",
      "[INFO] Batches: 830 • Seen: 106240 • 50.1 items/s\n",
      "[INFO] Batches: 840 • Seen: 107520 • 49.9 items/s\n",
      "[INFO] Batches: 850 • Seen: 108800 • 49.6 items/s\n",
      "[INFO] Batches: 860 • Seen: 110080 • 49.8 items/s\n",
      "[INFO] Batches: 870 • Seen: 111360 • 49.6 items/s\n",
      "[INFO] Batches: 880 • Seen: 112640 • 49.2 items/s\n",
      "[INFO] Batches: 890 • Seen: 113920 • 48.7 items/s\n",
      "[INFO] Batches: 900 • Seen: 115200 • 48.8 items/s\n",
      "[INFO] Batches: 910 • Seen: 116480 • 48.0 items/s\n",
      "[INFO] Batches: 920 • Seen: 117760 • 48.3 items/s\n",
      "[INFO] Batches: 930 • Seen: 119040 • 47.8 items/s\n",
      "[INFO] Batches: 940 • Seen: 120320 • 47.7 items/s\n",
      "[INFO] Batches: 950 • Seen: 121600 • 46.9 items/s\n",
      "[INFO] Batches: 960 • Seen: 122880 • 46.8 items/s\n",
      "[INFO] Batches: 970 • Seen: 124160 • 46.7 items/s\n",
      "[INFO] Batches: 980 • Seen: 125440 • 46.1 items/s\n",
      "[INFO] Batches: 990 • Seen: 126720 • 46.2 items/s\n",
      "[INFO] Batches: 1000 • Seen: 128000 • 46.0 items/s\n",
      "[INFO] Batches: 1010 • Seen: 129280 • 45.3 items/s\n",
      "[INFO] Batches: 1020 • Seen: 130560 • 44.5 items/s\n",
      "[INFO] Batches: 1030 • Seen: 131840 • 43.8 items/s\n",
      "[INFO] Batches: 1040 • Seen: 133120 • 43.6 items/s\n",
      "[INFO] Batches: 1050 • Seen: 134400 • 43.5 items/s\n",
      "[INFO] Batches: 1060 • Seen: 135680 • 43.5 items/s\n",
      "[INFO] Batches: 1070 • Seen: 136960 • 43.1 items/s\n",
      "[INFO] Batches: 1080 • Seen: 138240 • 43.0 items/s\n",
      "[INFO] Batches: 1090 • Seen: 139520 • 42.6 items/s\n",
      "[INFO] Batches: 1100 • Seen: 140800 • 42.6 items/s\n",
      "[INFO] Batches: 1110 • Seen: 142080 • 42.4 items/s\n",
      "[INFO] Batches: 1120 • Seen: 143360 • 42.5 items/s\n",
      "[INFO] Batches: 1130 • Seen: 144640 • 42.2 items/s\n",
      "[INFO] Batches: 1140 • Seen: 145920 • 41.8 items/s\n",
      "[INFO] Batches: 1150 • Seen: 147200 • 41.7 items/s\n",
      "[INFO] Batches: 1160 • Seen: 148480 • 41.5 items/s\n",
      "[INFO] Batches: 1170 • Seen: 149760 • 41.5 items/s\n",
      "[INFO] Batches: 1180 • Seen: 151040 • 41.6 items/s\n",
      "[INFO] Batches: 1190 • Seen: 152320 • 41.7 items/s\n",
      "[INFO] Batches: 1200 • Seen: 153600 • 41.8 items/s\n",
      "[INFO] Batches: 1210 • Seen: 154880 • 41.7 items/s\n",
      "[INFO] Batches: 1220 • Seen: 156160 • 41.6 items/s\n",
      "[INFO] Batches: 1230 • Seen: 157440 • 41.4 items/s\n",
      "[INFO] Batches: 1240 • Seen: 158720 • 41.3 items/s\n",
      "[INFO] Batches: 1250 • Seen: 160000 • 41.5 items/s\n",
      "[INFO] Batches: 1260 • Seen: 161280 • 40.8 items/s\n",
      "[INFO] Batches: 1270 • Seen: 162560 • 40.3 items/s\n",
      "[INFO] Batches: 1280 • Seen: 163840 • 40.1 items/s\n",
      "[INFO] Batches: 1290 • Seen: 165120 • 39.9 items/s\n",
      "[INFO] Batches: 1300 • Seen: 166400 • 39.7 items/s\n",
      "[INFO] Batches: 1310 • Seen: 167680 • 39.4 items/s\n",
      "[INFO] Batches: 1320 • Seen: 168960 • 39.2 items/s\n",
      "[INFO] Batches: 1330 • Seen: 170240 • 38.9 items/s\n",
      "[INFO] Batches: 1340 • Seen: 171520 • 38.8 items/s\n",
      "[INFO] Batches: 1350 • Seen: 172800 • 38.8 items/s\n",
      "[INFO] Batches: 1360 • Seen: 174080 • 38.8 items/s\n",
      "[INFO] Batches: 1370 • Seen: 175360 • 39.0 items/s\n",
      "[INFO] Batches: 1380 • Seen: 176640 • 38.9 items/s\n",
      "[INFO] Batches: 1390 • Seen: 177920 • 38.7 items/s\n",
      "[INFO] Batches: 1400 • Seen: 179200 • 38.4 items/s\n",
      "[INFO] Batches: 1410 • Seen: 180480 • 38.0 items/s\n",
      "[INFO] Batches: 1420 • Seen: 181760 • 38.1 items/s\n",
      "[INFO] Batches: 1430 • Seen: 183040 • 38.0 items/s\n",
      "[INFO] Batches: 1440 • Seen: 184320 • 37.8 items/s\n",
      "[INFO] Batches: 1450 • Seen: 185600 • 37.5 items/s\n",
      "[INFO] Batches: 1460 • Seen: 186880 • 37.3 items/s\n",
      "[INFO] Batches: 1470 • Seen: 188160 • 37.3 items/s\n",
      "[INFO] Batches: 1480 • Seen: 189440 • 37.0 items/s\n",
      "[INFO] Batches: 1490 • Seen: 190720 • 36.9 items/s\n",
      "[INFO] Batches: 1500 • Seen: 192000 • 36.9 items/s\n",
      "[INFO] Batches: 1510 • Seen: 193280 • 36.9 items/s\n",
      "[INFO] Batches: 1520 • Seen: 194560 • 36.4 items/s\n",
      "[INFO] Batches: 1530 • Seen: 195840 • 36.0 items/s\n",
      "[INFO] Batches: 1540 • Seen: 197120 • 36.1 items/s\n",
      "[INFO] Batches: 1550 • Seen: 198400 • 36.2 items/s\n",
      "[INFO] Batches: 1560 • Seen: 199680 • 36.3 items/s\n",
      "[INFO] Batches: 1570 • Seen: 200960 • 36.5 items/s\n",
      "[INFO] Batches: 1580 • Seen: 202240 • 36.6 items/s\n",
      "[INFO] Batches: 1590 • Seen: 203520 • 36.6 items/s\n",
      "[INFO] Batches: 1600 • Seen: 204800 • 36.6 items/s\n",
      "[INFO] Batches: 1610 • Seen: 206080 • 36.4 items/s\n",
      "[INFO] Batches: 1620 • Seen: 207360 • 36.3 items/s\n",
      "[INFO] Batches: 1630 • Seen: 208640 • 36.3 items/s\n",
      "[INFO] Batches: 1640 • Seen: 209920 • 36.2 items/s\n",
      "[INFO] Batches: 1650 • Seen: 211200 • 36.1 items/s\n",
      "[INFO] Embedding done: 211770 items in 5878.51s\n",
      "[INFO] Starting UMAP on 211770 items, dim=512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] UMAP done in 206.89s\n",
      "[INFO] Clustering with kmeans, k=60\n",
      "[INFO] Clustering done in 42.97s\n",
      "[INFO] Silhouette (↑): 0.11465297639369965\n",
      "[INFO] Davies–Bouldin (↓): 1.6620144583289869\n",
      "[INFO] Calinski–Harabasz (↑): 25601.323936378536\n",
      "[INFO] ARI (↑): 0.0\n",
      "[INFO] AMI (↑): 0.0\n",
      "[INFO] Hungarian Accuracy (↑): 0.07170515181564906\n",
      "✅ Report saved to: PR_k60_kmeans_subset100.html\n"
     ]
    }
   ],
   "source": [
    "# ===== Unsupervised Report (2% subset, safe mode) =====\n",
    "# Works with your improved SimCLR EncoderProj(proj_dim=256).\n",
    "# Deterministic 2% subset, safe single-process DataLoader to avoid hangs,\n",
    "# progress logs, and dataset sanity checks.\n",
    "#\n",
    "# Adds:\n",
    "# - Spectrogram-space intra-cluster consistency:\n",
    "#     * Mean spectrogram cosine similarity (↑ better)\n",
    "#     * Optional DTW distance on time traces (↓ better; requires fastdtw)\n",
    "# - Global summary card aggregating these per-cluster metrics.\n",
    "\n",
    "import os, math, contextlib, base64, time\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "from io import BytesIO\n",
    "from typing import Optional, Dict  # NEW\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# plotting / viz\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import entropy\n",
    "from scipy.signal import spectrogram, get_window\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def hungarian_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Maximum bipartite matching accuracy between predicted cluster IDs and true labels.\n",
    "    Works with string labels too.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    true_ids = {t: i for i, t in enumerate(np.unique(y_true))}\n",
    "    pred_ids = {c: i for i, c in enumerate(np.unique(y_pred))}\n",
    "    y_true_i = np.vectorize(true_ids.get)(y_true)\n",
    "    y_pred_i = np.vectorize(pred_ids.get)(y_pred)\n",
    "\n",
    "    W = np.zeros((len(pred_ids), len(true_ids)), dtype=np.int64)\n",
    "    for i in range(y_pred_i.size):\n",
    "        W[y_pred_i[i], y_true_i[i]] += 1\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(W.max() - W)  # maximize matches\n",
    "    return float(W[row_ind, col_ind].sum() / y_pred_i.size)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Import your model + (optional) pad_collate from training code\n",
    "# ------------------------------------------------------------------------------------\n",
    "# !!! IMPORTANT: change 'your_training_file' to your actual module name !!!\n",
    "try:\n",
    "    from your_training_file import EncoderProj, pad_collate  # noqa\n",
    "    _HAS_PAD_COL = True\n",
    "    print(\"[INFO] Imported EncoderProj and pad_collate from your_training_file.\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Could not import from your_training_file: {e}\")\n",
    "    # You MUST still provide EncoderProj somewhere on PYTHONPATH for this script to work.\n",
    "    # If pad_collate isn't available, we'll fall back to default collate.\n",
    "    _HAS_PAD_COL = False\n",
    "    try:\n",
    "        # If EncoderProj is defined elsewhere / already imported, this won't raise.\n",
    "        EncoderProj  # type: ignore  # noqa\n",
    "    except NameError:\n",
    "        raise RuntimeError(\n",
    "            \"EncoderProj is not imported. Please do:\\n\"\n",
    "            \"from <your_module> import EncoderProj  # and optionally pad_collate\"\n",
    "        )\n",
    "\n",
    "    if 'pad_collate' not in globals():\n",
    "        pad_collate = None  # type: ignore\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Dataset for precomputed spectrograms or raw waveforms (.npy files)\n",
    "# ------------------------------------------------------------------------------------\n",
    "def _wave_to_spec_1xFxT(y_np: np.ndarray, sr: int = 10000, n_mels: int = 64,\n",
    "                        n_fft: int = 1024, hop_length: int = 512) -> torch.Tensor:\n",
    "    y_np = np.nan_to_num(y_np.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    mx = np.max(np.abs(y_np)); mx = 1.0 if (not np.isfinite(mx) or mx < 1e-8) else mx\n",
    "    y_t = torch.from_numpy(y_np / mx).unsqueeze(0)  # (1, T)\n",
    "\n",
    "    try:\n",
    "        import torchaudio as ta\n",
    "        mel = ta.transforms.MelSpectrogram(sample_rate=sr, n_fft=n_fft,\n",
    "                                           hop_length=hop_length, n_mels=n_mels)\n",
    "        S = mel(y_t)                                # (1, F, T)\n",
    "        S = torch.clamp(S, min=1e-10).log()\n",
    "        return S.contiguous().float()\n",
    "    except Exception:\n",
    "        from scipy.signal import spectrogram, get_window\n",
    "        win = get_window('hann', n_fft, fftbins=True)\n",
    "        f, t, Z = spectrogram(y_np, fs=sr, window=win, nperseg=n_fft,\n",
    "                              noverlap=n_fft - hop_length, mode='magnitude')\n",
    "        S = np.log(np.maximum(Z, 1e-10)).astype(np.float32)  # (F_lin, T)\n",
    "        if S.shape[0] != n_mels:\n",
    "            idx = np.linspace(0, S.shape[0]-1, num=n_mels).astype(np.int32)\n",
    "            S = S[idx]\n",
    "        S = torch.from_numpy(S).unsqueeze(0)\n",
    "        return S.contiguous().float()\n",
    "\n",
    "\n",
    "class TwoViewPrecomputed(Dataset):\n",
    "    def __init__(self, spec_dir, T=256, per_sample_norm=True,\n",
    "                 sr=10000, n_mels=64, n_fft=1024, hop=512,\n",
    "                 wave_policy=\"convert\",   # \"convert\" or \"skip\"\n",
    "                 file_paths=None):\n",
    "        if file_paths is not None:\n",
    "            self.paths = list(file_paths)\n",
    "        else:\n",
    "            paths = []\n",
    "            for f in os.listdir(spec_dir):\n",
    "                if f.endswith(\".npy\") and f != \"labels.npy\":\n",
    "                    stem = os.path.splitext(f)[0]\n",
    "                    try: key = int(stem)\n",
    "                    except ValueError: key = stem\n",
    "                    paths.append((key, os.path.join(spec_dir, f)))\n",
    "            paths.sort(key=lambda t: t[0])\n",
    "            self.paths = [p for _, p in paths]\n",
    "\n",
    "        self.T = T\n",
    "        self.per_sample_norm = per_sample_norm\n",
    "        self.sr, self.n_mels, self.n_fft, self.hop = sr, n_mels, n_fft, hop\n",
    "        self.wave_policy = wave_policy\n",
    "\n",
    "        if self.wave_policy == \"skip\":\n",
    "            kept = []\n",
    "            for p in self.paths:\n",
    "                arr = np.load(p, mmap_mode=\"r\")\n",
    "                if arr.ndim == 1:\n",
    "                    continue\n",
    "                kept.append(p)\n",
    "            self.paths = kept\n",
    "\n",
    "    def _to_1xFxT(self, arr: np.ndarray) -> torch.Tensor:\n",
    "        arr = np.array(arr, copy=True)\n",
    "        if arr.ndim == 1:\n",
    "            if self.wave_policy == \"skip\":\n",
    "                raise RuntimeError(\"Waveform encountered but wave_policy='skip'.\")\n",
    "            return _wave_to_spec_1xFxT(arr, sr=self.sr, n_mels=self.n_mels,\n",
    "                                       n_fft=self.n_fft, hop_length=self.hop)\n",
    "        x = torch.from_numpy(arr)\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        elif x.ndim == 3:\n",
    "            if x.shape[0] != 1:\n",
    "                x = x.reshape(1, x.shape[0]*x.shape[1], x.shape[2])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected array shape {tuple(x.shape)}\")\n",
    "        return x.contiguous().float()\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat_np = np.load(self.paths[idx], mmap_mode=\"r\")\n",
    "        feat = self._to_1xFxT(feat_np)\n",
    "        if self.per_sample_norm:\n",
    "            m, s = feat.mean(), feat.std()\n",
    "            s = s if (torch.isfinite(s) and s >= 1e-6) else torch.tensor(1e-6, device=feat.device)\n",
    "            feat = (feat - m) / s\n",
    "        x1 = feat\n",
    "        x2 = feat.clone()\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# File utilities and viz helpers\n",
    "# ------------------------------------------------------------------------------------\n",
    "def list_audio_npy_files(folder):\n",
    "    files = []\n",
    "    for f in os.listdir(folder):\n",
    "        if f.endswith(\".npy\") and f != \"labels.npy\":\n",
    "            stem = os.path.splitext(f)[0]\n",
    "            try: key = int(stem)\n",
    "            except ValueError: key = stem\n",
    "            files.append((key, os.path.join(folder, f)))\n",
    "    files.sort(key=lambda t: t[0])\n",
    "    return [p for _, p in files]\n",
    "\n",
    "def load_class_labels_if_any(folder, count):\n",
    "    lbl_path = os.path.join(folder, \"labels.npy\")\n",
    "    if os.path.isfile(lbl_path):\n",
    "        try:\n",
    "            arr = np.load(lbl_path, allow_pickle=True)\n",
    "            if len(arr) < count:\n",
    "                pad = np.array([\"Unknown\"] * (count - len(arr)), dtype=object)\n",
    "                arr = np.concatenate([arr, pad], axis=0)\n",
    "            elif len(arr) > count:\n",
    "                arr = arr[:count]\n",
    "            return arr\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.array([\"Unknown\"] * count, dtype=object)\n",
    "\n",
    "def ali_spec(x: np.ndarray, fs: int):\n",
    "    Lframe2 = 1000\n",
    "    po = 80\n",
    "    lov = int(np.ceil((po / 100) * Lframe2))\n",
    "    taper = get_window('hann', Lframe2)\n",
    "    Nfft = 2 ** (int(np.floor(np.log2(Lframe2))) + 2)\n",
    "    f, t, s = spectrogram(x, fs=fs, window=taper, noverlap=lov, nfft=Nfft, mode='complex')\n",
    "    as_ = np.abs(s)\n",
    "    as_max = np.max(as_) if np.isfinite(np.max(as_)) and np.max(as_) > 0 else 1.0\n",
    "    sdb = 10 * np.log10(100 * as_ / as_max + 1e-10)\n",
    "    min_inx = np.argmin(np.abs(f - 0))\n",
    "    max_inx = np.argmin(np.abs(f - 800))\n",
    "    return sdb[min_inx:max_inx+1, :], f[min_inx:max_inx+1], t\n",
    "\n",
    "def generate_spectrogram_base64(audio, fs=10000, title=\"Spectrogram\"):\n",
    "    spec, f_axis, t_axis = ali_spec(audio, fs)\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.imshow(spec, aspect='auto', origin='lower',\n",
    "              extent=[t_axis[0], t_axis[-1], f_axis[0], f_axis[-1]], cmap='hsv')\n",
    "    ax.set_title(title); ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    fig.tight_layout()\n",
    "    buf = BytesIO(); fig.savefig(buf, format='png'); plt.close(fig); buf.seek(0)\n",
    "    image_base64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return f'<img class=\"d-block w-100\" src=\"data:image/png;base64,{image_base64}\" alt=\"{title}\">'\n",
    "\n",
    "def make_carousel(cluster_id, scope, imgs):\n",
    "    cid = f\"carousel_{scope}_{cluster_id}\"\n",
    "    indicators = \"\".join(\n",
    "        f'<button type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide-to=\"{i}\" {\"class=active\" if i==0 else \"\"} aria-current=\"true\" aria-label=\"Slide {i+1}\"></button>'\n",
    "        for i in range(len(imgs))\n",
    "    )\n",
    "    items = \"\".join(\n",
    "        f'<div class=\"carousel-item {\"active\" if i==0 else \"\"}\"><div class=\"d-flex justify-content-center\">{img}</div></div>'\n",
    "        for i, img in enumerate(imgs)\n",
    "    )\n",
    "    return f\"\"\"\n",
    "    <div id=\"{cid}\" class=\"carousel slide\" data-bs-interval=\"false\" data-bs-touch=\"false\">\n",
    "      <div class=\"carousel-indicators\">{indicators}</div>\n",
    "      <div class=\"carousel-inner\">{items}</div>\n",
    "      <button class=\"carousel-control-prev\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"prev\">\n",
    "        <span class=\"carousel-control-prev-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Previous</span>\n",
    "      </button>\n",
    "      <button class=\"carousel-control-next\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"next\">\n",
    "        <span class=\"carousel-control-next-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Next</span>\n",
    "      </button>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# NEW: spectrogram standardization & intra-cluster consistency metrics\n",
    "# ------------------------------------------------------------------------------------\n",
    "def _standardize_spec(S: np.ndarray, target_shape=(128, 256)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pad/crop 2D (F,T) spectrogram to target_shape. Per-spec z-norm for cosine comparability.\n",
    "    \"\"\"\n",
    "    S = np.asarray(S, dtype=np.float32)\n",
    "    S = (S - np.mean(S)) / (np.std(S) + 1e-8)\n",
    "    F, T = S.shape\n",
    "    Ft, Tt = target_shape\n",
    "\n",
    "    # F dimension\n",
    "    if F < Ft:\n",
    "        pad_top = (Ft - F) // 2\n",
    "        pad_bot = Ft - F - pad_top\n",
    "        S = np.pad(S, ((pad_top, pad_bot), (0, 0)), mode=\"constant\")\n",
    "    elif F > Ft:\n",
    "        start = (F - Ft) // 2\n",
    "        S = S[start:start+Ft, :]\n",
    "\n",
    "    # T dimension\n",
    "    F, T = S.shape\n",
    "    if T < Tt:\n",
    "        pad_left = (Tt - T) // 2\n",
    "        pad_right = Tt - T - pad_left\n",
    "        S = np.pad(S, ((0, 0), (pad_left, pad_right)), mode=\"constant\")\n",
    "    elif T > Tt:\n",
    "        start = (T - Tt) // 2\n",
    "        S = S[:, start:start+Tt]\n",
    "    return S\n",
    "\n",
    "def _load_spec_for_index(original_paths: np.ndarray, idx: int, fs: int = 10000,\n",
    "                         target_shape=(128, 256)) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load .npy (1D waveform or 2D spectrogram) and return standardized 2D spectrogram.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        arr = np.load(original_paths[idx], mmap_mode=\"r\")\n",
    "        if arr.ndim == 1:\n",
    "            S, _, _ = ali_spec(arr.astype(np.float32), fs=fs)\n",
    "        elif arr.ndim == 2:\n",
    "            S = arr.astype(np.float32)\n",
    "        else:\n",
    "            return None\n",
    "        return _standardize_spec(S, target_shape=target_shape)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _avg_intra_cluster_spec_cosine(original_paths: np.ndarray, idxs: np.ndarray,\n",
    "                                   *, fs: int = 10000, max_samples: int = 50,\n",
    "                                   target_shape=(128, 256)) -> float:\n",
    "    \"\"\"\n",
    "    Average pairwise cosine similarity of standardized spectrograms within a cluster.\n",
    "    Subsamples up to max_samples items for O(n^2) stability.\n",
    "    \"\"\"\n",
    "    if len(idxs) < 2:\n",
    "        return float(\"nan\")\n",
    "    if len(idxs) > max_samples:\n",
    "        rng = np.random.RandomState(42)\n",
    "        idxs = rng.choice(idxs, size=max_samples, replace=False)\n",
    "\n",
    "    specs = []\n",
    "    for i in idxs:\n",
    "        S = _load_spec_for_index(original_paths, int(i), fs=fs, target_shape=target_shape)\n",
    "        if S is not None:\n",
    "            specs.append(S.reshape(-1))\n",
    "    if len(specs) < 2:\n",
    "        return float(\"nan\")\n",
    "    X = np.stack(specs, axis=0)\n",
    "    sim = cosine_similarity(X)\n",
    "    iu = np.triu_indices_from(sim, k=1)\n",
    "    return float(np.mean(sim[iu]))\n",
    "\n",
    "def _avg_intra_cluster_spec_dtw(original_paths: np.ndarray, idxs: np.ndarray,\n",
    "                                *, fs: int = 10000, max_samples: int = 25,\n",
    "                                target_shape=(128, 256)) -> float:\n",
    "    \"\"\"\n",
    "    Average pairwise DTW distance between standardized spectrogram time-traces.\n",
    "    Lower = more similar. Optional: requires fastdtw.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from fastdtw import fastdtw\n",
    "        from scipy.spatial.distance import euclidean\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if len(idxs) < 2:\n",
    "        return float(\"nan\")\n",
    "    if len(idxs) > max_samples:\n",
    "        rng = np.random.RandomState(42)\n",
    "        idxs = rng.choice(idxs, size=max_samples, replace=False)\n",
    "\n",
    "    series = []\n",
    "    for i in idxs:\n",
    "        S = _load_spec_for_index(original_paths, int(i), fs=fs, target_shape=target_shape)\n",
    "        if S is not None:\n",
    "            series.append(np.mean(S, axis=0))  # collapse frequency to get a 1D time trace\n",
    "    if len(series) < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    n = len(series)\n",
    "    total, pairs = 0.0, 0\n",
    "    for a in range(n):\n",
    "        for b in range(a+1, n):\n",
    "            d, _ = fastdtw(series[a], series[b], dist=euclidean)\n",
    "            total += d\n",
    "            pairs += 1\n",
    "    return float(total / max(pairs, 1))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Clustering and metrics\n",
    "# ------------------------------------------------------------------------------------\n",
    "def choose_clusterer(algorithm: str, embeddings: np.ndarray, n_clusters: int):\n",
    "    if algorithm == 'kmeans':\n",
    "        clusterer = KMeans(n_clusters=n_clusters, n_init='auto').fit(embeddings)\n",
    "        return clusterer.labels_, None\n",
    "    elif algorithm == 'agglomerative':\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n_clusters).fit(embeddings)\n",
    "        return clusterer.labels_, None\n",
    "    elif algorithm == 'gmm':\n",
    "        clusterer = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=42).fit(embeddings)\n",
    "        return clusterer.predict(embeddings), clusterer.predict_proba(embeddings)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported clustering algorithm: {algorithm}\")\n",
    "\n",
    "def evaluate_cluster_metrics(embeddings, idxs, location_labels, location_entropy_base=None):\n",
    "    X = embeddings[idxs]\n",
    "    if X.shape[0] == 0:\n",
    "        return {'variance': 0.0, 'mean_sim': 1.0, 'entropy': 0.0, 'quality': 0.0, 'novelty': 0.0}\n",
    "    center = np.mean(X, axis=0, keepdims=True)\n",
    "    variance = float(np.mean(np.sum((X - center) ** 2, axis=1)))\n",
    "    if len(X) > 1:\n",
    "        cos_sim = cosine_similarity(X)\n",
    "        iu = np.triu_indices_from(cos_sim, k=1)\n",
    "        mean_sim = float(np.mean(cos_sim[iu])) if iu[0].size > 0 else 1.0\n",
    "    else:\n",
    "        mean_sim = 1.0\n",
    "    loc_counts = Counter(location_labels[idxs])\n",
    "    loc_probs = np.array(list(loc_counts.values()), dtype=np.float32)\n",
    "    loc_probs /= max(loc_probs.sum(), 1e-8)\n",
    "    base = int(location_entropy_base or len(set(location_labels)))\n",
    "    loc_entropy = float(entropy(loc_probs, base=base)) if base > 1 else 0.0\n",
    "    max_ent = np.log2(base) if base > 1 else 1.0\n",
    "    entropy_score = 1.0 - (loc_entropy / max_ent) if base > 1 else 1.0\n",
    "    quality = float((mean_sim / (variance + 1e-8)) * entropy_score)\n",
    "    novelty = float((loc_entropy / max_ent) * variance) if base > 1 else 0.0\n",
    "    return {'variance': variance, 'mean_sim': mean_sim, 'entropy': loc_entropy, 'quality': quality, 'novelty': novelty}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Embedding extraction\n",
    "# ------------------------------------------------------------------------------------\n",
    "def _load_encoder(encoder_ckpt_path, device):\n",
    "    enc = EncoderProj(proj_dim=256).to(device).to(memory_format=torch.channels_last)\n",
    "    ckpt = torch.load(encoder_ckpt_path, map_location=device)\n",
    "    if isinstance(ckpt, dict) and \"online\" in ckpt:\n",
    "        state_dict = ckpt[\"online\"]\n",
    "    elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "        state_dict = ckpt[\"state_dict\"]\n",
    "    else:\n",
    "        state_dict = ckpt\n",
    "    new_state = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace(\"encoder.\", \"\").replace(\"module.\", \"\")\n",
    "        new_state[new_key] = v\n",
    "    enc.load_state_dict(new_state, strict=False)\n",
    "    enc.eval()\n",
    "    print(f\"[INFO] Encoder loaded on {device} • CUDA available: {torch.cuda.is_available()}\")\n",
    "    return enc\n",
    "\n",
    "def extract_embeddings_from_specdir(\n",
    "    encoder_ckpt_path,\n",
    "    spec_dir,\n",
    "    batch_size=128,\n",
    "    n_workers=0,          # SAFE: 0 workers to avoid hangs for first run\n",
    "    device_str=None,\n",
    "    subset_fraction=0.02, # 2% subset\n",
    "    subset_seed=42,\n",
    "    subset_strategy=\"random\",  # \"random\" | \"tail\" | \"head\"\n",
    "    wave_policy=\"convert\"      # \"convert\" works for 1-D waveforms; use \"skip\" if you *know* files are (1,F,T)\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        H: (N_subset, D) embeddings as float32\n",
    "        chosen_files: list[str] of file paths used, length N_subset\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Scanning: {spec_dir}\")\n",
    "    all_files = list_audio_npy_files(spec_dir)\n",
    "    print(f\"[INFO] Found {len(all_files)} candidate .npy files\")\n",
    "    if len(all_files) == 0:\n",
    "        raise RuntimeError(f\"No .npy files found in {spec_dir}\")\n",
    "\n",
    "    n_sub = max(1, int(math.ceil(len(all_files) * subset_fraction)))\n",
    "    if subset_strategy == \"random\":\n",
    "        rng = np.random.RandomState(subset_seed)\n",
    "        chosen_idx = np.sort(rng.choice(len(all_files), size=n_sub, replace=False))\n",
    "    elif subset_strategy == \"tail\":\n",
    "        chosen_idx = np.arange(len(all_files) - n_sub, len(all_files))\n",
    "    elif subset_strategy == \"head\":\n",
    "        chosen_idx = np.arange(0, n_sub)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown subset_strategy: {subset_strategy}\")\n",
    "\n",
    "    chosen_files = [all_files[i] for i in chosen_idx]\n",
    "    print(f\"[INFO] Subset size: {len(chosen_files)} (fraction={subset_fraction})\")\n",
    "\n",
    "    device = torch.device(device_str or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    enc = _load_encoder(encoder_ckpt_path, device)\n",
    "\n",
    "    # ---- SAFE DataLoader (no multiprocessing, no persistence) ----\n",
    "    ds = TwoViewPrecomputed(\n",
    "        spec_dir,\n",
    "        T=256,\n",
    "        per_sample_norm=True,\n",
    "        sr=10000, n_mels=64, n_fft=1024, hop=512,\n",
    "        wave_policy=wave_policy,\n",
    "        file_paths=chosen_files\n",
    "    )\n",
    "    if len(ds) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No samples in dataset. Likely cause: wave_policy='skip' but files are 1-D waveforms. \"\n",
    "            \"Use wave_policy='convert' or precompute spectrograms.\"\n",
    "        )\n",
    "\n",
    "    loader_kwargs = dict(\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=n_workers,\n",
    "        pin_memory=(device.type == \"cuda\" and n_workers == 0),  # conservative\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    if _HAS_PAD_COL and pad_collate is not None:\n",
    "        loader_kwargs[\"collate_fn\"] = pad_collate\n",
    "\n",
    "    loader = DataLoader(ds, **loader_kwargs)\n",
    "    print(f\"[INFO] DataLoader ready: {len(ds)} samples • batch_size={batch_size} • workers={n_workers}\")\n",
    "\n",
    "    feats = []\n",
    "    use_cuda = (device.type == \"cuda\")\n",
    "    autocast_ctx = torch.cuda.amp.autocast if use_cuda else contextlib.nullcontext\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    n_seen = 0\n",
    "    print(f\"[INFO] Starting embedding pass on {device} (AMP={use_cuda})\")\n",
    "    with torch.inference_mode():\n",
    "        with autocast_ctx():\n",
    "            for step, (x1, _) in enumerate(loader):\n",
    "                n_seen += x1.size(0)\n",
    "                x = x1.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "                h, _ = enc(x, return_backbone=True)\n",
    "                feats.append(h.detach().to(\"cpu\", non_blocking=True).float())\n",
    "                if use_cuda:\n",
    "                    torch.cuda.synchronize()\n",
    "                if (step + 1) % 10 == 0 or (step == 0):\n",
    "                    dt = time.perf_counter() - t0\n",
    "                    print(f\"[INFO] Batches: {step+1} • Seen: {n_seen} • {n_seen/max(dt,1e-9):.1f} items/s\", flush=True)\n",
    "                del x, h\n",
    "        if use_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "    emb_time = time.perf_counter() - t0\n",
    "    print(f\"[INFO] Embedding done: {n_seen} items in {emb_time:.2f}s\")\n",
    "\n",
    "    H = torch.cat(feats, dim=0).numpy().astype(\"float32\")\n",
    "    return H, chosen_files\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Main analysis (2% subset) — now with spectrogram consistency\n",
    "# ------------------------------------------------------------------------------------\n",
    "def analyze_all_unsupervised_to_html(\n",
    "    encoder_ckpt_path,\n",
    "    dataset_paths,\n",
    "    labels_list,\n",
    "    cluster_method='kmeans',\n",
    "    n_clusters=20,\n",
    "    subset_fraction=0.02,\n",
    "    subset_seed=42,\n",
    "    subset_strategy=\"random\",\n",
    "    wave_policy=\"convert\",\n",
    "    # ---- NEW knobs for signal-space consistency ----\n",
    "    signal_consistency_mode: str = \"global\",   # \"off\" | \"global\" | \"per_cluster\"\n",
    "    global_consistency_cluster_sample: int = 20,\n",
    "    spec_cos_max_samples: int = 30,\n",
    "    spec_dtw_max_samples: int = 12,\n",
    "    compute_dtw: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds an HTML section with:\n",
    "      - UMAP 3D scatter\n",
    "      - Global internal/external clustering metrics\n",
    "      - (Optional) Global spectrogram-consistency summary across clusters\n",
    "      - Per-cluster cards (NO signal-space metrics unless signal_consistency_mode == 'per_cluster')\n",
    "\n",
    "    The heavy spectrogram pairwise work is skipped for per-cluster cards unless\n",
    "    signal_consistency_mode='per_cluster'. If 'global', a lightweight pass samples clusters\n",
    "    (and items per cluster) to compute an overall mean spec-cos (and optional DTW).\n",
    "    \"\"\"\n",
    "    # ---- Gather embeddings from each dataset (2% subset by default) ----\n",
    "    embeddings_all, loc_labels_all, class_labels_all, file_paths_all = [], [], [], []\n",
    "\n",
    "    for path, loc_label in zip(dataset_paths, labels_list):\n",
    "        H, chosen_files = extract_embeddings_from_specdir(\n",
    "            encoder_ckpt_path, path,\n",
    "            batch_size=128, n_workers=0,\n",
    "            subset_fraction=subset_fraction,\n",
    "            subset_seed=subset_seed,\n",
    "            subset_strategy=subset_strategy,\n",
    "            wave_policy=wave_policy\n",
    "        )\n",
    "        embeddings_all.append(H)\n",
    "        file_paths_all.extend(chosen_files)\n",
    "        loc_labels_all.extend([loc_label] * H.shape[0])\n",
    "\n",
    "        full_files_sorted = list_audio_npy_files(path)\n",
    "        cls_full = load_class_labels_if_any(path, count=len(full_files_sorted))\n",
    "        name_to_label = {os.path.basename(p): cls_full[i] for i, p in enumerate(full_files_sorted)}\n",
    "        class_labels_all.extend([name_to_label[os.path.basename(p)] for p in chosen_files])\n",
    "\n",
    "    embeddings = np.vstack(embeddings_all).astype(np.float32)\n",
    "    location_labels = np.array(loc_labels_all, dtype=object)\n",
    "    class_labels = np.array(class_labels_all, dtype=object)\n",
    "    original_paths = np.array(file_paths_all, dtype=object)\n",
    "\n",
    "    # ---- UMAP ----\n",
    "    print(f\"[INFO] Starting UMAP on {embeddings.shape[0]} items, dim={embeddings.shape[1]}\")\n",
    "    reducer = UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=42)\n",
    "    t_umap0 = time.perf_counter()\n",
    "    proj_3d = reducer.fit_transform(embeddings)\n",
    "    t_umap1 = time.perf_counter()\n",
    "    print(f\"[INFO] UMAP done in {t_umap1 - t_umap0:.2f}s\")\n",
    "\n",
    "    # ---- Clustering ----\n",
    "    print(f\"[INFO] Clustering with {cluster_method}, k={n_clusters}\")\n",
    "    t_cl0 = time.perf_counter()\n",
    "    cluster_labels, cluster_probs = choose_clusterer(cluster_method, embeddings, n_clusters)\n",
    "    t_cl1 = time.perf_counter()\n",
    "    print(f\"[INFO] Clustering done in {t_cl1 - t_cl0:.2f}s\")\n",
    "\n",
    "    # ---- Global clustering quality metrics (internal) ----\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    valid_for_metrics = (len(unique_clusters) > 1) and (embeddings.shape[0] > len(unique_clusters))\n",
    "\n",
    "    def _safe_metric(fn, X, y):\n",
    "        try:\n",
    "            return float(fn(X, y)) if valid_for_metrics else float(\"nan\")\n",
    "        except Exception:\n",
    "            return float(\"nan\")\n",
    "\n",
    "    from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "    sil = _safe_metric(silhouette_score, embeddings, cluster_labels)         # ↑ better\n",
    "    dbi = _safe_metric(davies_bouldin_score, embeddings, cluster_labels)     # ↓ better\n",
    "    ch  = _safe_metric(calinski_harabasz_score, embeddings, cluster_labels)  # ↑ better\n",
    "\n",
    "    # ---- External metrics vs labels.npy (if available) ----\n",
    "    def _safe_external(fn, y_true, y_pred):\n",
    "        try:\n",
    "            return float(fn(y_true, y_pred)) if valid_for_metrics else float(\"nan\")\n",
    "        except Exception:\n",
    "            return float(\"nan\")\n",
    "\n",
    "    from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "    ari  = _safe_external(adjusted_rand_score, class_labels, cluster_labels)         # ↑ better\n",
    "    ami  = _safe_external(adjusted_mutual_info_score, class_labels, cluster_labels)  # ↑ better\n",
    "\n",
    "    # Hungarian accuracy (already defined above)\n",
    "    hacc = _safe_external(hungarian_accuracy, class_labels, cluster_labels)          # ↑ better\n",
    "\n",
    "    print(f\"[INFO] Silhouette (↑): {sil if sil==sil else 'nan'}\")\n",
    "    print(f\"[INFO] Davies–Bouldin (↓): {dbi if dbi==dbi else 'nan'}\")\n",
    "    print(f\"[INFO] Calinski–Harabasz (↑): {ch if ch==ch else 'nan'}\")\n",
    "    print(f\"[INFO] ARI (↑): {ari if ari==ari else 'nan'}\")\n",
    "    print(f\"[INFO] AMI (↑): {ami if ami==ami else 'nan'}\")\n",
    "    print(f\"[INFO] Hungarian Accuracy (↑): {hacc if hacc==hacc else 'nan'}\")\n",
    "\n",
    "    title_txt = (\n",
    "        f\"Clustering with {cluster_method} \"\n",
    "        f\"(Sil={sil:.3f} | DBI={dbi:.3f} | CH={ch:.1f} | \"\n",
    "        f\"ARI={ari:.3f} | AMI={ami:.3f} | H-Acc={hacc:.3f})\"\n",
    "    )\n",
    "\n",
    "    umap_fig = px.scatter_3d(\n",
    "        x=proj_3d[:, 0], y=proj_3d[:, 1], z=proj_3d[:, 2],\n",
    "        color=[str(c) for c in cluster_labels],\n",
    "        symbol=location_labels,\n",
    "        hover_data={\"Cluster\": cluster_labels, \"Class\": class_labels},\n",
    "        title=title_txt,\n",
    "        opacity=0.85, height=800\n",
    "    )\n",
    "    umap_html = pio.to_html(umap_fig, include_plotlyjs=\"cdn\", full_html=False)\n",
    "\n",
    "    # ---- Per-cluster cards (embedding-space metrics only) ----\n",
    "    cluster_html = \"\"\n",
    "    cluster_blocks = []\n",
    "    base_for_entropy = len(set(location_labels))\n",
    "\n",
    "    # accumulators ONLY when showing per-cluster consistency\n",
    "    spec_cos_scores, spec_dtw_scores = [], []\n",
    "\n",
    "    for c in sorted(set(cluster_labels)):\n",
    "        idxs = np.where(cluster_labels == c)[0]\n",
    "        if idxs.size == 0:\n",
    "            continue\n",
    "\n",
    "        cluster_counts = Counter(location_labels[idxs])\n",
    "        class_counts = Counter(class_labels[idxs])\n",
    "\n",
    "        metrics = evaluate_cluster_metrics(\n",
    "            embeddings, idxs, location_labels, location_entropy_base=base_for_entropy\n",
    "        )\n",
    "\n",
    "        # ---- OPTIONAL per-cluster signal-space metrics ----\n",
    "        if signal_consistency_mode == \"per_cluster\":\n",
    "            spec_cos = _avg_intra_cluster_spec_cosine(\n",
    "                original_paths, idxs, fs=10000, max_samples=spec_cos_max_samples\n",
    "            )\n",
    "            spec_dtw = (\n",
    "                _avg_intra_cluster_spec_dtw(original_paths, idxs, fs=10000, max_samples=spec_dtw_max_samples)\n",
    "                if compute_dtw else float(\"nan\")\n",
    "            )\n",
    "            if np.isfinite(spec_cos): spec_cos_scores.append(spec_cos)\n",
    "            if np.isfinite(spec_dtw): spec_dtw_scores.append(spec_dtw)\n",
    "        else:\n",
    "            spec_cos, spec_dtw = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "        # ---- Build the HTML for this cluster ----\n",
    "        meta_html = \"<p><strong>Location Distribution:</strong></p><ul>\" + \"\".join(\n",
    "            f\"<li><b>{loc}</b>: {count} ({count/len(idxs):.1%})</li>\" for loc, count in cluster_counts.items()\n",
    "        ) + \"</ul>\"\n",
    "\n",
    "        meta_html += \"<p><strong>Class Distribution:</strong></p><ul>\" + \"\".join(\n",
    "            f\"<li>{cls}: {count}</li>\" for cls, count in class_counts.items()\n",
    "        ) + \"</ul>\"\n",
    "\n",
    "        meta_html += f\"\"\"\n",
    "        <p><strong>Cluster Metrics (Embedding Space):</strong></p>\n",
    "        <ul>\n",
    "            <li>Size: {len(idxs)}</li>\n",
    "            <li>Intra-Cluster Variance: {metrics['variance']:.4f}</li>\n",
    "            <li>Mean Cosine Similarity (Embeddings): {metrics['mean_sim']:.4f}</li>\n",
    "            <li>Location Entropy: {metrics['entropy']:.3f}</li>\n",
    "            <li>Composite Quality Score: {metrics['quality']:.4f}</li>\n",
    "            <li><strong>Novelty Score:</strong> {metrics['novelty']:.4f}</li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "\n",
    "        if signal_consistency_mode == \"per_cluster\":\n",
    "            meta_html += (\n",
    "                \"<p><strong>Spectrogram Consistency (Signal Space):</strong></p>\"\n",
    "                \"<ul>\"\n",
    "                f\"<li>Mean Spectrogram Cosine Similarity (↑ better): {spec_cos if np.isfinite(spec_cos) else float('nan'):.4f}</li>\"\n",
    "                f\"<li>Mean Spectrogram DTW Distance (↓ better): {spec_dtw if np.isfinite(spec_dtw) else float('nan'):.2f}</li>\"\n",
    "                \"</ul>\"\n",
    "            )\n",
    "\n",
    "        # Nearest-to-center exemplars (images)\n",
    "        center = np.mean(embeddings[idxs], axis=0, keepdims=True)\n",
    "        distances = np.linalg.norm(embeddings[idxs] - center, axis=1)\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        sampled_idxs = idxs[sorted_indices[:min(5, len(sorted_indices))]]\n",
    "\n",
    "        imgs = []\n",
    "        for i, chosen_idx in enumerate(sampled_idxs):\n",
    "            try:\n",
    "                x = np.load(original_paths[chosen_idx], mmap_mode=\"r\")\n",
    "                title = f\"#{i+1} | {location_labels[chosen_idx]} | Class {class_labels[chosen_idx]}\"\n",
    "                if x.ndim == 1:\n",
    "                    imgs.append(generate_spectrogram_base64(x.astype(np.float32), title=title))\n",
    "                else:\n",
    "                    S = x.squeeze(0)\n",
    "                    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "                    ax.imshow(S, aspect='auto', origin='lower', cmap='viridis')\n",
    "                    ax.set_title(title)\n",
    "                    ax.set_xlabel(\"Frames\"); ax.set_ylabel(\"Mel bins\")\n",
    "                    fig.tight_layout()\n",
    "                    buf = BytesIO(); fig.savefig(buf, format='png'); plt.close(fig); buf.seek(0)\n",
    "                    image_base64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "                    imgs.append(f'<img class=\"d-block w-100\" src=\"data:image/png;base64,{image_base64}\" alt=\"Spec\">')\n",
    "            except Exception as e:\n",
    "                imgs.append(f\"<p class='text-danger'>Error loading sample {i+1}: {e}</p>\")\n",
    "\n",
    "        carousel_html = make_carousel(c, \"all\", imgs)\n",
    "        block = f\"<div class='col-md-6 mb-4'><h4>Cluster {c}</h4>{meta_html}{carousel_html}</div>\"\n",
    "        cluster_blocks.append(block)\n",
    "\n",
    "    for i in range(0, len(cluster_blocks), 2):\n",
    "        cluster_html += \"<div class='row'>\" + \"\".join(cluster_blocks[i:i+2]) + \"</div>\"\n",
    "\n",
    "    # ---- Global spectrogram-consistency summary across clusters ----\n",
    "    def _consistency_for_idxs(idxs):\n",
    "        cos = _avg_intra_cluster_spec_cosine(\n",
    "            original_paths, idxs, fs=10000, max_samples=spec_cos_max_samples\n",
    "        )\n",
    "        dtw = (\n",
    "            _avg_intra_cluster_spec_dtw(original_paths, idxs, fs=10000, max_samples=spec_dtw_max_samples)\n",
    "            if compute_dtw else float(\"nan\")\n",
    "        )\n",
    "        return cos, dtw\n",
    "\n",
    "    if signal_consistency_mode == \"per_cluster\":\n",
    "        global_spec_cos = float(np.nanmean(spec_cos_scores)) if len(spec_cos_scores) else float(\"nan\")\n",
    "        global_spec_dtw = float(np.nanmean(spec_dtw_scores)) if len(spec_dtw_scores) else float(\"nan\")\n",
    "    elif signal_consistency_mode == \"global\":\n",
    "        rng = np.random.RandomState(42)\n",
    "        all_clusters = sorted(set(cluster_labels))\n",
    "        if isinstance(global_consistency_cluster_sample, int) and global_consistency_cluster_sample > 0:\n",
    "            sampled_clusters = rng.choice(\n",
    "                all_clusters,\n",
    "                size=min(global_consistency_cluster_sample, len(all_clusters)),\n",
    "                replace=False\n",
    "            )\n",
    "        else:\n",
    "            sampled_clusters = all_clusters\n",
    "\n",
    "        cos_list, dtw_list = [], []\n",
    "        for c in sampled_clusters:\n",
    "            idxs = np.where(cluster_labels == c)[0]\n",
    "            if len(idxs) < 2:\n",
    "                continue\n",
    "            cos, dtw = _consistency_for_idxs(idxs)\n",
    "            if np.isfinite(cos): cos_list.append(cos)\n",
    "            if np.isfinite(dtw): dtw_list.append(dtw)\n",
    "        global_spec_cos = float(np.nanmean(cos_list)) if len(cos_list) else float(\"nan\")\n",
    "        global_spec_dtw = float(np.nanmean(dtw_list)) if len(dtw_list) else float(\"nan\")\n",
    "    else:\n",
    "        global_spec_cos, global_spec_dtw = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    summary_card = f\"\"\"\n",
    "    <div class='row mb-4'>\n",
    "      <div class='col-md-12'>\n",
    "        <div class='alert alert-info' role='alert'>\n",
    "          <h5 class='mb-2'>Spectrogram Consistency (Cluster Averages)</h5>\n",
    "          <ul class='mb-0'>\n",
    "            <li><strong>Mean Spectrogram Cosine Similarity</strong>: {global_spec_cos if np.isfinite(global_spec_cos) else float('nan'):.4f} (↑ better)</li>\n",
    "            <li><strong>Mean Spectrogram DTW Distance</strong>: {global_spec_dtw if np.isfinite(global_spec_dtw) else float('nan'):.2f} (↓ better)</li>\n",
    "          </ul>\n",
    "          <small>Note: Cosine on standardized spectrograms (z-norm, padded/cropped to common shape). DTW optional and subsampled. Per-cluster consistency is omitted unless mode='per_cluster'.</small>\n",
    "        </div>\n",
    "      </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Final section HTML ----\n",
    "    return f\"\"\"\n",
    "    <div class='section'>\n",
    "      <h2>{cluster_method.capitalize()} Clustering Analysis ({int(subset_fraction*100)}% subset)</h2>\n",
    "      <div class=\"container\">\n",
    "        <div class=\"row justify-content-center mb-4\">\n",
    "          <div class=\"col-md-12 d-flex justify-content-center\">{umap_html}</div>\n",
    "        </div>\n",
    "        {summary_card if signal_consistency_mode in (\"global\", \"per_cluster\") else \"\"}\n",
    "      </div>\n",
    "      {cluster_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Report generator\n",
    "# ------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Report generator (UPDATED)\n",
    "# ------------------------------------------------------------------------------------\n",
    "def generate_full_report(\n",
    "    encoder_ckpt_path,\n",
    "    dataset_paths,\n",
    "    labels_list,\n",
    "    cluster_method='kmeans',\n",
    "    n_clusters=20,\n",
    "    out_prefix=\"unsup_report\",\n",
    "    subset_fraction=0.02,\n",
    "    subset_seed=42,\n",
    "    subset_strategy=\"random\",\n",
    "    wave_policy=\"convert\",\n",
    "    # --- NEW passthrough args for signal-space consistency ---\n",
    "    signal_consistency_mode: str = \"global\",       # \"off\" | \"global\" | \"per_cluster\"\n",
    "    global_consistency_cluster_sample: int | None = 20,  # None = use ALL clusters\n",
    "    spec_cos_max_samples: int = 30,                # items per cluster for cosine\n",
    "    spec_dtw_max_samples: int = 12,                # items per cluster for DTW\n",
    "    compute_dtw: bool = False,                     # set True to include DTW\n",
    "):\n",
    "    section_html = analyze_all_unsupervised_to_html(\n",
    "        encoder_ckpt_path=encoder_ckpt_path,\n",
    "        dataset_paths=list(dataset_paths),\n",
    "        labels_list=list(labels_list),\n",
    "        cluster_method=cluster_method,\n",
    "        n_clusters=int(n_clusters),\n",
    "        subset_fraction=subset_fraction,\n",
    "        subset_seed=subset_seed,\n",
    "        subset_strategy=subset_strategy,\n",
    "        wave_policy=wave_policy,\n",
    "        # --- forward the new args ---\n",
    "        signal_consistency_mode=signal_consistency_mode,\n",
    "        global_consistency_cluster_sample=global_consistency_cluster_sample,\n",
    "        spec_cos_max_samples=spec_cos_max_samples,\n",
    "        spec_dtw_max_samples=spec_dtw_max_samples,\n",
    "        compute_dtw=compute_dtw,\n",
    "    )\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Unsupervised Clustering Report ({int(subset_fraction*100)}% subset)</title>\n",
    "        <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; padding: 20px; background-color: #f5f5f5; }}\n",
    "            h1 {{ color: #2c3e50; }}\n",
    "            h2, h4 {{ color: #34495e; }}\n",
    "            hr {{ border-top: 2px solid #bbb; margin-top: 40px; margin-bottom: 40px; }}\n",
    "            .section {{ margin-bottom: 60px; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "      <h1 class='mb-4'>Unsupervised Latent Clustering Report ({int(subset_fraction*100)}% subset)</h1>\n",
    "      <p><strong>Compared Locations:</strong> {', '.join(labels_list)}</p>\n",
    "      <p><strong>Subset:</strong> {int(subset_fraction*100)}% • Strategy: {subset_strategy} • Seed: {subset_seed}</p>\n",
    "      <hr>\n",
    "      {section_html}\n",
    "    </body></html>\n",
    "    \"\"\"\n",
    "    output_path = f\"{out_prefix}_{cluster_method}_subset{int(subset_fraction*100)}.html\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Example usage\n",
    "# ------------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # !!! IMPORTANT: Ensure EncoderProj is importable (see the try/except at top) !!!\n",
    "    encoder_ckpt_path = \"reef_ssl_precomp_ckpt.pth\"\n",
    "\n",
    "    dataset_paths = [\n",
    "        #\"/notebooks/2017_mona_elbow_U1276\"\n",
    "        \"/notebooks/MX_PA_2022_preprocessed\"\n",
    "    ]\n",
    "    labels_list = [\"MX_PA_2022_preprocessed\"] #, \"mona_U1274\"]\n",
    "\n",
    "    report_path = generate_full_report(\n",
    "        encoder_ckpt_path=encoder_ckpt_path,\n",
    "        dataset_paths=dataset_paths,\n",
    "        labels_list=labels_list,\n",
    "        cluster_method='kmeans',\n",
    "        n_clusters=60,\n",
    "        out_prefix=\"PR_k60\",\n",
    "        subset_fraction=1,\n",
    "        subset_seed=45,\n",
    "        subset_strategy=\"random\",\n",
    "        wave_policy=\"convert\",\n",
    "\n",
    "        # --- keep only the global summary, computed like before ---\n",
    "        signal_consistency_mode=\"global\",       # no per-cluster section, keep global card\n",
    "        global_consistency_cluster_sample=None, # use ALL clusters (no sampling)\n",
    "        spec_cos_max_samples=50,                # match old per-cluster cap for cosine\n",
    "        spec_dtw_max_samples=25,                # match old per-cluster cap for DTW\n",
    "        compute_dtw=True                        # DTW was on before; set False to speed up\n",
    "    )\n",
    "\n",
    "    print(\"✅ Report saved to:\", report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive Report\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T21:56:22.716132Z",
     "iopub.status.busy": "2025-09-06T21:56:22.715652Z",
     "iopub.status.idle": "2025-09-06T22:03:54.261362Z",
     "shell.execute_reply": "2025-09-06T22:03:54.260669Z",
     "shell.execute_reply.started": "2025-09-06T21:56:22.716112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Could not import from your_training_file: No module named 'your_training_file'\n",
      "[INFO] Scanning: /notebooks/dataset_preprocessed\n",
      "[INFO] Found 413272 candidate .npy files\n",
      "[INFO] Subset size: 8266 (fraction=0.02)\n",
      "[INFO] Encoder loaded on cuda • CUDA available: True\n",
      "[INFO] DataLoader ready: 8266 samples • batch_size=128 • workers=0\n",
      "[INFO] Starting embedding pass on cuda (AMP=True)\n",
      "[INFO] Batches: 1 • Seen: 128 • 16.9 items/s\n",
      "[INFO] Batches: 10 • Seen: 1280 • 27.2 items/s\n",
      "[INFO] Batches: 20 • Seen: 2560 • 39.0 items/s\n",
      "[INFO] Batches: 30 • Seen: 3840 • 47.0 items/s\n",
      "[INFO] Batches: 40 • Seen: 5120 • 55.8 items/s\n",
      "[INFO] Batches: 50 • Seen: 6400 • 62.2 items/s\n",
      "[INFO] Batches: 60 • Seen: 7680 • 63.7 items/s\n",
      "[INFO] Embedding done: 8266 items in 127.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] UMAP done in 9.83s\n",
      "[INFO] Clustering with kmeans, k=40\n",
      "[INFO] Silhouette (↑): 0.2531849145889282\n",
      "[INFO] Davies–Bouldin (↓): 1.229338827427351\n",
      "[INFO] Calinski–Harabasz (↑): 1968.1967613655538\n",
      "[INFO] ARI (↑): 0.06729750079531047\n",
      "[INFO] AMI (↑): 0.17398020326790878\n",
      "[INFO] Hungarian Accuracy (↑): 0.15073796273893056\n",
      "✅ Report saved to: SimCLR_kmeans_k60_kmeans_subset2.html\n"
     ]
    }
   ],
   "source": [
    "# ===== Unsupervised Report (2% subset, safe mode, enhanced) =====\n",
    "# Adds: L2-normalized clustering, UMAP-by-cluster & by-GT,\n",
    "# k-scan (Sil/DBI/CH), stability vs seed, retrieval@k,\n",
    "# per-cluster silhouette & centroid-distance violins,\n",
    "# and your spectrogram-consistency metrics + exemplar carousels.\n",
    "#\n",
    "# Usage:\n",
    "#   output_path = generate_full_report(\n",
    "#       encoder_ckpt_path=\"...pth\",\n",
    "#       dataset_paths=[\"/path/to/spec_or_wave_npy_dir\"],\n",
    "#       labels_list=[\"SITE1\"],    # label per dataset_paths entry\n",
    "#       cluster_method=\"kmeans\",  # 'kmeans'|'gmm'|'agglomerative'\n",
    "#       n_clusters=60,\n",
    "#       out_prefix=\"unsup_report\",\n",
    "#       subset_fraction=0.02, subset_seed=42, subset_strategy=\"random\",\n",
    "#       wave_policy=\"convert\",    # 'convert' if files are waveforms; 'skip' if already (F,T)\n",
    "#   )\n",
    "\n",
    "import os, math, contextlib, base64, time, io\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "from io import BytesIO\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, fowlkes_mallows_score, silhouette_samples\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# plotting / viz\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import entropy\n",
    "from scipy.signal import spectrogram, get_window\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ------------------------- small helpers -------------------------\n",
    "def _fig_to_img_html(fig) -> str:\n",
    "    \"\"\"Convert a Matplotlib figure to an inline <img> (base64 PNG).\"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(buf, format=\"png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    b64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return f'<img class=\"img-fluid\" src=\"data:image/png;base64,{b64}\" alt=\"plot\">'\n",
    "\n",
    "\n",
    "def hungarian_accuracy(y_true, y_pred):\n",
    "    \"\"\"Max bipartite matching accuracy between predicted cluster IDs and true labels.\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    true_ids = {t: i for i, t in enumerate(np.unique(y_true))}\n",
    "    pred_ids = {c: i for i, c in enumerate(np.unique(y_pred))}\n",
    "    y_true_i = np.vectorize(true_ids.get)(y_true)\n",
    "    y_pred_i = np.vectorize(pred_ids.get)(y_pred)\n",
    "    W = np.zeros((len(pred_ids), len(true_ids)), dtype=np.int64)\n",
    "    for i in range(y_pred_i.size):\n",
    "        W[y_pred_i[i], y_true_i[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(W.max() - W)\n",
    "    return float(W[row_ind, col_ind].sum() / y_pred_i.size)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Import your model + (optional) pad_collate from training code\n",
    "# ------------------------------------------------------------------------------------\n",
    "# !!! IMPORTANT: change 'your_training_file' to your actual module name !!!\n",
    "try:\n",
    "    from your_training_file import EncoderProj, pad_collate  # noqa\n",
    "    _HAS_PAD_COL = True\n",
    "    print(\"[INFO] Imported EncoderProj and pad_collate from your_training_file.\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Could not import from your_training_file: {e}\")\n",
    "    _HAS_PAD_COL = False\n",
    "    try:\n",
    "        EncoderProj  # type: ignore\n",
    "    except NameError:\n",
    "        raise RuntimeError(\n",
    "            \"EncoderProj is not importable. Provide it via:\\n\"\n",
    "            \"from <your_module> import EncoderProj  # (and optionally pad_collate)\"\n",
    "        )\n",
    "    if 'pad_collate' not in globals():\n",
    "        pad_collate = None  # type: ignore\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Dataset for precomputed spectrograms or raw waveforms (.npy files)\n",
    "# ------------------------------------------------------------------------------------\n",
    "def _wave_to_spec_1xFxT(y_np: np.ndarray, sr: int = 10000, n_mels: int = 64,\n",
    "                        n_fft: int = 1024, hop_length: int = 512) -> torch.Tensor:\n",
    "    y_np = np.nan_to_num(y_np.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    mx = np.max(np.abs(y_np)); mx = 1.0 if (not np.isfinite(mx) or mx < 1e-8) else mx\n",
    "    y_t = torch.from_numpy(y_np / mx).unsqueeze(0)  # (1, T)\n",
    "    try:\n",
    "        import torchaudio as ta\n",
    "        mel = ta.transforms.MelSpectrogram(sample_rate=sr, n_fft=n_fft,\n",
    "                                           hop_length=hop_length, n_mels=n_mels)\n",
    "        S = mel(y_t)                                # (1, F, T)\n",
    "        S = torch.clamp(S, min=1e-10).log()\n",
    "        return S.contiguous().float()\n",
    "    except Exception:\n",
    "        from scipy.signal import spectrogram, get_window\n",
    "        win = get_window('hann', n_fft, fftbins=True)\n",
    "        f, t, Z = spectrogram(y_np, fs=sr, window=win, nperseg=n_fft,\n",
    "                              noverlap=n_fft - hop_length, mode='magnitude')\n",
    "        S = np.log(np.maximum(Z, 1e-10)).astype(np.float32)  # (F_lin, T)\n",
    "        if S.shape[0] != n_mels:\n",
    "            idx = np.linspace(0, S.shape[0]-1, num=n_mels).astype(np.int32)\n",
    "            S = S[idx]\n",
    "        S = torch.from_numpy(S).unsqueeze(0)\n",
    "        return S.contiguous().float()\n",
    "\n",
    "\n",
    "class TwoViewPrecomputed(Dataset):\n",
    "    def __init__(self, spec_dir, T=256, per_sample_norm=True,\n",
    "                 sr=10000, n_mels=64, n_fft=1024, hop=512,\n",
    "                 wave_policy=\"convert\",   # \"convert\" or \"skip\"\n",
    "                 file_paths=None):\n",
    "        if file_paths is not None:\n",
    "            self.paths = list(file_paths)\n",
    "        else:\n",
    "            paths = []\n",
    "            for f in os.listdir(spec_dir):\n",
    "                if f.endswith(\".npy\") and f != \"labels.npy\":\n",
    "                    stem = os.path.splitext(f)[0]\n",
    "                    try: key = int(stem)\n",
    "                    except ValueError: key = stem\n",
    "                    paths.append((key, os.path.join(spec_dir, f)))\n",
    "            paths.sort(key=lambda t: t[0])\n",
    "            self.paths = [p for _, p in paths]\n",
    "\n",
    "        self.T = T\n",
    "        self.per_sample_norm = per_sample_norm\n",
    "        self.sr, self.n_mels, self.n_fft, self.hop = sr, n_mels, n_fft, hop\n",
    "        self.wave_policy = wave_policy\n",
    "\n",
    "        if self.wave_policy == \"skip\":\n",
    "            kept = []\n",
    "            for p in self.paths:\n",
    "                arr = np.load(p, mmap_mode=\"r\")\n",
    "                if arr.ndim == 1:\n",
    "                    continue\n",
    "                kept.append(p)\n",
    "            self.paths = kept\n",
    "\n",
    "    def _to_1xFxT(self, arr: np.ndarray) -> torch.Tensor:\n",
    "        arr = np.array(arr, copy=True)\n",
    "        if arr.ndim == 1:\n",
    "            if self.wave_policy == \"skip\":\n",
    "                raise RuntimeError(\"Waveform encountered but wave_policy='skip'.\")\n",
    "            return _wave_to_spec_1xFxT(arr, sr=self.sr, n_mels=self.n_mels,\n",
    "                                       n_fft=self.n_fft, hop_length=self.hop)\n",
    "        x = torch.from_numpy(arr)\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        elif x.ndim == 3:\n",
    "            if x.shape[0] != 1:\n",
    "                x = x.reshape(1, x.shape[0]*x.shape[1], x.shape[2])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected array shape {tuple(x.shape)}\")\n",
    "        return x.contiguous().float()\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat_np = np.load(self.paths[idx], mmap_mode=\"r\")\n",
    "        feat = self._to_1xFxT(feat_np)\n",
    "        if self.per_sample_norm:\n",
    "            m, s = feat.mean(), feat.std()\n",
    "            s = s if (torch.isfinite(s) and s >= 1e-6) else torch.tensor(1e-6, device=feat.device)\n",
    "            feat = (feat - m) / s\n",
    "        x1 = feat\n",
    "        x2 = feat.clone()\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# File utilities and viz helpers\n",
    "# ------------------------------------------------------------------------------------\n",
    "def list_audio_npy_files(folder):\n",
    "    files = []\n",
    "    for f in os.listdir(folder):\n",
    "        if f.endswith(\".npy\") and f != \"labels.npy\":\n",
    "            stem = os.path.splitext(f)[0]\n",
    "            try: key = int(stem)\n",
    "            except ValueError: key = stem\n",
    "            files.append((key, os.path.join(folder, f)))\n",
    "    files.sort(key=lambda t: t[0])\n",
    "    return [p for _, p in files]\n",
    "\n",
    "def load_class_labels_if_any(folder, count):\n",
    "    lbl_path = os.path.join(folder, \"labels.npy\")\n",
    "    if os.path.isfile(lbl_path):\n",
    "        try:\n",
    "            arr = np.load(lbl_path, allow_pickle=True)\n",
    "            if len(arr) < count:\n",
    "                pad = np.array([\"Unknown\"] * (count - len(arr)), dtype=object)\n",
    "                arr = np.concatenate([arr, pad], axis=0)\n",
    "            elif len(arr) > count:\n",
    "                arr = arr[:count]\n",
    "            return arr\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.array([\"Unknown\"] * count, dtype=object)\n",
    "\n",
    "def ali_spec(x: np.ndarray, fs: int):\n",
    "    Lframe2 = 1000\n",
    "    po = 80\n",
    "    lov = int(np.ceil((po / 100) * Lframe2))\n",
    "    taper = get_window('hann', Lframe2)\n",
    "    Nfft = 2 ** (int(np.floor(np.log2(Lframe2))) + 2)\n",
    "    f, t, s = spectrogram(x, fs=fs, window=taper, noverlap=lov, nfft=Nfft, mode='complex')\n",
    "    as_ = np.abs(s)\n",
    "    as_max = np.max(as_) if np.isfinite(np.max(as_)) and np.max(as_) > 0 else 1.0\n",
    "    sdb = 10 * np.log10(100 * as_ / as_max + 1e-10)\n",
    "    min_inx = np.argmin(np.abs(f - 0))\n",
    "    max_inx = np.argmin(np.abs(f - 800))\n",
    "    return sdb[min_inx:max_inx+1, :], f[min_inx:max_inx+1], t\n",
    "\n",
    "def generate_spectrogram_base64(audio, fs=10000, title=\"Spectrogram\"):\n",
    "    spec, f_axis, t_axis = ali_spec(audio, fs)\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.imshow(spec, aspect='auto', origin='lower',\n",
    "              extent=[t_axis[0], t_axis[-1], f_axis[0], f_axis[-1]], cmap='hsv')\n",
    "    ax.set_title(title); ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    html = _fig_to_img_html(fig)\n",
    "    return html\n",
    "\n",
    "def make_carousel(cluster_id, scope, imgs):\n",
    "    cid = f\"carousel_{scope}_{cluster_id}\"\n",
    "    indicators = \"\".join(\n",
    "        f'<button type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide-to=\"{i}\" {\"class=active\" if i==0 else \"\"} aria-current=\"true\" aria-label=\"Slide {i+1}\"></button>'\n",
    "        for i in range(len(imgs))\n",
    "    )\n",
    "    items = \"\".join(\n",
    "        f'<div class=\"carousel-item {\"active\" if i==0 else \"\"}\"><div class=\"d-flex justify-content-center\">{img}</div></div>'\n",
    "        for i, img in enumerate(imgs)\n",
    "    )\n",
    "    return f\"\"\"\n",
    "    <div id=\"{cid}\" class=\"carousel slide\" data-bs-interval=\"false\" data-bs-touch=\"false\">\n",
    "      <div class=\"carousel-indicators\">{indicators}</div>\n",
    "      <div class=\"carousel-inner\">{items}</div>\n",
    "      <button class=\"carousel-control-prev\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"prev\">\n",
    "        <span class=\"carousel-control-prev-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Previous</span>\n",
    "      </button>\n",
    "      <button class=\"carousel-control-next\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"next\">\n",
    "        <span class=\"carousel-control-next-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Next</span>\n",
    "      </button>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Spectrogram standardization & intra-cluster consistency metrics\n",
    "# ------------------------------------------------------------------------------------\n",
    "def _standardize_spec(S: np.ndarray, target_shape=(128, 256)) -> np.ndarray:\n",
    "    S = np.asarray(S, dtype=np.float32)\n",
    "    S = (S - np.mean(S)) / (np.std(S) + 1e-8)\n",
    "    F, T = S.shape\n",
    "    Ft, Tt = target_shape\n",
    "    if F < Ft:\n",
    "        pad_top = (Ft - F) // 2\n",
    "        pad_bot = Ft - F - pad_top\n",
    "        S = np.pad(S, ((pad_top, pad_bot), (0, 0)), mode=\"constant\")\n",
    "    elif F > Ft:\n",
    "        start = (F - Ft) // 2\n",
    "        S = S[start:start+Ft, :]\n",
    "    F, T = S.shape\n",
    "    if T < Tt:\n",
    "        pad_left = (Tt - T) // 2\n",
    "        pad_right = Tt - T - pad_left\n",
    "        S = np.pad(S, ((0, 0), (pad_left, pad_right)), mode=\"constant\")\n",
    "    elif T > Tt:\n",
    "        start = (T - Tt) // 2\n",
    "        S = S[:, start:start+Tt]\n",
    "    return S\n",
    "\n",
    "def _load_spec_for_index(original_paths: np.ndarray, idx: int, fs: int = 10000,\n",
    "                         target_shape=(128, 256)) -> Optional[np.ndarray]:\n",
    "    try:\n",
    "        arr = np.load(original_paths[idx], mmap_mode=\"r\")\n",
    "        if arr.ndim == 1:\n",
    "            S, _, _ = ali_spec(arr.astype(np.float32), fs=fs)\n",
    "        elif arr.ndim == 2:\n",
    "            S = arr.astype(np.float32)\n",
    "        else:\n",
    "            return None\n",
    "        return _standardize_spec(S, target_shape=target_shape)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _avg_intra_cluster_spec_cosine(original_paths: np.ndarray, idxs: np.ndarray,\n",
    "                                   *, fs: int = 10000, max_samples: int = 50,\n",
    "                                   target_shape=(128, 256)) -> float:\n",
    "    if len(idxs) < 2:\n",
    "        return float(\"nan\")\n",
    "    if len(idxs) > max_samples:\n",
    "        rng = np.random.RandomState(42)\n",
    "        idxs = rng.choice(idxs, size=max_samples, replace=False)\n",
    "    specs = []\n",
    "    for i in idxs:\n",
    "        S = _load_spec_for_index(original_paths, int(i), fs=fs, target_shape=target_shape)\n",
    "        if S is not None:\n",
    "            specs.append(S.reshape(-1))\n",
    "    if len(specs) < 2:\n",
    "        return float(\"nan\")\n",
    "    X = np.stack(specs, axis=0)\n",
    "    sim = cosine_similarity(X)\n",
    "    iu = np.triu_indices_from(sim, k=1)\n",
    "    return float(np.mean(sim[iu]))\n",
    "\n",
    "def _avg_intra_cluster_spec_dtw(original_paths: np.ndarray, idxs: np.ndarray,\n",
    "                                *, fs: int = 10000, max_samples: int = 25,\n",
    "                                target_shape=(128, 256)) -> float:\n",
    "    try:\n",
    "        from fastdtw import fastdtw\n",
    "        from scipy.spatial.distance import euclidean\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "    if len(idxs) < 2:\n",
    "        return float(\"nan\")\n",
    "    if len(idxs) > max_samples:\n",
    "        rng = np.random.RandomState(42)\n",
    "        idxs = rng.choice(idxs, size=max_samples, replace=False)\n",
    "    series = []\n",
    "    for i in idxs:\n",
    "        S = _load_spec_for_index(original_paths, int(i), fs=fs, target_shape=target_shape)\n",
    "        if S is not None:\n",
    "            series.append(np.mean(S, axis=0))\n",
    "    if len(series) < 2:\n",
    "        return float(\"nan\")\n",
    "    n = len(series); total, pairs = 0.0, 0\n",
    "    for a in range(n):\n",
    "        for b in range(a+1, n):\n",
    "            d, _ = fastdtw(series[a], series[b], dist=euclidean)\n",
    "            total += d; pairs += 1\n",
    "    return float(total / max(pairs, 1))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Clustering and metrics\n",
    "# ------------------------------------------------------------------------------------\n",
    "def choose_clusterer(algorithm: str, embeddings: np.ndarray, n_clusters: int, seed: int = 42):\n",
    "    if algorithm == 'kmeans':\n",
    "        clusterer = KMeans(n_clusters=n_clusters, n_init=20, random_state=seed).fit(embeddings)\n",
    "        return clusterer.labels_, None\n",
    "    elif algorithm == 'agglomerative':\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n_clusters).fit(embeddings)\n",
    "        return clusterer.labels_, None\n",
    "    elif algorithm == 'gmm':\n",
    "        clusterer = GaussianMixture(n_components=n_clusters, covariance_type='diag', random_state=seed, max_iter=300).fit(embeddings)\n",
    "        return clusterer.predict(embeddings), clusterer.predict_proba(embeddings)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported clustering algorithm: {algorithm}\")\n",
    "\n",
    "def evaluate_cluster_metrics(embeddings, idxs, location_labels, location_entropy_base=None):\n",
    "    X = embeddings[idxs]\n",
    "    if X.shape[0] == 0:\n",
    "        return {'variance': 0.0, 'mean_sim': 1.0, 'entropy': 0.0, 'quality': 0.0, 'novelty': 0.0}\n",
    "    center = np.mean(X, axis=0, keepdims=True)\n",
    "    variance = float(np.mean(np.sum((X - center) ** 2, axis=1)))\n",
    "    if len(X) > 1:\n",
    "        cos_sim = cosine_similarity(X)\n",
    "        iu = np.triu_indices_from(cos_sim, k=1)\n",
    "        mean_sim = float(np.mean(cos_sim[iu])) if iu[0].size > 0 else 1.0\n",
    "    else:\n",
    "        mean_sim = 1.0\n",
    "    loc_counts = Counter(location_labels[idxs])\n",
    "    loc_probs = np.array(list(loc_counts.values()), dtype=np.float32)\n",
    "    loc_probs /= max(loc_probs.sum(), 1e-8)\n",
    "    base = int(location_entropy_base or len(set(location_labels)))\n",
    "    loc_entropy = float(entropy(loc_probs, base=base)) if base > 1 else 0.0\n",
    "    max_ent = np.log2(base) if base > 1 else 1.0\n",
    "    entropy_score = 1.0 - (loc_entropy / max_ent) if base > 1 else 1.0\n",
    "    quality = float((mean_sim / (variance + 1e-8)) * entropy_score)\n",
    "    novelty = float((loc_entropy / max_ent) * variance) if base > 1 else 0.0\n",
    "    return {'variance': variance, 'mean_sim': mean_sim, 'entropy': loc_entropy, 'quality': quality, 'novelty': novelty}\n",
    "\n",
    "def _safe_metric(fn, *a, **k) -> float:\n",
    "    try:\n",
    "        return float(fn(*a, **k))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def _coassign_jaccard(y1: np.ndarray, y2: np.ndarray) -> float:\n",
    "    \"\"\"Jaccard of same-cluster co-assignment (on the same index set).\"\"\"\n",
    "    n = len(y1)\n",
    "    inter = 0; union = 0\n",
    "    for i in range(n):\n",
    "        yi = y1[i]\n",
    "        for j in range(i+1, n):\n",
    "            a = (yi == y1[j]); b = (y2[i] == y2[j])\n",
    "            inter += int(a and b); union += int(a or b)\n",
    "    return float(inter / max(union, 1))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Embedding extraction\n",
    "# ------------------------------------------------------------------------------------\n",
    "def _load_encoder(encoder_ckpt_path, device):\n",
    "    enc = EncoderProj(proj_dim=256).to(device).to(memory_format=torch.channels_last)\n",
    "    ckpt = torch.load(encoder_ckpt_path, map_location=device)\n",
    "    if isinstance(ckpt, dict) and \"online\" in ckpt:\n",
    "        state_dict = ckpt[\"online\"]\n",
    "    elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "        state_dict = ckpt[\"state_dict\"]\n",
    "    else:\n",
    "        state_dict = ckpt\n",
    "    new_state = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace(\"encoder.\", \"\").replace(\"module.\", \"\")\n",
    "        new_state[new_key] = v\n",
    "    enc.load_state_dict(new_state, strict=False)\n",
    "    enc.eval()\n",
    "    print(f\"[INFO] Encoder loaded on {device} • CUDA available: {torch.cuda.is_available()}\")\n",
    "    return enc\n",
    "\n",
    "def extract_embeddings_from_specdir(\n",
    "    encoder_ckpt_path,\n",
    "    spec_dir,\n",
    "    batch_size=128,\n",
    "    n_workers=0,          # SAFE: 0 workers to avoid hangs for first run\n",
    "    device_str=None,\n",
    "    subset_fraction=0.02, # 2% subset\n",
    "    subset_seed=42,\n",
    "    subset_strategy=\"random\",  # \"random\" | \"tail\" | \"head\"\n",
    "    wave_policy=\"convert\"      # \"convert\" for waveforms; \"skip\" if files are (F,T) specs\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        H: (N_subset, D) embeddings as float32\n",
    "        chosen_files: list[str] of file paths used, length N_subset\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Scanning: {spec_dir}\")\n",
    "    all_files = list_audio_npy_files(spec_dir)\n",
    "    print(f\"[INFO] Found {len(all_files)} candidate .npy files\")\n",
    "    if len(all_files) == 0:\n",
    "        raise RuntimeError(f\"No .npy files found in {spec_dir}\")\n",
    "\n",
    "    n_sub = max(1, int(math.ceil(len(all_files) * subset_fraction)))\n",
    "    if subset_strategy == \"random\":\n",
    "        rng = np.random.RandomState(subset_seed)\n",
    "        chosen_idx = np.sort(rng.choice(len(all_files), size=n_sub, replace=False))\n",
    "    elif subset_strategy == \"tail\":\n",
    "        chosen_idx = np.arange(len(all_files) - n_sub, len(all_files))\n",
    "    elif subset_strategy == \"head\":\n",
    "        chosen_idx = np.arange(0, n_sub)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown subset_strategy: {subset_strategy}\")\n",
    "\n",
    "    chosen_files = [all_files[i] for i in chosen_idx]\n",
    "    print(f\"[INFO] Subset size: {len(chosen_files)} (fraction={subset_fraction})\")\n",
    "\n",
    "    device = torch.device(device_str or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    enc = _load_encoder(encoder_ckpt_path, device)\n",
    "\n",
    "    ds = TwoViewPrecomputed(\n",
    "        spec_dir,\n",
    "        T=256,\n",
    "        per_sample_norm=True,\n",
    "        sr=10000, n_mels=64, n_fft=1024, hop=512,\n",
    "        wave_policy=wave_policy,\n",
    "        file_paths=chosen_files\n",
    "    )\n",
    "    if len(ds) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No samples in dataset. Likely cause: wave_policy='skip' but files are 1-D waveforms. \"\n",
    "            \"Use wave_policy='convert' or precompute spectrograms.\"\n",
    "        )\n",
    "\n",
    "    loader_kwargs = dict(\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=n_workers,\n",
    "        pin_memory=(device.type == \"cuda\" and n_workers == 0),\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    if _HAS_PAD_COL and pad_collate is not None:\n",
    "        loader_kwargs[\"collate_fn\"] = pad_collate\n",
    "\n",
    "    loader = DataLoader(ds, **loader_kwargs)\n",
    "    print(f\"[INFO] DataLoader ready: {len(ds)} samples • batch_size={batch_size} • workers={n_workers}\")\n",
    "\n",
    "    feats = []\n",
    "    use_cuda = (device.type == \"cuda\")\n",
    "    autocast_ctx = torch.cuda.amp.autocast if use_cuda else contextlib.nullcontext\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    n_seen = 0\n",
    "    print(f\"[INFO] Starting embedding pass on {device} (AMP={use_cuda})\")\n",
    "    with torch.inference_mode():\n",
    "        with autocast_ctx():\n",
    "            for step, (x1, _) in enumerate(loader):\n",
    "                n_seen += x1.size(0)\n",
    "                x = x1.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "                h, _ = enc(x, return_backbone=True)\n",
    "                feats.append(h.detach().to(\"cpu\", non_blocking=True).float())\n",
    "                if use_cuda:\n",
    "                    torch.cuda.synchronize()\n",
    "                if (step + 1) % 10 == 0 or (step == 0):\n",
    "                    dt = time.perf_counter() - t0\n",
    "                    print(f\"[INFO] Batches: {step+1} • Seen: {n_seen} • {n_seen/max(dt,1e-9):.1f} items/s\", flush=True)\n",
    "                del x, h\n",
    "        if use_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "    emb_time = time.perf_counter() - t0\n",
    "    print(f\"[INFO] Embedding done: {n_seen} items in {emb_time:.2f}s\")\n",
    "\n",
    "    H = torch.cat(feats, dim=0).numpy().astype(\"float32\")\n",
    "    return H, chosen_files\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# New helpers for k-scan, stability, retrieval, violins\n",
    "# ------------------------------------------------------------------------------------\n",
    "def _k_scan_curves(Xc: np.ndarray, method: str, ks=(20,30,40,50,60,80,100)):\n",
    "    rows = []\n",
    "    for kk in ks:\n",
    "        yk, _ = choose_clusterer(method, Xc, kk, seed=42)\n",
    "        rows.append([\n",
    "            kk,\n",
    "            _safe_metric(silhouette_score, Xc, yk, metric='euclidean'),\n",
    "            _safe_metric(davies_bouldin_score, Xc, yk),\n",
    "            _safe_metric(calinski_harabasz_score, Xc, yk)\n",
    "        ])\n",
    "    arr = np.array(rows, dtype=float)\n",
    "    # Make three small figures\n",
    "    figs = []\n",
    "    for col, ylabel, title in [(1,\"Silhouette (↑)\",\"k-scan Silhouette\"),\n",
    "                               (2,\"Davies–Bouldin (↓)\",\"k-scan DBI\"),\n",
    "                               (3,\"Calinski–Harabasz (↑)\",\"k-scan CH\")]:\n",
    "        fig = plt.figure(figsize=(6.5,4.2))\n",
    "        plt.plot(arr[:,0], arr[:,col], marker='o')\n",
    "        plt.xlabel(\"k\"); plt.ylabel(ylabel); plt.title(title)\n",
    "        figs.append(fig)\n",
    "    return arr, [_fig_to_img_html(f) for f in figs]\n",
    "\n",
    "def _stability_vs_seed(Xc: np.ndarray, method: str, k: int, seeds=(0,1,2,3,4), subsample=2000):\n",
    "    N = len(Xc); M = min(N, subsample)\n",
    "    rng = np.random.RandomState(123)\n",
    "    idx = np.sort(rng.choice(N, size=M, replace=False))\n",
    "    Xs = Xc[idx]\n",
    "    labels = []\n",
    "    for s in seeds:\n",
    "        y, _ = choose_clusterer(method, Xs, k, seed=int(s))\n",
    "        labels.append(y)\n",
    "    j_list, fmi_list = [], []\n",
    "    for i in range(len(seeds)):\n",
    "        for j in range(i+1, len(seeds)):\n",
    "            j_list.append(_coassign_jaccard(labels[i], labels[j]))\n",
    "            fmi_list.append(fowlkes_mallows_score(labels[i], labels[j]))\n",
    "    jacc = float(np.mean(j_list)); fmi = float(np.mean(fmi_list))\n",
    "    # bar fig\n",
    "    fig = plt.figure(figsize=(5.5,3.8))\n",
    "    plt.bar([0,1],[jacc,fmi])\n",
    "    plt.xticks([0,1],[\"Jaccard\\n(co-assign.)\",\"Fowlkes–\\nMallows\"])\n",
    "    plt.ylim(0,1)\n",
    "    plt.title(f\"Stability @ k={k} • subsample={M}\")\n",
    "    return {\"jaccard\": jacc, \"fmi\": fmi}, _fig_to_img_html(fig)\n",
    "\n",
    "def _retrieval_at_k(Xn: np.ndarray, y_true: np.ndarray, ks=(1,5,10)):\n",
    "    mask = np.array([(t is not None) and (str(t).lower() != \"unknown\") for t in y_true])\n",
    "    idxs = np.where(mask)[0]\n",
    "    if idxs.size < 2:\n",
    "        return {k: float(\"nan\") for k in ks}, None\n",
    "    Xq = Xn[idxs]; yq = y_true[idxs].astype(object)\n",
    "    S = Xq @ Xq.T\n",
    "    np.fill_diagonal(S, -np.inf)\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        topk = np.argpartition(-S, kth=min(k, S.shape[1]-1), axis=1)[:, :k]\n",
    "        hits = 0\n",
    "        for i in range(len(idxs)):\n",
    "            hits += int(np.sum(yq[topk[i]] == yq[i]))\n",
    "        out[k] = float(hits / (len(idxs) * k))\n",
    "    # bar\n",
    "    fig = plt.figure(figsize=(5.5,3.8))\n",
    "    xs = list(out.keys()); ys = [out[t] for t in xs]\n",
    "    plt.bar(range(len(xs)), ys)\n",
    "    plt.xticks(range(len(xs)), [f\"@{t}\" for t in xs])\n",
    "    plt.ylim(0,1)\n",
    "    plt.title(\"Label Transfer Precision@k\")\n",
    "    return out, _fig_to_img_html(fig)\n",
    "\n",
    "def _per_cluster_violins(Xc: np.ndarray, y_hat: np.ndarray):\n",
    "    # per-sample silhouette\n",
    "    try:\n",
    "        sil_samples = silhouette_samples(Xc, y_hat, metric=\"euclidean\")\n",
    "    except Exception:\n",
    "        sil_samples = np.full(len(Xc), np.nan, dtype=np.float32)\n",
    "    # centroid distances\n",
    "    clusters = sorted(np.unique(y_hat))\n",
    "    centers = np.vstack([Xc[y_hat == c].mean(axis=0) for c in clusters])\n",
    "    dists = np.zeros(len(Xc), dtype=np.float32)\n",
    "    for c, centroid in zip(clusters, centers):\n",
    "        idxs = np.where(y_hat == c)[0]\n",
    "        dists[idxs] = np.linalg.norm(Xc[idxs] - centroid, axis=1)\n",
    "    # violins\n",
    "    sil_groups = [sil_samples[y_hat == c] for c in clusters]\n",
    "    dist_groups = [dists[y_hat == c] for c in clusters]\n",
    "\n",
    "    fig1 = plt.figure(figsize=(min(12, 0.18*len(clusters)+6), 4.6))\n",
    "    plt.violinplot(sil_groups, showmeans=True, showextrema=False)\n",
    "    plt.xlabel(\"Cluster index\"); plt.ylabel(\"Silhouette (per-sample)\")\n",
    "    plt.title(\"Per-cluster Silhouette\")\n",
    "    v1 = _fig_to_img_html(fig1)\n",
    "\n",
    "    fig2 = plt.figure(figsize=(min(12, 0.18*len(clusters)+6), 4.6))\n",
    "    plt.violinplot(dist_groups, showmeans=True, showextrema=False)\n",
    "    plt.xlabel(\"Cluster index\"); plt.ylabel(\"Distance to centroid\")\n",
    "    plt.title(\"Per-cluster Centroid Distance\")\n",
    "    v2 = _fig_to_img_html(fig2)\n",
    "\n",
    "    return sil_samples, dists, v1, v2\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Main analysis (2% subset) — enhanced with new plots\n",
    "# ------------------------------------------------------------------------------------\n",
    "def analyze_all_unsupervised_to_html(encoder_ckpt_path, dataset_paths, labels_list,\n",
    "                                     cluster_method='kmeans', n_clusters=20,\n",
    "                                     subset_fraction=0.02, subset_seed=42,\n",
    "                                     subset_strategy=\"random\", wave_policy=\"convert\",\n",
    "                                     ks_scan=(20,30,40,50,60,80,100),\n",
    "                                     stability_seeds=(0,1,2,3,4), stability_subsample=2000,\n",
    "                                     sr_signal=10000, pca_dim=None):\n",
    "    embeddings_all, loc_labels_all, class_labels_all, file_paths_all = [], [], [], []\n",
    "\n",
    "    for path, loc_label in zip(dataset_paths, labels_list):\n",
    "        H, chosen_files = extract_embeddings_from_specdir(\n",
    "            encoder_ckpt_path, path,\n",
    "            batch_size=128, n_workers=0,\n",
    "            subset_fraction=subset_fraction,\n",
    "            subset_seed=subset_seed,\n",
    "            subset_strategy=subset_strategy,\n",
    "            wave_policy=wave_policy\n",
    "        )\n",
    "        embeddings_all.append(H)\n",
    "        file_paths_all.extend(chosen_files)\n",
    "        loc_labels_all.extend([loc_label] * H.shape[0])\n",
    "\n",
    "        full_files_sorted = list_audio_npy_files(path)\n",
    "        cls_full = load_class_labels_if_any(path, count=len(full_files_sorted))\n",
    "        name_to_label = {os.path.basename(p): cls_full[i] for i, p in enumerate(full_files_sorted)}\n",
    "        class_labels_all.extend([name_to_label[os.path.basename(p)] for p in chosen_files])\n",
    "\n",
    "    # Stack and normalize (Euclidean on L2-normalized ≈ cosine)\n",
    "    X = np.vstack(embeddings_all).astype(np.float32)\n",
    "    Xn = normalize(X, norm=\"l2\", axis=1)\n",
    "    Xc = Xn\n",
    "    if (pca_dim is not None) and (Xn.shape[1] > pca_dim):\n",
    "        Xc = PCA(n_components=int(pca_dim), random_state=42).fit_transform(Xn)\n",
    "\n",
    "    location_labels = np.array(loc_labels_all, dtype=object)\n",
    "    class_labels = np.array(class_labels_all, dtype=object)\n",
    "    original_paths = np.array(file_paths_all, dtype=object)\n",
    "\n",
    "    # ---- UMAP (cosine) for viz (by clusters & by GT)\n",
    "    reducer = UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=42)\n",
    "    t_umap0 = time.perf_counter()\n",
    "    proj_3d = reducer.fit_transform(Xn)\n",
    "    t_umap1 = time.perf_counter()\n",
    "    print(f\"[INFO] UMAP done in {t_umap1 - t_umap0:.2f}s\")\n",
    "\n",
    "    # ---- clustering on Xc\n",
    "    print(f\"[INFO] Clustering with {cluster_method}, k={n_clusters}\")\n",
    "    cluster_labels, cluster_probs = choose_clusterer(cluster_method, Xc, n_clusters, seed=42)\n",
    "\n",
    "    # ---- Global internal metrics\n",
    "    uniq = np.unique(cluster_labels)\n",
    "    valid_for_metrics = (len(uniq) > 1) and (Xc.shape[0] > len(uniq))\n",
    "\n",
    "    sil = _safe_metric(silhouette_score, Xc, cluster_labels, metric=\"euclidean\") if valid_for_metrics else float(\"nan\")\n",
    "    dbi = _safe_metric(davies_bouldin_score, Xc, cluster_labels) if valid_for_metrics else float(\"nan\")\n",
    "    ch  = _safe_metric(calinski_harabasz_score, Xc, cluster_labels) if valid_for_metrics else float(\"nan\")\n",
    "\n",
    "    # ---- External metrics vs any labels.npy\n",
    "    def _safe_external(fn, y_true, y_pred):\n",
    "        try:\n",
    "            return float(fn(y_true, y_pred)) if valid_for_metrics else float(\"nan\")\n",
    "        except Exception:\n",
    "            return float(\"nan\")\n",
    "    ari  = _safe_external(adjusted_rand_score, class_labels, cluster_labels)\n",
    "    ami  = _safe_external(adjusted_mutual_info_score, class_labels, cluster_labels)\n",
    "    hacc = _safe_external(hungarian_accuracy, class_labels, cluster_labels)\n",
    "\n",
    "    print(f\"[INFO] Silhouette (↑): {sil if sil==sil else 'nan'}\")\n",
    "    print(f\"[INFO] Davies–Bouldin (↓): {dbi if dbi==dbi else 'nan'}\")\n",
    "    print(f\"[INFO] Calinski–Harabasz (↑): {ch if ch==ch else 'nan'}\")\n",
    "    print(f\"[INFO] ARI (↑): {ari if ari==ari else 'nan'}\")\n",
    "    print(f\"[INFO] AMI (↑): {ami if ami==ami else 'nan'}\")\n",
    "    print(f\"[INFO] Hungarian Accuracy (↑): {hacc if hacc==hacc else 'nan'}\")\n",
    "\n",
    "    title_txt = (\n",
    "        f\"{cluster_method.capitalize()} @ k={n_clusters} \"\n",
    "        f\"(Sil={sil:.3f} | DBI={dbi:.3f} | CH={ch:.1f} | \"\n",
    "        f\"ARI={ari:.3f} | AMI={ami:.3f} | H-Acc={hacc:.3f})\"\n",
    "    )\n",
    "\n",
    "    # UMAP colored by cluster\n",
    "    umap_by_cluster = px.scatter_3d(\n",
    "        x=proj_3d[:, 0], y=proj_3d[:, 1], z=proj_3d[:, 2],\n",
    "        color=[str(c) for c in cluster_labels],\n",
    "        symbol=location_labels,\n",
    "        hover_data={\"Cluster\": cluster_labels, \"GT\": class_labels},\n",
    "        title=title_txt + \" • colored by cluster\",\n",
    "        opacity=0.85, height=700\n",
    "    )\n",
    "    umap_cluster_html = pio.to_html(umap_by_cluster, include_plotlyjs=\"cdn\", full_html=False)\n",
    "\n",
    "    # UMAP colored by GT labels (object-safe)\n",
    "    gt_colors = class_labels.astype(str)\n",
    "    umap_by_gt = px.scatter_3d(\n",
    "        x=proj_3d[:, 0], y=proj_3d[:, 1], z=proj_3d[:, 2],\n",
    "        color=gt_colors,\n",
    "        symbol=location_labels,\n",
    "        hover_data={\"Cluster\": cluster_labels, \"GT\": class_labels},\n",
    "        title=title_txt + \" • colored by GT\",\n",
    "        opacity=0.85, height=700\n",
    "    )\n",
    "    umap_gt_html = pio.to_html(umap_by_gt, include_plotlyjs=False, full_html=False)\n",
    "\n",
    "    # ---- k-scan\n",
    "    kscan_arr, kscan_imgs = _k_scan_curves(Xc, cluster_method, ks=tuple(ks_scan))\n",
    "    kscan_html = \"\".join(f'<div class=\"col-md-4 mb-3\">{img}</div>' for img in kscan_imgs)\n",
    "\n",
    "    # ---- stability vs seed\n",
    "    stability_stats, stability_img = _stability_vs_seed(Xc, cluster_method, n_clusters,\n",
    "                                                        seeds=tuple(stability_seeds),\n",
    "                                                        subsample=int(stability_subsample))\n",
    "\n",
    "    # ---- retrieval@k (if any labels)\n",
    "    retrieval_stats, retrieval_img = _retrieval_at_k(Xn, class_labels, ks=(1,5,10))\n",
    "\n",
    "    # ---- per-cluster violins\n",
    "    sil_samples, dists, violin_sil_html, violin_dist_html = _per_cluster_violins(Xc, cluster_labels)\n",
    "\n",
    "    # ---- Per-cluster sections + spectrogram consistency\n",
    "    cluster_html = \"\"\n",
    "    cluster_blocks = []\n",
    "    base_for_entropy = len(set(location_labels))\n",
    "\n",
    "    spec_cos_scores, spec_dtw_scores = [], []\n",
    "\n",
    "    for c in sorted(set(cluster_labels)):\n",
    "        idxs = np.where(cluster_labels == c)[0]\n",
    "        if idxs.size == 0:\n",
    "            continue\n",
    "\n",
    "        cluster_counts = Counter(location_labels[idxs])\n",
    "        class_counts = Counter(class_labels[idxs])\n",
    "\n",
    "        metrics = evaluate_cluster_metrics(Xc, idxs, location_labels, location_entropy_base=base_for_entropy)\n",
    "\n",
    "        spec_cos = _avg_intra_cluster_spec_cosine(original_paths, idxs, fs=sr_signal, max_samples=50)\n",
    "        spec_dtw = _avg_intra_cluster_spec_dtw(original_paths, idxs, fs=sr_signal, max_samples=25)\n",
    "\n",
    "        if np.isfinite(spec_cos): spec_cos_scores.append(spec_cos)\n",
    "        if np.isfinite(spec_dtw): spec_dtw_scores.append(spec_dtw)\n",
    "\n",
    "        meta_html = \"<p><strong>Location Distribution:</strong></p><ul>\" + \"\".join(\n",
    "            f\"<li><b>{loc}</b>: {count} ({count/len(idxs):.1%})</li>\" for loc, count in cluster_counts.items()\n",
    "        ) + \"</ul>\"\n",
    "\n",
    "        meta_html += \"<p><strong>Class Distribution:</strong></p><ul>\" + \"\".join(\n",
    "            f\"<li>{cls}: {count}</li>\" for cls, count in class_counts.items()\n",
    "        ) + \"</ul>\"\n",
    "\n",
    "        meta_html += f\"\"\"\n",
    "        <p><strong>Cluster Metrics (Embedding Space):</strong></p>\n",
    "        <ul>\n",
    "            <li>Size: {len(idxs)}</li>\n",
    "            <li>Intra-Cluster Variance: {metrics['variance']:.4f}</li>\n",
    "            <li>Mean Cosine Similarity (Embeddings): {metrics['mean_sim']:.4f}</li>\n",
    "            <li>Location Entropy: {metrics['entropy']:.3f}</li>\n",
    "            <li>Composite Quality Score: {metrics['quality']:.4f}</li>\n",
    "            <li><strong>Novelty Score:</strong> {metrics['novelty']:.4f}</li>\n",
    "        </ul>\n",
    "        <p><strong>Spectrogram Consistency (Signal Space):</strong></p>\n",
    "        <ul>\n",
    "            <li>Mean Spectrogram Cosine (↑): {spec_cos if np.isfinite(spec_cos) else float('nan'):.4f}</li>\n",
    "            <li>Mean Spectrogram DTW (↓): {spec_dtw if np.isfinite(spec_dtw) else float('nan'):.2f}</li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "\n",
    "        # Nearest-to-center exemplars (4 images)\n",
    "        center = np.mean(Xc[idxs], axis=0, keepdims=True)\n",
    "        distances = np.linalg.norm(Xc[idxs] - center, axis=1)\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        sampled_idxs = idxs[sorted_indices[:min(4, len(sorted_indices))]]\n",
    "\n",
    "        imgs = []\n",
    "        for i, chosen_idx in enumerate(sampled_idxs):\n",
    "            try:\n",
    "                x = np.load(original_paths[chosen_idx], mmap_mode=\"r\")\n",
    "                title = f\"#{i+1} | {location_labels[chosen_idx]} | Class {class_labels[chosen_idx]}\"\n",
    "                if x.ndim == 1:\n",
    "                    imgs.append(generate_spectrogram_base64(x.astype(np.float32), fs=sr_signal, title=title))\n",
    "                else:\n",
    "                    S = x.squeeze(0).astype(np.float32)\n",
    "                    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "                    ax.imshow(S, aspect='auto', origin='lower', cmap='viridis')\n",
    "                    ax.set_title(title); ax.set_xlabel(\"Frames\"); ax.set_ylabel(\"Mel bins\")\n",
    "                    imgs.append(_fig_to_img_html(fig))\n",
    "            except Exception as e:\n",
    "                imgs.append(f\"<p class='text-danger'>Error loading sample {i+1}: {e}</p>\")\n",
    "\n",
    "        carousel_html = make_carousel(c, \"all\", imgs)\n",
    "        block = f\"<div class='col-md-6 mb-4'><h4>Cluster {c}</h4>{meta_html}{carousel_html}</div>\"\n",
    "        cluster_blocks.append(block)\n",
    "\n",
    "    for i in range(0, len(cluster_blocks), 2):\n",
    "        cluster_html += \"<div class='row'>\" + \"\".join(cluster_blocks[i:i+2]) + \"</div>\"\n",
    "\n",
    "    # Global spectrogram-consistency summary across clusters\n",
    "    global_spec_cos = float(np.mean(spec_cos_scores)) if len(spec_cos_scores) else float(\"nan\")\n",
    "    global_spec_dtw = float(np.mean(spec_dtw_scores)) if len(spec_dtw_scores) else float(\"nan\")\n",
    "\n",
    "    summary_card = f\"\"\"\n",
    "    <div class='row mb-4'>\n",
    "      <div class='col-md-12'>\n",
    "        <div class='alert alert-info' role='alert'>\n",
    "          <h5 class='mb-2'>Spectrogram Consistency (Cluster Averages)</h5>\n",
    "          <ul class='mb-0'>\n",
    "            <li><strong>Mean Spectrogram Cosine</strong>: {global_spec_cos if np.isfinite(global_spec_cos) else float('nan'):.4f} (↑ better)</li>\n",
    "            <li><strong>Mean Spectrogram DTW</strong>: {global_spec_dtw if np.isfinite(global_spec_dtw) else float('nan'):.2f} (↓ better)</li>\n",
    "          </ul>\n",
    "          <small>Cosine on standardized spectrograms (z-norm, padded/cropped). DTW optional & subsampled.</small>\n",
    "        </div>\n",
    "      </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # Overview row with UMAPs and key plots\n",
    "    overview_row = f\"\"\"\n",
    "    <div class=\"row mb-4\">\n",
    "      <div class=\"col-md-6\">{umap_cluster_html}</div>\n",
    "      <div class=\"col-md-6\">{umap_gt_html}</div>\n",
    "    </div>\n",
    "    <div class=\"row mb-3\">{kscan_html}</div>\n",
    "    <div class=\"row mb-3\">\n",
    "      <div class=\"col-md-6\">{stability_img}</div>\n",
    "      <div class=\"col-md-6\">{retrieval_img if retrieval_img else \"<p>No labeled subset found for retrieval@k.</p>\"}</div>\n",
    "    </div>\n",
    "    <div class=\"row mb-3\">\n",
    "      <div class=\"col-md-12\">{violin_sil_html}</div>\n",
    "    </div>\n",
    "    <div class=\"row mb-4\">\n",
    "      <div class=\"col-md-12\">{violin_dist_html}</div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # Metrics mini-table\n",
    "    metrics_table = f\"\"\"\n",
    "    <div class=\"table-responsive mb-4\">\n",
    "      <table class=\"table table-sm table-bordered\">\n",
    "        <thead class=\"table-light\"><tr>\n",
    "          <th>Silhouette (↑)</th><th>Davies–Bouldin (↓)</th><th>Calinski–Harabasz (↑)</th>\n",
    "          <th>ARI (↑)</th><th>AMI (↑)</th><th>Hungarian Acc. (↑)</th>\n",
    "          <th>Stability Jaccard</th><th>Stability FMI</th>\n",
    "        </tr></thead>\n",
    "        <tbody><tr>\n",
    "          <td>{sil:.3f}</td><td>{dbi:.3f}</td><td>{ch:.1f}</td>\n",
    "          <td>{ari:.3f}</td><td>{ami:.3f}</td><td>{hacc:.3f}</td>\n",
    "          <td>{stability_stats['jaccard']:.3f}</td><td>{stability_stats['fmi']:.3f}</td>\n",
    "        </tr></tbody>\n",
    "      </table>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    <div class='section'>\n",
    "      <h2>{cluster_method.capitalize()} Clustering Analysis (2% subset)</h2>\n",
    "      <p class=\"text-muted\">Embeddings L2-normalized prior to clustering; Euclidean distance used (≈ cosine).</p>\n",
    "      {metrics_table}\n",
    "      {overview_row}\n",
    "      {summary_card}\n",
    "      {cluster_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Report generator\n",
    "# ------------------------------------------------------------------------------------\n",
    "def generate_full_report(encoder_ckpt_path, dataset_paths, labels_list,\n",
    "                         cluster_method='kmeans', n_clusters=20, out_prefix=\"unsup_report\",\n",
    "                         subset_fraction=0.02, subset_seed=42, subset_strategy=\"random\",\n",
    "                         wave_policy=\"convert\", ks_scan=(20,30,40,50,60,80,100),\n",
    "                         stability_seeds=(0,1,2,3,4), stability_subsample=2000,\n",
    "                         sr_signal=10000, pca_dim=None):\n",
    "    section_html = analyze_all_unsupervised_to_html(\n",
    "        encoder_ckpt_path=encoder_ckpt_path,\n",
    "        dataset_paths=list(dataset_paths),\n",
    "        labels_list=list(labels_list),\n",
    "        cluster_method=cluster_method,\n",
    "        n_clusters=int(n_clusters),\n",
    "        subset_fraction=subset_fraction,\n",
    "        subset_seed=subset_seed,\n",
    "        subset_strategy=subset_strategy,\n",
    "        wave_policy=wave_policy,\n",
    "        ks_scan=ks_scan,\n",
    "        stability_seeds=stability_seeds,\n",
    "        stability_subsample=stability_subsample,\n",
    "        sr_signal=sr_signal,\n",
    "        pca_dim=pca_dim\n",
    "    )\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Unsupervised Clustering Report (2% subset)</title>\n",
    "        <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; padding: 20px; background-color: #f5f5f5; }}\n",
    "            h1 {{ color: #2c3e50; }}\n",
    "            h2, h4 {{ color: #34495e; }}\n",
    "            hr {{ border-top: 2px solid #bbb; margin-top: 24px; margin-bottom: 24px; }}\n",
    "            .section {{ margin-bottom: 60px; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "            img {{ max-width: 100%; height: auto; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "      <h1 class='mb-3'>Unsupervised Latent Clustering Report (2% subset)</h1>\n",
    "      <p><strong>Compared Locations:</strong> {', '.join(labels_list)}</p>\n",
    "      <p><strong>Subset:</strong> {int(subset_fraction*100)}% • Strategy: {subset_strategy} • Seed: {subset_seed}</p>\n",
    "      <hr>\n",
    "      {section_html}\n",
    "    </body></html>\n",
    "    \"\"\"\n",
    "    output_path = f\"{out_prefix}_{cluster_method}_subset{int(subset_fraction*100)}.html\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Example (commented)\n",
    "# ------------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    encoder_ckpt_path = \"reef_ssl_precomp_ckpt.pth\"\n",
    "    dataset_paths = [\"/notebooks/dataset_preprocessed\"]\n",
    "    labels_list = [\"Full\"]\n",
    "    report_path = generate_full_report(\n",
    "        encoder_ckpt_path=encoder_ckpt_path,\n",
    "        dataset_paths=dataset_paths,\n",
    "        labels_list=labels_list,\n",
    "        cluster_method='kmeans',     # 'kmeans' | 'gmm' | 'agglomerative'\n",
    "        n_clusters=40,\n",
    "        out_prefix=\"SimCLR_kmeans_k60\",\n",
    "        subset_fraction=0.02,\n",
    "        subset_seed=45,\n",
    "        subset_strategy=\"random\",\n",
    "        wave_policy=\"convert\",\n",
    "        ks_scan=(2,5,10,20,30,40,50,60,80,100),\n",
    "        stability_seeds=(0,1,2,3,4),\n",
    "        stability_subsample=2000,\n",
    "        sr_signal=10000,\n",
    "        pca_dim=None,  # e.g., 64 to reduce dimensionality before clustering\n",
    "    )\n",
    "    print(\"✅ Report saved to:\", report_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
