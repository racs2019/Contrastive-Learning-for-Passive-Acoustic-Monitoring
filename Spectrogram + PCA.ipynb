{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e6b3ca-017a-4a4e-80aa-39d00bdacc45",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c905746d-0346-4149-b4f6-a2fe42084a9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T15:06:17.740139Z",
     "iopub.status.busy": "2025-08-27T15:06:17.739888Z",
     "iopub.status.idle": "2025-08-27T15:06:27.579343Z",
     "shell.execute_reply": "2025-08-27T15:06:27.578796Z",
     "shell.execute_reply.started": "2025-08-27T15:06:17.740120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2020.6.20)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.6 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.7.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.66.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6->umap-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting plotly\n",
      "  Downloading plotly-6.3.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (23.2)\n",
      "Downloading plotly-6.3.0-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.2.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m401.0/401.0 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-2.2.0 plotly-6.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n",
    "!pip install umap-learn\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307b688-7312-4914-8807-b4dc8d4e0f53",
   "metadata": {},
   "source": [
    "Algorithm and report\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41157e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T15:13:57.130082Z",
     "iopub.status.busy": "2025-08-27T15:13:57.129841Z",
     "iopub.status.idle": "2025-08-27T15:13:57.147417Z",
     "shell.execute_reply": "2025-08-27T15:13:57.146857Z",
     "shell.execute_reply.started": "2025-08-27T15:13:57.130065Z"
    }
   },
   "outputs": [],
   "source": [
    "# pca_kmeans_pipeline.py  â€” TEST-ONLY version\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score, adjusted_mutual_info_score,\n",
    "    silhouette_score, davies_bouldin_score, calinski_harabasz_score,\n",
    ")\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple, Dict, Any, Optional, Sequence\n",
    "\n",
    "# Safe UMAP import\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------\n",
    "#  Hungarian Accuracy\n",
    "# ----------------------------\n",
    "def hungarian_accuracy(y_true, y_pred):\n",
    "    D = int(max(y_pred.max(), y_true.max())) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(len(y_pred)):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return float(sum(w[i, j] for i, j in zip(row_ind, col_ind)) / len(y_pred))\n",
    "\n",
    "# ----------------------------\n",
    "#  Helpers to get paths/labels and load features for a SUBSET ONLY\n",
    "# ----------------------------\n",
    "def _sorted_paths_and_labels(audio_dir: str) -> Tuple[list, np.ndarray]:\n",
    "    paths = sorted(\n",
    "        [os.path.join(audio_dir, f) for f in os.listdir(audio_dir)\n",
    "         if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "        key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "    )\n",
    "    labels = np.load(os.path.join(audio_dir, \"labels.npy\"))\n",
    "    if len(paths) != len(labels):\n",
    "        raise ValueError(f\"{len(paths)} audio files != {len(labels)} labels\")\n",
    "    return paths, labels\n",
    "\n",
    "def _load_logmel_subset(\n",
    "    paths: Sequence[str],\n",
    "    labels: np.ndarray,\n",
    "    indices: Sequence[int],\n",
    "    sr: int = 16000,\n",
    "    n_mels: int = 64\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X, y = [], []\n",
    "    for idx in indices:\n",
    "        p = paths[idx]\n",
    "        try:\n",
    "            audio = np.load(p).astype(np.float32)\n",
    "            m = np.max(np.abs(audio))\n",
    "            if m > 0:\n",
    "                audio = audio / m\n",
    "            mel = librosa.feature.melspectrogram(\n",
    "                y=audio, sr=sr, n_fft=1024, hop_length=512, n_mels=n_mels\n",
    "            )\n",
    "            logmel = librosa.power_to_db(mel + 1e-8)\n",
    "            X.append(logmel.flatten())\n",
    "            y.append(labels[idx])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {p}: {e}\")\n",
    "    return np.asarray(X), np.asarray(y)\n",
    "\n",
    "# ----------------------------\n",
    "#  Internal cluster indices (on TEST space)\n",
    "# ----------------------------\n",
    "def _internal_indices(X: np.ndarray, labels: np.ndarray) -> Dict[str, float]:\n",
    "    out = {\"silhouette\": float(\"nan\"),\n",
    "           \"davies_bouldin\": float(\"nan\"),\n",
    "           \"calinski_harabasz\": float(\"nan\")}\n",
    "    uniq = np.unique(labels)\n",
    "    if len(uniq) <= 1 or len(uniq) >= len(labels):\n",
    "        return out\n",
    "    try: out[\"silhouette\"] = float(silhouette_score(X, labels))\n",
    "    except Exception: pass\n",
    "    try: out[\"davies_bouldin\"] = float(davies_bouldin_score(X, labels))\n",
    "    except Exception: pass\n",
    "    try: out[\"calinski_harabasz\"] = float(calinski_harabasz_score(X, labels))\n",
    "    except Exception: pass\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "#  Runner (TEST-ONLY)\n",
    "# ----------------------------\n",
    "def run_pca_kmeans_test_only(\n",
    "    dataset_path: str,\n",
    "    *,\n",
    "    n_components: int = 100,\n",
    "    random_state: int = 42,\n",
    "    do_umap: bool = True,\n",
    "    umap_components: int = 3,\n",
    "    umap_html_path: str = \"pca_kmeans_umap.html\",\n",
    "    kmeans_k: Optional[int] = None,     # choose k explicitly\n",
    "    test_size: float = 0.2,             # 20% test set\n",
    "    test_indices: Optional[Sequence[int]] = None,  # pass a fixed 20% if you want consistency across methods\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    TEST-ONLY pipeline:\n",
    "      - Make a stratified 80/20 split by indices (or use provided test_indices)\n",
    "      - Load + process features for the TEST split ONLY\n",
    "      - Fit StandardScaler/PCA on TEST only\n",
    "      - Fit KMeans on TEST only, compute ARI/AMI/Hungarian + Silhouette/DB/CH\n",
    "      - (Optional) UMAP of TEST embeddings\n",
    "    \"\"\"\n",
    "    label_path = os.path.join(dataset_path, \"labels.npy\")\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {dataset_path}\")\n",
    "    if not os.path.isfile(label_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    # 1) Fixed split by indices\n",
    "    paths, labels = _sorted_paths_and_labels(dataset_path)\n",
    "    all_idx = np.arange(len(paths))\n",
    "    if test_indices is None:\n",
    "        # Stratified split; we ignore the train indices and do not process/train on them.\n",
    "        _, test_idx = train_test_split(\n",
    "            all_idx, test_size=test_size, stratify=labels, random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        test_idx = np.asarray(list(test_indices), dtype=int)\n",
    "\n",
    "    # 2) Load ONLY the TEST subset\n",
    "    X_test, y_test = _load_logmel_subset(paths, labels, test_idx)\n",
    "    if X_test.size == 0:\n",
    "        raise ValueError(\"No test features were loaded.\")\n",
    "\n",
    "    # 3) Scale + PCA (fit on TEST ONLY)\n",
    "    scaler = StandardScaler().fit(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    pca = PCA(n_components=n_components, random_state=random_state).fit(X_test_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    # 4) KMeans on TEST ONLY\n",
    "    n_clusters = int(kmeans_k) if kmeans_k is not None else len(np.unique(y_test))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=random_state)\n",
    "    test_preds = kmeans.fit_predict(X_test_pca)\n",
    "\n",
    "    # 5) Metrics (external + internal) on TEST\n",
    "    ari  = adjusted_rand_score(y_test, test_preds)\n",
    "    ami  = adjusted_mutual_info_score(y_test, test_preds)\n",
    "    hacc = hungarian_accuracy(y_test, test_preds)\n",
    "    extra = _internal_indices(X_test_pca, test_preds)\n",
    "\n",
    "    print(f\"\\nðŸ“Š TEST-ONLY PCA+KMeans (k={n_clusters})\")\n",
    "    print(f\"ARI: {ari:.4f} | AMI: {ami:.4f} | Hungarian: {hacc:.4f}\")\n",
    "    print(f\"Silhouette: {extra['silhouette']:.4f} | \"\n",
    "          f\"Daviesâ€“Bouldin: {extra['davies_bouldin']:.4f} | \"\n",
    "          f\"Calinskiâ€“Harabasz: {extra['calinski_harabasz']:.2f}\")\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"test_samples\": int(len(X_test)),\n",
    "        \"n_components\": int(n_components),\n",
    "        \"kmeans_k\": int(n_clusters),\n",
    "        \"metrics\": {\n",
    "            \"ari\": float(ari),\n",
    "            \"ami\": float(ami),\n",
    "            \"hungarian_accuracy\": float(hacc),\n",
    "            \"silhouette\": float(extra[\"silhouette\"]),\n",
    "            \"davies_bouldin\": float(extra[\"davies_bouldin\"]),\n",
    "            \"calinski_harabasz\": float(extra[\"calinski_harabasz\"]),\n",
    "        },\n",
    "        \"used_test_indices\": np.asarray(test_idx).tolist(),\n",
    "    }\n",
    "\n",
    "    # 6) Optional UMAP (TEST ONLY)\n",
    "    if do_umap:\n",
    "        if umap_components not in (2, 3):\n",
    "            raise ValueError(\"umap_components must be 2 or 3\")\n",
    "        reducer = umap.UMAP(n_components=umap_components, random_state=random_state)\n",
    "        X_umap = reducer.fit_transform(X_test_pca)\n",
    "        if umap_components == 3:\n",
    "            fig = px.scatter_3d(\n",
    "                x=X_umap[:, 0], y=X_umap[:, 1], z=X_umap[:, 2],\n",
    "                color=y_test.astype(str),\n",
    "                title=(f\"TEST-ONLY PCA+KMeans (k={n_clusters}) | \"\n",
    "                       f\"ARI {ari:.4f}, AMI {ami:.4f}, H-Acc {hacc:.4f}\"),\n",
    "                labels={\"x\": \"UMAP-1\", \"y\": \"UMAP-2\", \"z\": \"UMAP-3\"},\n",
    "                opacity=0.7\n",
    "            )\n",
    "        else:\n",
    "            fig = px.scatter(\n",
    "                x=X_umap[:, 0], y=X_umap[:, 1],\n",
    "                color=y_test.astype(str),\n",
    "                title=(f\"TEST-ONLY PCA+KMeans (k={n_clusters}) | \"\n",
    "                       f\"ARI {ari:.4f}, AMI {ami:.4f}\"),\n",
    "                labels={\"x\": \"UMAP-1\", \"y\": \"UMAP-2\"},\n",
    "                opacity=0.7\n",
    "            )\n",
    "        fig.write_html(umap_html_path)\n",
    "        print(f\"Saved UMAP visualization to '{umap_html_path}'\")\n",
    "        results[\"umap_html_path\"] = umap_html_path\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6131432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T00:47:04.620754Z",
     "iopub.status.busy": "2025-08-29T00:47:04.619753Z",
     "iopub.status.idle": "2025-08-29T01:09:05.706093Z",
     "shell.execute_reply": "2025-08-29T01:09:05.705274Z",
     "shell.execute_reply.started": "2025-08-29T00:47:04.620728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PCA on TEST: (82655, 100) â€¢ 49.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Report saved to: pca_k6.html\n",
      "Report: pca_k6.html\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "PCA + KMeans (TEST-ONLY) â€” Unsupervised Clustering HTML Report\n",
    "\n",
    "- Accepts one or more dataset directories and matching location tags\n",
    "- Makes a stratified 80/20 split (or you can pass fixed TEST indices per dataset)\n",
    "- Loads ONLY the TEST portions, computes log-mel (if waveform) or uses given 2D specs\n",
    "- Fits StandardScaler+PCA on the combined TEST features\n",
    "- Runs KMeans on TEST embeddings and reports:\n",
    "    * Global internal metrics: Silhouette (â†‘), DBI (â†“), CH (â†‘)\n",
    "    * External metrics vs labels.npy: ARI, AMI, Hungarian\n",
    "- UMAP 3D scatter (TEST embeddings)\n",
    "- Per-cluster cards: size, variance, embedding cosine, location entropy, quality, novelty\n",
    "- Spectrogram thumbnails per cluster (nearest-to-center)\n",
    "- Signal-space consistency per cluster (mean spectrogram cosine; optional DTW)\n",
    "\n",
    "Requires: numpy, librosa, scikit-learn, plotly, matplotlib, umap-learn\n",
    "Optional: fastdtw  (for DTW; otherwise shows NaN)\n",
    "\"\"\"\n",
    "\n",
    "import os, math, base64, time\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Dict, Tuple, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML / metrics\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, davies_bouldin_score, calinski_harabasz_score,\n",
    "    adjusted_rand_score, adjusted_mutual_info_score\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.signal import spectrogram, get_window\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "# ========================= Helpers: files, labels, audio =========================\n",
    "\n",
    "def _sorted_paths_and_labels(audio_dir: str) -> Tuple[List[str], np.ndarray]:\n",
    "    paths = sorted(\n",
    "        [os.path.join(audio_dir, f) for f in os.listdir(audio_dir)\n",
    "         if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "        key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "    )\n",
    "    lbl_path = os.path.join(audio_dir, \"labels.npy\")\n",
    "    if not os.path.isfile(lbl_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {lbl_path}\")\n",
    "    labels = np.load(lbl_path, allow_pickle=True)\n",
    "    if len(paths) != len(labels):\n",
    "        raise ValueError(f\"{len(paths)} audio files != {len(labels)} labels in {audio_dir}\")\n",
    "    return paths, labels\n",
    "\n",
    "\n",
    "def _load_logmel_or_flatten(arr: np.ndarray, *, sr: int = 16000, n_mels: int = 64) -> np.ndarray:\n",
    "    \"\"\"If 1D waveform: compute log-mel; if 2D array: assume already a spec and flatten.\"\"\"\n",
    "    if arr.ndim == 1:\n",
    "        x = arr.astype(np.float32)\n",
    "        mx = np.max(np.abs(x))\n",
    "        if np.isfinite(mx) and mx > 0:\n",
    "            x = x / mx\n",
    "        mel = librosa.feature.melspectrogram(y=x, sr=sr, n_fft=1024, hop_length=512, n_mels=n_mels)\n",
    "        logmel = librosa.power_to_db(mel + 1e-8)\n",
    "        return logmel.flatten().astype(np.float32)\n",
    "    elif arr.ndim == 2:\n",
    "        return arr.astype(np.float32).flatten()\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unexpected array shape {arr.shape}\")\n",
    "\n",
    "\n",
    "def _load_logmel_subset(paths: Sequence[str], labels: np.ndarray, indices: Sequence[int],\n",
    "                        *, sr: int = 16000, n_mels: int = 64) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    X, y, used_paths = [], [], []\n",
    "    for idx in indices:\n",
    "        p = paths[idx]\n",
    "        try:\n",
    "            arr = np.load(p, mmap_mode=\"r\")\n",
    "            feat = _load_logmel_or_flatten(arr, sr=sr, n_mels=n_mels)\n",
    "            X.append(feat); y.append(labels[idx]); used_paths.append(p)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {p}: {e}\")\n",
    "    return np.asarray(X), np.asarray(y), used_paths\n",
    "\n",
    "\n",
    "# ========================= Viz: spectrograms for thumbnails =========================\n",
    "\n",
    "def _ali_spec(x: np.ndarray, fs: int = 16000) -> np.ndarray:\n",
    "    \"\"\"Hann spectrogram â†’ dB-like; returns (F,T) slice ~0â€“1 kHz (adjust as needed).\"\"\"\n",
    "    Lframe2 = 1024\n",
    "    po = 80\n",
    "    lov = int(math.ceil((po / 100) * Lframe2))\n",
    "    taper = get_window('hann', Lframe2)\n",
    "    Nfft = 2 ** (int(math.floor(math.log2(Lframe2))) + 2)\n",
    "    f, t, s = spectrogram(x, fs=fs, window=taper, noverlap=lov, nfft=Nfft, mode='magnitude')\n",
    "    as_max = np.max(s) if np.isfinite(np.max(s)) and np.max(s) > 0 else 1.0\n",
    "    sdb = 10 * np.log10(np.maximum(s / as_max, 1e-10))\n",
    "    # keep up to 1000 Hz\n",
    "    max_inx = np.argmin(np.abs(f - 1000))\n",
    "    return sdb[:max_inx+1, :]\n",
    "\n",
    "\n",
    "def _standardize_spec(S: np.ndarray, target_shape=(128, 256)) -> np.ndarray:\n",
    "    S = np.asarray(S, dtype=np.float32)\n",
    "    S = (S - np.mean(S)) / (np.std(S) + 1e-8)\n",
    "    F, T = S.shape\n",
    "    Ft, Tt = target_shape\n",
    "    # F\n",
    "    if F < Ft:\n",
    "        pt = (Ft - F) // 2\n",
    "        pb = Ft - F - pt\n",
    "        S = np.pad(S, ((pt, pb), (0, 0)), mode=\"constant\")\n",
    "    elif F > Ft:\n",
    "        s = (F - Ft) // 2\n",
    "        S = S[s:s+Ft, :]\n",
    "    # T\n",
    "    F, T = S.shape\n",
    "    if T < Tt:\n",
    "        pl = (Tt - T) // 2\n",
    "        pr = Tt - T - pl\n",
    "        S = np.pad(S, ((0, 0), (pl, pr)), mode=\"constant\")\n",
    "    elif T > Tt:\n",
    "        s = (T - Tt) // 2\n",
    "        S = S[:, s:s+Tt]\n",
    "    return S\n",
    "\n",
    "\n",
    "def _spec_img_base64(audio_or_spec: np.ndarray, title: str, fs: int = 16000) -> str:\n",
    "    if audio_or_spec.ndim == 1:\n",
    "        S = _ali_spec(audio_or_spec.astype(np.float32), fs=fs)\n",
    "    elif audio_or_spec.ndim == 2:\n",
    "        S = audio_or_spec.astype(np.float32)\n",
    "    else:\n",
    "        return f\"<p class='text-danger'>Bad shape: {audio_or_spec.shape}</p>\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.imshow(S, aspect='auto', origin='lower')\n",
    "    ax.set_title(title); ax.set_xlabel(\"Frames\"); ax.set_ylabel(\"Bins\")\n",
    "    fig.tight_layout()\n",
    "    buf = BytesIO(); fig.savefig(buf, format='png', dpi=120); plt.close(fig); buf.seek(0)\n",
    "    image_base64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return f'<img class=\"d-block w-100\" src=\"data:image/png;base64,{image_base64}\" alt=\"{title}\">'\n",
    "\n",
    "\n",
    "def _load_std_spec_for_index(paths: np.ndarray, idx: int, *, fs: int = 16000,\n",
    "                             target_shape=(128, 256)) -> Optional[np.ndarray]:\n",
    "    try:\n",
    "        arr = np.load(paths[idx], mmap_mode=\"r\")\n",
    "        if arr.ndim == 1:\n",
    "            S = _ali_spec(arr.astype(np.float32), fs=fs)\n",
    "        elif arr.ndim == 2:\n",
    "            S = arr.astype(np.float32)\n",
    "        else:\n",
    "            return None\n",
    "        return _standardize_spec(S, target_shape=target_shape)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _avg_intra_cluster_spec_cosine(paths: np.ndarray, idxs: np.ndarray, *,\n",
    "                                   fs: int = 16000, max_samples: int = 50,\n",
    "                                   target_shape=(128, 256)) -> float:\n",
    "    if len(idxs) < 2:\n",
    "        return float(\"nan\")\n",
    "    if len(idxs) > max_samples:\n",
    "        rng = np.random.RandomState(42)\n",
    "        idxs = rng.choice(idxs, size=max_samples, replace=False)\n",
    "    specs = []\n",
    "    for i in idxs:\n",
    "        S = _load_std_spec_for_index(paths, int(i), fs=fs, target_shape=target_shape)\n",
    "        if S is not None:\n",
    "            specs.append(S.reshape(-1))\n",
    "    if len(specs) < 2:\n",
    "        return float(\"nan\")\n",
    "    X = np.stack(specs, axis=0)\n",
    "    sim = cosine_similarity(X)\n",
    "    iu = np.triu_indices_from(sim, k=1)\n",
    "    return float(np.mean(sim[iu]))\n",
    "\n",
    "\n",
    "def _avg_intra_cluster_spec_dtw(paths: np.ndarray, idxs: np.ndarray, *,\n",
    "                                fs: int = 16000, max_samples: int = 25,\n",
    "                                target_shape=(128, 256)) -> float:\n",
    "    try:\n",
    "        from fastdtw import fastdtw\n",
    "        from scipy.spatial.distance import euclidean\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "    if len(idxs) < 2:\n",
    "        return float(\"nan\")\n",
    "    if len(idxs) > max_samples:\n",
    "        rng = np.random.RandomState(42)\n",
    "        idxs = rng.choice(idxs, size=max_samples, replace=False)\n",
    "    series = []\n",
    "    for i in idxs:\n",
    "        S = _load_std_spec_for_index(paths, int(i), fs=fs, target_shape=target_shape)\n",
    "        if S is not None:\n",
    "            series.append(np.mean(S, axis=0))  # collapse freq â†’ time trace\n",
    "    if len(series) < 2:\n",
    "        return float(\"nan\")\n",
    "    n = len(series)\n",
    "    total, pairs = 0.0, 0\n",
    "    for a in range(n):\n",
    "        for b in range(a+1, n):\n",
    "            d, _ = fastdtw(series[a], series[b], dist=euclidean)\n",
    "            total += d; pairs += 1\n",
    "    return float(total / max(pairs, 1))\n",
    "\n",
    "\n",
    "# ========================= Metrics & clustering =========================\n",
    "\n",
    "def _hungarian_accuracy(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    true_ids = {t: i for i, t in enumerate(np.unique(y_true))}\n",
    "    pred_ids = {c: i for i, c in enumerate(np.unique(y_pred))}\n",
    "    yi = np.vectorize(true_ids.get)(y_true)\n",
    "    pi = np.vectorize(pred_ids.get)(y_pred)\n",
    "    W = np.zeros((len(pred_ids), len(true_ids)), dtype=np.int64)\n",
    "    for i in range(pi.size):\n",
    "        W[pi[i], yi[i]] += 1\n",
    "    r, c = linear_sum_assignment(W.max() - W)\n",
    "    return float(W[r, c].sum() / pi.size)\n",
    "\n",
    "\n",
    "def _internal_indices(X: np.ndarray, labels: np.ndarray) -> Dict[str, float]:\n",
    "    out = {\"silhouette\": float(\"nan\"), \"davies_bouldin\": float(\"nan\"), \"calinski_harabasz\": float(\"nan\")}\n",
    "    uniq = np.unique(labels)\n",
    "    if len(uniq) <= 1 or len(uniq) >= len(labels):\n",
    "        return out\n",
    "    try: out[\"silhouette\"] = float(silhouette_score(X, labels))\n",
    "    except Exception: pass\n",
    "    try: out[\"davies_bouldin\"] = float(davies_bouldin_score(X, labels))\n",
    "    except Exception: pass\n",
    "    try: out[\"calinski_harabasz\"] = float(calinski_harabasz_score(X, labels))\n",
    "    except Exception: pass\n",
    "    return out\n",
    "\n",
    "\n",
    "def _evaluate_cluster_metrics(embeddings: np.ndarray, idxs: np.ndarray,\n",
    "                              location_labels: np.ndarray,\n",
    "                              location_entropy_base: Optional[int] = None) -> Dict[str, float]:\n",
    "    X = embeddings[idxs]\n",
    "    if X.shape[0] == 0:\n",
    "        return {'variance': 0.0, 'mean_sim': 1.0, 'entropy': 0.0, 'quality': 0.0, 'novelty': 0.0}\n",
    "    center = np.mean(X, axis=0, keepdims=True)\n",
    "    variance = float(np.mean(np.sum((X - center) ** 2, axis=1)))\n",
    "    if len(X) > 1:\n",
    "        cos_sim = cosine_similarity(X)\n",
    "        iu = np.triu_indices_from(cos_sim, k=1)\n",
    "        mean_sim = float(np.mean(cos_sim[iu])) if iu[0].size > 0 else 1.0\n",
    "    else:\n",
    "        mean_sim = 1.0\n",
    "    loc_counts = Counter(location_labels[idxs])\n",
    "    loc_probs = np.array(list(loc_counts.values()), dtype=np.float32)\n",
    "    loc_probs /= max(loc_probs.sum(), 1e-8)\n",
    "    base = int(location_entropy_base or len(set(location_labels)))\n",
    "    loc_entropy = float(entropy(loc_probs, base=base)) if base > 1 else 0.0\n",
    "    max_ent = np.log2(base) if base > 1 else 1.0\n",
    "    entropy_score = 1.0 - (loc_entropy / max_ent) if base > 1 else 1.0\n",
    "    quality = float((mean_sim / (variance + 1e-8)) * entropy_score)\n",
    "    novelty = float((loc_entropy / max_ent) * variance) if base > 1 else 0.0\n",
    "    return {'variance': variance, 'mean_sim': mean_sim,\n",
    "            'entropy': loc_entropy, 'quality': quality, 'novelty': novelty}\n",
    "\n",
    "\n",
    "# ========================= HTML bits =========================\n",
    "\n",
    "def _make_carousel(cluster_id: int, scope: str, imgs: List[str]) -> str:\n",
    "    cid = f\"carousel_{scope}_{cluster_id}\"\n",
    "    indicators = \"\".join(\n",
    "        f'<button type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide-to=\"{i}\" {\"class=active\" if i==0 else \"\"} aria-current=\"true\" aria-label=\"Slide {i+1}\"></button>'\n",
    "        for i in range(len(imgs))\n",
    "    )\n",
    "    items = \"\".join(\n",
    "        f'<div class=\"carousel-item {\"active\" if i==0 else \"\"}\"><div class=\"d-flex justify-content-center\">{img}</div></div>'\n",
    "        for i, img in enumerate(imgs)\n",
    "    )\n",
    "    return f\"\"\"\n",
    "    <div id=\"{cid}\" class=\"carousel slide\" data-bs-interval=\"false\" data-bs-touch=\"false\">\n",
    "      <div class=\"carousel-indicators\">{indicators}</div>\n",
    "      <div class=\"carousel-inner\">{items}</div>\n",
    "      <button class=\"carousel-control-prev\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"prev\">\n",
    "        <span class=\"carousel-control-prev-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Previous</span>\n",
    "      </button>\n",
    "      <button class=\"carousel-control-next\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"next\">\n",
    "        <span class=\"carousel-control-next-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Next</span>\n",
    "      </button>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# ========================= Core: analysis â†’ HTML =========================\n",
    "\n",
    "def analyze_pca_kmeans_to_html(\n",
    "    *,\n",
    "    dataset_paths: List[str],\n",
    "    location_tags: List[str],\n",
    "    n_components: int = 100,\n",
    "    random_state: int = 42,\n",
    "    kmeans_k: Optional[int] = None,\n",
    "    test_size: float = 0.2,\n",
    "    test_indices_map: Optional[Dict[str, Sequence[int]]] = None,  # tag -> indices\n",
    "    do_umap: bool = True,\n",
    "    umap_components: int = 3,\n",
    "    samples_per_cluster: int = 4,\n",
    "    top_n_clusters: Optional[int] = None,\n",
    "    sr: int = 16000,\n",
    "    n_mels: int = 64,\n",
    ") -> str:\n",
    "    assert len(dataset_paths) == len(location_tags), \"dataset_paths and location_tags must match\"\n",
    "\n",
    "    # 1) Build combined TEST set across locations\n",
    "    X_list, y_list, paths_list, loc_list = [], [], [], []\n",
    "\n",
    "    for path, tag in zip(dataset_paths, location_tags):\n",
    "        all_paths, labels = _sorted_paths_and_labels(path)\n",
    "        all_idx = np.arange(len(all_paths))\n",
    "        if test_indices_map and tag in test_indices_map:\n",
    "            test_idx = np.asarray(list(test_indices_map[tag]), dtype=int)\n",
    "        else:\n",
    "            _, test_idx = train_test_split(all_idx, test_size=test_size, stratify=labels, random_state=random_state)\n",
    "\n",
    "        X_test, y_test, used_paths = _load_logmel_subset(all_paths, labels, test_idx, sr=sr, n_mels=n_mels)\n",
    "        if X_test.size == 0:\n",
    "            continue\n",
    "\n",
    "        X_list.append(X_test)\n",
    "        y_list.append(y_test)\n",
    "        paths_list.append(np.array(used_paths, dtype=object))\n",
    "        loc_list.append(np.array([tag] * X_test.shape[0], dtype=object))\n",
    "\n",
    "    if not X_list:\n",
    "        raise ValueError(\"No TEST features loaded from provided datasets.\")\n",
    "\n",
    "    X = np.vstack(X_list).astype(np.float32)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    original_paths = np.concatenate(paths_list, axis=0)\n",
    "    location_labels = np.concatenate(loc_list, axis=0).astype(object)\n",
    "\n",
    "    # 2) Scale + PCA (fit on TEST only)\n",
    "    t0 = time.perf_counter()\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "    pca = PCA(n_components=n_components, random_state=random_state).fit(Xs)\n",
    "    Z = pca.transform(Xs)  # TEST embeddings\n",
    "    print(f\"[INFO] PCA on TEST: {Z.shape} â€¢ {(time.perf_counter()-t0):.2f}s\")\n",
    "\n",
    "    # 3) KMeans on TEST\n",
    "    k = int(kmeans_k) if kmeans_k is not None else len(np.unique(y))\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=random_state)\n",
    "    cluster_labels = kmeans.fit_predict(Z)\n",
    "\n",
    "    # 4) Global metrics\n",
    "    uniq = np.unique(cluster_labels)\n",
    "    valid = (len(uniq) > 1) and (Z.shape[0] > len(uniq))\n",
    "\n",
    "    def _safe(fn, *args):\n",
    "        try:\n",
    "            return float(fn(*args)) if valid else float(\"nan\")\n",
    "        except Exception:\n",
    "            return float(\"nan\")\n",
    "\n",
    "    sil = _safe(silhouette_score, Z, cluster_labels)\n",
    "    dbi = _safe(davies_bouldin_score, Z, cluster_labels)\n",
    "    ch  = _safe(calinski_harabasz_score, Z, cluster_labels)\n",
    "\n",
    "    # external\n",
    "    def _to_float(v): \n",
    "        try: return float(v)\n",
    "        except: return float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        ari  = _to_float(adjusted_rand_score(y, cluster_labels)) if valid else float(\"nan\")\n",
    "    except Exception:\n",
    "        ari = float(\"nan\")\n",
    "    try:\n",
    "        ami  = _to_float(adjusted_mutual_info_score(y, cluster_labels)) if valid else float(\"nan\")\n",
    "    except Exception:\n",
    "        ami = float(\"nan\")\n",
    "    try:\n",
    "        hacc = _to_float(_hungarian_accuracy(y, cluster_labels)) if valid else float(\"nan\")\n",
    "    except Exception:\n",
    "        hacc = float(\"nan\")\n",
    "\n",
    "    # 5) UMAP (on TEST embeddings)\n",
    "    if do_umap:\n",
    "        reducer = umap.UMAP(n_components=umap_components, random_state=random_state)\n",
    "        U = reducer.fit_transform(Z)\n",
    "        if umap_components == 3:\n",
    "            fig = px.scatter_3d(\n",
    "                x=U[:,0], y=U[:,1], z=U[:,2],\n",
    "                #color=[str(c) for c in cluster_labels],\n",
    "                color=y.astype(str),\n",
    "                symbol=location_labels,\n",
    "                hover_data={\"Cluster\": cluster_labels, \"Class\": y.astype(str)},\n",
    "                title=(f\"PCA+KMeans TEST (k={k}) | \"\n",
    "                       f\"Sil {sil:.3f} Â· DBI {dbi:.3f} Â· CH {ch:.1f} Â· \"\n",
    "                       f\"ARI {ari:.3f} Â· AMI {ami:.3f} Â· H-Acc {hacc:.3f}\"),\n",
    "                opacity=0.85, height=800\n",
    "            )\n",
    "        else:\n",
    "            fig = px.scatter(\n",
    "                x=U[:,0], y=U[:,1],\n",
    "                #color=[str(c) for c in cluster_labels],\n",
    "                color=y.astype(str),\n",
    "                symbol=location_labels,\n",
    "                hover_data={\"Cluster\": cluster_labels, \"Class\": y.astype(str)},\n",
    "                title=(f\"PCA+KMeans TEST (k={k}) | \"\n",
    "                       f\"Sil {sil:.3f} Â· DBI {dbi:.3f} Â· CH {ch:.1f} Â· \"\n",
    "                       f\"ARI {ari:.3f} Â· AMI {ami:.3f} Â· H-Acc {hacc:.3f}\"),\n",
    "                opacity=0.85, height=700\n",
    "            )\n",
    "        umap_html = pio.to_html(fig, include_plotlyjs=\"cdn\", full_html=False)\n",
    "    else:\n",
    "        umap_html = \"<p class='text-muted'>UMAP disabled.</p>\"\n",
    "\n",
    "    # 6) Per-cluster cards\n",
    "    base_for_entropy = len(set(location_labels))\n",
    "    cluster_ids = sorted(set(cluster_labels), key=lambda c: np.sum(cluster_labels==c), reverse=True)\n",
    "    if top_n_clusters is not None:\n",
    "        cluster_ids = cluster_ids[:int(top_n_clusters)]\n",
    "\n",
    "    spec_cos_scores, spec_dtw_scores = [], []\n",
    "    blocks = []\n",
    "\n",
    "    for cid in cluster_ids:\n",
    "        idxs = np.where(cluster_labels == cid)[0]\n",
    "        if idxs.size == 0:\n",
    "            continue\n",
    "\n",
    "        sz = len(idxs)\n",
    "        loc_counts = Counter(location_labels[idxs])\n",
    "        cls_counts = Counter(y[idxs])\n",
    "\n",
    "        metrics = _evaluate_cluster_metrics(Z, idxs, location_labels, location_entropy_base=base_for_entropy)\n",
    "        spec_cos = _avg_intra_cluster_spec_cosine(original_paths, idxs, fs=sr, max_samples=50)\n",
    "        spec_dtw = _avg_intra_cluster_spec_dtw(original_paths, idxs, fs=sr, max_samples=25)\n",
    "\n",
    "        if np.isfinite(spec_cos): spec_cos_scores.append(spec_cos)\n",
    "        if np.isfinite(spec_dtw): spec_dtw_scores.append(spec_dtw)\n",
    "\n",
    "        meta_html = \"<p><strong>Location Distribution:</strong></p><ul>\" + \"\".join(\n",
    "            f\"<li><b>{loc}</b>: {count} ({count/sz:.1%})</li>\" for loc, count in loc_counts.items()\n",
    "        ) + \"</ul>\"\n",
    "        meta_html += \"<p><strong>Class Distribution:</strong></p><ul>\" + \"\".join(\n",
    "            f\"<li>{str(cls)}: {count}</li>\" for cls, count in cls_counts.items()\n",
    "        ) + \"</ul>\"\n",
    "\n",
    "        meta_html += f\"\"\"\n",
    "        <p><strong>Cluster Metrics (Embedding Space):</strong></p>\n",
    "        <ul>\n",
    "          <li>Size: {sz}</li>\n",
    "          <li>Intra-Cluster Variance: {metrics['variance']:.4f}</li>\n",
    "          <li>Mean Cosine Similarity (Embeddings): {metrics['mean_sim']:.4f}</li>\n",
    "          <li>Location Entropy: {metrics['entropy']:.3f}</li>\n",
    "          <li>Composite Quality Score: {metrics['quality']:.4f}</li>\n",
    "          <li><strong>Novelty Score:</strong> {metrics['novelty']:.4f}</li>\n",
    "        </ul>\n",
    "        <p><strong>Spectrogram Consistency (Signal Space):</strong></p>\n",
    "        <ul>\n",
    "          <li>Mean Spectrogram Cosine Similarity (â†‘ better): {spec_cos if np.isfinite(spec_cos) else float('nan'):.4f}</li>\n",
    "          <li>Mean Spectrogram DTW Distance (â†“ better): {spec_dtw if np.isfinite(spec_dtw) else float('nan'):.2f}</li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "\n",
    "        # Nearest-to-center exemplars\n",
    "        center = np.mean(Z[idxs], axis=0, keepdims=True)\n",
    "        dists = np.linalg.norm(Z[idxs] - center, axis=1)\n",
    "        order = np.argsort(dists)\n",
    "        pick = idxs[order[:min(samples_per_cluster, len(order))]]\n",
    "\n",
    "        thumbs = []\n",
    "        for i, p in enumerate(pick):\n",
    "            try:\n",
    "                arr = np.load(original_paths[p], mmap_mode=\"r\")\n",
    "                title = f\"#{i+1} | {location_labels[p]} | Class {str(y[p])}\"\n",
    "                thumbs.append(_spec_img_base64(arr, title, fs=sr))\n",
    "            except Exception as e:\n",
    "                thumbs.append(f\"<p class='text-danger'>Error: {e}</p>\")\n",
    "\n",
    "        blocks.append(f\"<div class='col-md-6 mb-4'><h4>Cluster {cid}</h4>{meta_html}{_make_carousel(cid, 'pca', thumbs)}</div>\")\n",
    "\n",
    "    cluster_html = \"\"\n",
    "    for i in range(0, len(blocks), 2):\n",
    "        cluster_html += \"<div class='row'>\" + \"\".join(blocks[i:i+2]) + \"</div>\"\n",
    "\n",
    "    # 7) Global spectrogram-consistency summary\n",
    "    global_spec_cos = float(np.mean(spec_cos_scores)) if len(spec_cos_scores) else float(\"nan\")\n",
    "    global_spec_dtw = float(np.mean(spec_dtw_scores)) if len(spec_dtw_scores) else float(\"nan\")\n",
    "\n",
    "    summary_card = f\"\"\"\n",
    "    <div class='row mb-4'>\n",
    "      <div class='col-md-12'>\n",
    "        <div class='alert alert-info' role='alert'>\n",
    "          <h5 class='mb-2'>Spectrogram Consistency (Cluster Averages)</h5>\n",
    "          <ul class='mb-0'>\n",
    "            <li><strong>Mean Spectrogram Cosine Similarity</strong>: {global_spec_cos if np.isfinite(global_spec_cos) else float('nan'):.4f} (â†‘ better)</li>\n",
    "            <li><strong>Mean Spectrogram DTW Distance</strong>: {global_spec_dtw if np.isfinite(global_spec_dtw) else float('nan'):.2f} (â†“ better)</li>\n",
    "          </ul>\n",
    "          <small>Cosine computed on standardized spectrograms (z-norm; padded/cropped). DTW optional and subsampled.</small>\n",
    "        </div>\n",
    "      </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # Assemble section\n",
    "    title_txt = (f\"PCA+KMeans TEST (k={k}) â€¢ \"\n",
    "                 f\"Sil={sil:.3f} | DBI={dbi:.3f} | CH={ch:.1f} | \"\n",
    "                 f\"ARI={ari:.3f} | AMI={ami:.3f} | H-Acc={hacc:.3f}\")\n",
    "\n",
    "    return f\"\"\"\n",
    "    <div class='section'>\n",
    "      <h2>{title_txt}</h2>\n",
    "      <div class=\"container\">\n",
    "        <div class=\"row justify-content-center mb-4\">\n",
    "          <div class=\"col-md-12 d-flex justify-content-center\">{umap_html}</div>\n",
    "        </div>\n",
    "        {summary_card}\n",
    "      </div>\n",
    "      {cluster_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def generate_pca_kmeans_full_report(\n",
    "    *,\n",
    "    dataset_paths: List[str],\n",
    "    location_tags: List[str],\n",
    "    out_html: str = \"pca_kmeans_unsupervised_report.html\",\n",
    "    n_components: int = 100,\n",
    "    random_state: int = 42,\n",
    "    kmeans_k: Optional[int] = None,\n",
    "    test_size: float = 0.2,\n",
    "    test_indices_map: Optional[Dict[str, Sequence[int]]] = None,  # tag -> indices\n",
    "    do_umap: bool = True,\n",
    "    umap_components: int = 3,\n",
    "    samples_per_cluster: int = 4,\n",
    "    top_n_clusters: Optional[int] = None,\n",
    "    sr: int = 16000,\n",
    "    n_mels: int = 64,\n",
    ") -> str:\n",
    "    section = analyze_pca_kmeans_to_html(\n",
    "        dataset_paths=list(dataset_paths),\n",
    "        location_tags=list(location_tags),\n",
    "        n_components=n_components,\n",
    "        random_state=random_state,\n",
    "        kmeans_k=kmeans_k,\n",
    "        test_size=test_size,\n",
    "        test_indices_map=test_indices_map,\n",
    "        do_umap=do_umap,\n",
    "        umap_components=umap_components,\n",
    "        samples_per_cluster=samples_per_cluster,\n",
    "        top_n_clusters=top_n_clusters,\n",
    "        sr=sr,\n",
    "        n_mels=n_mels,\n",
    "    )\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Unsupervised Clustering Report (PCA + KMeans â€¢ TEST-ONLY)</title>\n",
    "        <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; padding: 20px; background-color: #f5f5f5; }}\n",
    "            h1 {{ color: #2c3e50; }}\n",
    "            h2, h4 {{ color: #34495e; }}\n",
    "            hr {{ border-top: 2px solid #bbb; margin-top: 40px; margin-bottom: 40px; }}\n",
    "            .section {{ margin-bottom: 60px; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "      <h1 class='mb-4'>Unsupervised Latent Clustering Report (PCA + KMeans â€¢ TEST-ONLY)</h1>\n",
    "      <p><strong>Locations:</strong> {', '.join(location_tags)}</p>\n",
    "      <p><strong>TEST split:</strong> {int(test_size*100)}% (stratified) â€¢ Random state: {random_state}</p>\n",
    "      <hr>\n",
    "      {section}\n",
    "    </body></html>\n",
    "    \"\"\"\n",
    "    with open(out_html, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"âœ… Report saved to: {out_html}\")\n",
    "    return out_html\n",
    "\n",
    "\n",
    "# ------------------------ Example direct call ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Single site example (adjust paths + tags)\n",
    "    report_path = generate_pca_kmeans_full_report(\n",
    "        dataset_paths=[\"/notebooks/dataset_preprocessed\"],\n",
    "        location_tags=[\"PR_U1137\"],\n",
    "        out_html=\"pca_k6.html\",\n",
    "        n_components=100,\n",
    "        random_state=42,\n",
    "        kmeans_k=60,\n",
    "        test_size=0.2,\n",
    "        test_indices_map=None,     # or {\"PR_U1137\": [list_of_test_indices]}\n",
    "        do_umap=True,\n",
    "        umap_components=3,\n",
    "        samples_per_cluster=10,\n",
    "        top_n_clusters=None,\n",
    "        sr=16000,\n",
    "        n_mels=64,\n",
    "    )\n",
    "    print(\"Report:\", report_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
