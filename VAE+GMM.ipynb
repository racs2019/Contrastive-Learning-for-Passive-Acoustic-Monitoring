{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbba9ab7-73b5-419d-8053-6a41a22e212b",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3d000c-ffd6-4626-9599-5dc7ed365f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:54:36.708468Z",
     "iopub.status.busy": "2025-09-23T18:54:36.707917Z",
     "iopub.status.idle": "2025-09-23T18:54:56.652655Z",
     "shell.execute_reply": "2025-09-23T18:54:56.651936Z",
     "shell.execute_reply.started": "2025-09-23T18:54:36.708449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.62.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.3)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.45.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2020.6.20)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (429 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m430.0/430.0 kB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.62.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.6/242.6 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.45.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soxr, msgpack, llvmlite, audioread, soundfile, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.11.0 llvmlite-0.45.0 msgpack-1.1.1 numba-0.62.0 pooch-1.8.2 soundfile-0.13.1 soxr-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting umap-learn\n",
      "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.11.2)\n",
      "Collecting scikit-learn>=1.6 (from umap-learn)\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.62.0)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.66.1)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.45.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6->umap-learn) (3.2.0)\n",
      "Downloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, pynndescent, umap-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.0\n",
      "    Uninstalling scikit-learn-1.3.0:\n",
      "      Successfully uninstalled scikit-learn-1.3.0\n",
      "Successfully installed pynndescent-0.5.13 scikit-learn-1.7.2 umap-learn-0.5.9.post2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting plotly\n",
      "  Downloading plotly-6.3.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (23.2)\n",
      "Downloading plotly-6.3.0-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.5.0-py3-none-any.whl (407 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m407.3/407.3 kB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-2.5.0 plotly-6.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting hdbscan\n",
      "  Downloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20->hdbscan) (3.2.0)\n",
      "Downloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hdbscan\n",
      "Successfully installed hdbscan-0.8.40\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n",
    "!pip install umap-learn\n",
    "!pip install plotly\n",
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310c60a-4a82-4080-85d8-2ed4dcad5b00",
   "metadata": {},
   "source": [
    "Report generation code\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c087e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T22:25:43.611524Z",
     "iopub.status.busy": "2025-09-23T22:25:43.611266Z",
     "iopub.status.idle": "2025-09-23T22:25:43.714275Z",
     "shell.execute_reply": "2025-09-23T22:25:43.713697Z",
     "shell.execute_reply.started": "2025-09-23T22:25:43.611504Z"
    }
   },
   "outputs": [],
   "source": [
    "# vae_runner.py\n",
    "\n",
    "import os, json, math, random, base64\n",
    "from io import BytesIO\n",
    "from typing import List, Tuple, Sequence, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "import torchaudio\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.signal import spectrogram, get_window\n",
    "\n",
    "# Safe optional deps for visualization\n",
    "try:\n",
    "    import umap.umap_ as umap\n",
    "    import plotly.express as px\n",
    "    import plotly.io as pio\n",
    "    import matplotlib.pyplot as plt\n",
    "    _HAVE_VIS = True\n",
    "except Exception:\n",
    "    _HAVE_VIS = False\n",
    "\n",
    "# ----------------------------\n",
    "# Repro & helpers\n",
    "# ----------------------------\n",
    "def _set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def _atomic_save(state: dict, path: str):\n",
    "    \"\"\"Save to a temp file then replace, to avoid partial/corrupt checkpoints.\"\"\"\n",
    "    tmp = path + \".tmp\"\n",
    "    torch.save(state, tmp)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def _load_resume_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "    ckpt_path: str,\n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"Load a training checkpoint and restore model/optimizer/scaler/epoch/best_val and RNG.\"\"\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    if scaler is not None and \"scaler_state_dict\" in ckpt and scaler.is_enabled():\n",
    "        scaler.load_state_dict(ckpt[\"scaler_state_dict\"])\n",
    "    start_epoch = int(ckpt.get(\"epoch\", 0)) + 1\n",
    "    best_val = float(ckpt.get(\"best_val\", math.inf))\n",
    "    # Restore RNG (optional)\n",
    "    try:\n",
    "        if \"rng_state\" in ckpt:\n",
    "            torch.set_rng_state(ckpt[\"rng_state\"][\"torch\"])\n",
    "            random.setstate(ckpt[\"rng_state\"][\"python\"])\n",
    "            np.random.set_state(ckpt[\"rng_state\"][\"numpy\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return start_epoch, best_val\n",
    "\n",
    "# ======================================================\n",
    "# One-click precompute helpers\n",
    "# ======================================================\n",
    "def _mel_pack_dir(audio_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the sibling directory where precomputed log-mels (.pt) live.\n",
    "    E.g., /data/siteA  ->  /data/siteA__mels64_fft1024_h512\n",
    "    \"\"\"\n",
    "    return f\"{audio_dir}__mels64_fft1024_h512\"\n",
    "\n",
    "def precompute_logmels_if_needed(\n",
    "    audio_dir: str,\n",
    "    *,\n",
    "    sr: int = 10000,\n",
    "    n_mels: int = 64,\n",
    "    n_fft: int = 1024,\n",
    "    hop_length: int = 512,\n",
    "    dtype: torch.dtype = torch.float16,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    If a precomputed mel pack does not exist, build it once.\n",
    "    Returns the mel_dir path.\n",
    "    \"\"\"\n",
    "    mel_dir = _mel_pack_dir(audio_dir)\n",
    "    labels_src = os.path.join(audio_dir, \"labels.npy\")\n",
    "    if not os.path.exists(labels_src):\n",
    "        raise FileNotFoundError(f\"labels.npy missing in {audio_dir}\")\n",
    "\n",
    "    os.makedirs(mel_dir, exist_ok=True)\n",
    "    labels_dst = os.path.join(mel_dir, \"labels.npy\")\n",
    "\n",
    "    raw_files = sorted(\n",
    "        [f for f in os.listdir(audio_dir) if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "        key=lambda x: int(os.path.splitext(x)[0])\n",
    "    )\n",
    "    pt_files = {f.replace(\".npy\", \".pt\") for f in os.listdir(mel_dir) if f.endswith(\".pt\")}\n",
    "    if len(pt_files) >= int(0.99 * len(raw_files)) and os.path.exists(labels_dst):\n",
    "        return mel_dir\n",
    "\n",
    "    print(f\"Precomputing log-mels into: {mel_dir} (this is one-time)\")\n",
    "    np.save(labels_dst, np.load(labels_src))\n",
    "\n",
    "    mel_tfm = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
    "    )\n",
    "    for f in raw_files:\n",
    "        out_pt = os.path.join(mel_dir, f.replace(\".npy\", \".pt\"))\n",
    "        if os.path.exists(out_pt):\n",
    "            continue\n",
    "        y = np.load(os.path.join(audio_dir, f)).astype(np.float32)\n",
    "        m = np.max(np.abs(y)) + 1e-8\n",
    "        y = y / m\n",
    "        y_t = torch.from_numpy(y).unsqueeze(0)  # [1, T]\n",
    "        with torch.no_grad():\n",
    "            logmel = torch.log(mel_tfm(y_t) + 1e-8).to(dtype)\n",
    "        torch.save(logmel.contiguous(), out_pt)\n",
    "\n",
    "    print(\"âœ… Log-mel precompute complete.\")\n",
    "    return mel_dir\n",
    "\n",
    "# ----------------------------\n",
    "# Datasets\n",
    "# ----------------------------\n",
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads 1D npy audio and returns log-mel tensor [1, 64, T].\n",
    "    (On-the-fly MelSpectrogram; slower than precomputed.)\n",
    "    \"\"\"\n",
    "    def __init__(self, audio_dir: str, sr: int = 10000, n_mels: int = 64, cache: bool = True):\n",
    "        self.audio_paths = sorted(\n",
    "            [os.path.join(audio_dir, f) for f in os.listdir(audio_dir)\n",
    "             if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "            key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "        )\n",
    "        labels_path = os.path.join(audio_dir, \"labels.npy\")\n",
    "        if not os.path.exists(labels_path):\n",
    "            raise FileNotFoundError(f\"labels.npy missing at {labels_path}\")\n",
    "        self.labels = np.load(labels_path)\n",
    "        if len(self.audio_paths) != len(self.labels):\n",
    "            raise ValueError(f\"{len(self.audio_paths)} audio files != {len(self.labels)}\")\n",
    "\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sr, n_fft=1024, hop_length=512, n_mels=n_mels\n",
    "        )\n",
    "        self.cache_enabled = cache\n",
    "        self.cache = [None] * len(self.audio_paths) if cache else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def _compute_logmel(self, idx: int) -> torch.Tensor:\n",
    "        y = np.load(self.audio_paths[idx]).astype(np.float32)\n",
    "        y = y / (np.max(np.abs(y)) + 1e-8)\n",
    "        y_tensor = torch.from_numpy(y).unsqueeze(0)  # [1, T]\n",
    "        mel = self.mel_transform(y_tensor)          # [1, 64, Tm]\n",
    "        logmel = torch.log(mel + 1e-8)\n",
    "        return logmel\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cache_enabled and self.cache[idx] is not None:\n",
    "            logmel = self.cache[idx]\n",
    "        else:\n",
    "            logmel = self._compute_logmel(idx)\n",
    "            if self.cache_enabled:\n",
    "                self.cache[idx] = logmel\n",
    "        label = int(self.labels[idx])\n",
    "        return logmel, label\n",
    "\n",
    "class PrecomputedMelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads precomputed log-mels saved as .pt tensors (shape [1, 64, T]).\n",
    "    \"\"\"\n",
    "    def __init__(self, mel_dir: str, cache: bool = False):\n",
    "        self.paths = sorted(\n",
    "            [os.path.join(mel_dir, f) for f in os.listdir(mel_dir) if f.endswith(\".pt\")],\n",
    "            key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "        )\n",
    "        labels_path = os.path.join(mel_dir, \"labels.npy\")\n",
    "        if not os.path.exists(labels_path):\n",
    "            raise FileNotFoundError(f\"labels.npy missing at {labels_path}\")\n",
    "        self.labels = np.load(labels_path)\n",
    "        if len(self.paths) != len(self.labels):\n",
    "            raise ValueError(f\"{len(self.paths)} mel files != {len(self.labels)} labels\")\n",
    "        self.cache = [None] * len(self.paths) if cache else None\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cache is not None and self.cache[idx] is not None:\n",
    "            x = self.cache[idx]\n",
    "        else:\n",
    "            x = torch.load(self.paths[idx], map_location=\"cpu\")  # [1, 64, T] (likely fp16)\n",
    "            if self.cache is not None:\n",
    "                self.cache[idx] = x\n",
    "        return x, int(self.labels[idx])\n",
    "\n",
    "# ----------------------------\n",
    "# Collate: pad/crop time dim to target_T\n",
    "# ----------------------------\n",
    "def _pad_or_crop_time(x: torch.Tensor, target_T: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [B, 1, 64, Tvar] -> [B, 1, 64, target_T]\n",
    "    \"\"\"\n",
    "    B, C, Freq, T = x.shape\n",
    "    if T == target_T:\n",
    "        return x\n",
    "    if T > target_T:\n",
    "        start = (T - target_T) // 2\n",
    "        return x[..., start:start+target_T]\n",
    "    total_pad = target_T - T\n",
    "    left = total_pad // 2\n",
    "    right = total_pad - left\n",
    "    return F.pad(x, (left, right), mode=\"constant\", value=0.0)\n",
    "\n",
    "def _collate_fixed_T(batch, target_T: int):\n",
    "    xs, ys = zip(*batch)\n",
    "    # Fast path: stack if all Ts match target_T\n",
    "    try:\n",
    "        Ts = {t.shape[-1] for t in xs}\n",
    "        if len(Ts) == 1 and list(Ts)[0] == target_T:\n",
    "            return torch.stack(xs, dim=0), torch.tensor(ys, dtype=torch.long)\n",
    "    except Exception:\n",
    "        pass\n",
    "    max_T = max(t.shape[-1] for t in xs)\n",
    "    stacked = torch.zeros(len(xs), 1, 64, max_T, dtype=xs[0].dtype)\n",
    "    for i, t in enumerate(xs):\n",
    "        T = t.shape[-1]\n",
    "        stacked[i, :, :, :T] = t\n",
    "    fixed = _pad_or_crop_time(stacked, target_T)\n",
    "    labels = torch.tensor(ys, dtype=torch.long)\n",
    "    return fixed, labels\n",
    "\n",
    "# ----------------------------\n",
    "# VAE\n",
    "# ----------------------------\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=64, input_shape=(1, 64, 63)):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            h = self.encoder_cnn(dummy_input)\n",
    "            self.encoder_output_shape = h.shape[1:]  # [C,H,W]\n",
    "            self.flattened_dim = h.numel()\n",
    "\n",
    "        self.encoder = nn.Sequential(self.encoder_cnn, nn.Flatten())\n",
    "        self.fc_mu = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, self.flattened_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, self.encoder_output_shape),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1), nn.Sigmoid(),\n",
    "            nn.Upsample(size=self.input_shape[1:], mode='bilinear', align_corners=False)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(self.fc_decode(z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# ----------------------------\n",
    "# Loss & metrics\n",
    "# ----------------------------\n",
    "def _vae_loss(x_recon, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "def _hungarian_accuracy(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(len(y_pred)):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return sum(w[i, j] for i, j in zip(row_ind, col_ind)) / len(y_pred)\n",
    "\n",
    "def _internal_indices(X: np.ndarray, labels: np.ndarray) -> Dict[str, float]:\n",
    "    out = {\"silhouette\": float(\"nan\"),\n",
    "           \"davies_bouldin\": float(\"nan\"),\n",
    "           \"calinski_harabasz\": float(\"nan\")}\n",
    "    n = len(labels)\n",
    "    k = len(np.unique(labels))\n",
    "    # need at least 2 clusters and fewer than n samples,\n",
    "    # and at least one cluster with >1 sample for silhouette\n",
    "    if k <= 1 or k >= n:\n",
    "        return out\n",
    "    try:\n",
    "        out[\"silhouette\"] = float(silhouette_score(X, labels))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        out[\"davies_bouldin\"] = float(davies_bouldin_score(X, labels))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        out[\"calinski_harabasz\"] = float(calinski_harabasz_score(X, labels))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "# ======================================================\n",
    "# Fixed split helper â€” do the stratified 80/20 split OUTSIDE training\n",
    "# ======================================================\n",
    "def make_fixed_train_test_indices(dataset_path: str, *, test_size: float = 0.2, random_state: int = 42) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    audio_paths = sorted(\n",
    "        [os.path.join(dataset_path, f) for f in os.listdir(dataset_path)\n",
    "         if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "        key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "    )\n",
    "    labels = np.load(os.path.join(dataset_path, \"labels.npy\"))\n",
    "    assert len(audio_paths) == len(labels), \"Mismatch between files and labels\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    indices = np.arange(len(audio_paths))\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        indices, test_size=test_size, stratify=labels, random_state=random_state\n",
    "    )\n",
    "    return np.asarray(train_idx), np.asarray(test_idx)\n",
    "\n",
    "# ----------------------------\n",
    "# Training (uses ONLY provided train indices)\n",
    "# ----------------------------\n",
    "def run_training(\n",
    "    audio_dir: str,\n",
    "    *,\n",
    "    train_indices: Sequence[int],\n",
    "    val_split: float = 0.1,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 256,\n",
    "    lr: float = 1e-3,\n",
    "    latent_dim: int = 64,\n",
    "    save_path: str = \"vae_model.pth\",\n",
    "    seed: int = 42,\n",
    "    amp: bool = True,\n",
    "    resume_from: Optional[str] = None,\n",
    "    checkpoint_path: str = \"vae_train.ckpt\",\n",
    "    precompute_mels: bool = True,\n",
    "    num_workers: Optional[int] = None,\n",
    "    grad_accum_steps: int = 1,\n",
    "    early_stop_patience: int = 10,\n",
    "):\n",
    "    _set_seed(seed)\n",
    "\n",
    "    dataset_path_for_training = audio_dir\n",
    "    use_precomputed = False\n",
    "    if precompute_mels:\n",
    "        try:\n",
    "            dataset_path_for_training = precompute_logmels_if_needed(audio_dir)\n",
    "            use_precomputed = True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Precompute failed or skipped, using on-the-fly mel spec: {e}\")\n",
    "\n",
    "    full_dataset = (\n",
    "        PrecomputedMelDataset(dataset_path_for_training, cache=False)\n",
    "        if use_precomputed else SpectrogramDataset(audio_dir)\n",
    "    )\n",
    "\n",
    "    model = ConvVAE(latent_dim=latent_dim)\n",
    "    target_T = model.input_shape[2]\n",
    "\n",
    "    train_base = Subset(full_dataset, list(train_indices))\n",
    "    val_size = max(1, int(round(val_split * len(train_base))))\n",
    "    tr_size = len(train_base) - val_size\n",
    "    if tr_size <= 0:\n",
    "        raise ValueError(f\"val_split too large for train size {len(train_base)}\")\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, val_dataset = random_split(train_base, [tr_size, val_size], generator=g)\n",
    "\n",
    "    if num_workers is None:\n",
    "        num_workers = max(2, (os.cpu_count() or 4) // 2)\n",
    "    loader_kwargs = dict(\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=4,\n",
    "        collate_fn=lambda b: _collate_fixed_T(b, target_T),\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **loader_kwargs)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, **loader_kwargs)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Fast math\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Make the model channels-last\n",
    "    model = model.to(device).to(memory_format=torch.channels_last)\n",
    "    # (keep torch.compile disabled to avoid cudagraph live-storage errors)\n",
    "    # try:\n",
    "    #     model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    # except Exception:\n",
    "    #     pass\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4, foreach=True)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp and device.type == \"cuda\")\n",
    "\n",
    "    # Resume support\n",
    "    start_epoch = 1\n",
    "    best_val = math.inf\n",
    "    if resume_from is not None and os.path.isfile(resume_from):\n",
    "        try:\n",
    "            start_epoch, best_val = _load_resume_checkpoint(model, optimizer, scaler, resume_from, device)\n",
    "            print(f\"ğŸ” Resumed from '{resume_from}' at epoch {start_epoch} (best_val={best_val:.2f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Failed to resume from '{resume_from}': {e}. Starting fresh.\")\n",
    "\n",
    "    # Warmup + cosine\n",
    "    warmup_epochs = max(1, epochs // 20)\n",
    "    def lr_lambda(e):\n",
    "        if e <= warmup_epochs: return e / float(warmup_epochs)\n",
    "        t = (e - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    history, bad = [], 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        for i, (x, _) in enumerate(train_loader):\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            #  Upcast any fp16 precomputed mels to fp32 to match model params\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.to(torch.float32)\n",
    "            #  Ensure consistent layout (channels-last)\n",
    "            x = x.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=amp and device.type == \"cuda\"):\n",
    "                x_recon, mu, logvar = model(x)\n",
    "                loss = _vae_loss(x_recon, x, mu, logvar) / max(1, grad_accum_steps)\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if (i + 1) % max(1, grad_accum_steps) == 0:\n",
    "                if scaler.is_enabled():\n",
    "                    scaler.step(optimizer); scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            total_loss += loss.item() * max(1, grad_accum_steps)\n",
    "\n",
    "        # Validation (sampled ~20% for speed)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for j, (x, _) in enumerate(val_loader):\n",
    "                if j > max(1, len(val_loader) // 5):\n",
    "                    break\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                if x.dtype != torch.float32:\n",
    "                    x = x.to(torch.float32)\n",
    "                x = x.contiguous(memory_format=torch.channels_last)\n",
    "                x_recon, mu, logvar = model(x)\n",
    "                val_loss += _vae_loss(x_recon, x, mu, logvar).item()\n",
    "\n",
    "        history.append({\"epoch\": epoch, \"train_loss_sum\": total_loss, \"val_loss_sum\": val_loss})\n",
    "\n",
    "        # Save BEST eval-only weights\n",
    "        if val_loss < best_val - 1e-6:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            print(f\" Epoch {epoch}: new best val {best_val:.2f} â€” saved {save_path}\")\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        # Always save a TRAINING CHECKPOINT (atomic overwrite) so we can resume later\n",
    "        rng_state = {\n",
    "            \"torch\": torch.get_rng_state(),\n",
    "            \"python\": random.getstate(),\n",
    "            \"numpy\": np.random.get_state(),\n",
    "        }\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val\": float(best_val),\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scaler_state_dict\": scaler.state_dict() if scaler.is_enabled() else None,\n",
    "            \"config\": {\n",
    "                \"audio_dir\": audio_dir,\n",
    "                \"latent_dim\": latent_dim,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"lr\": lr,\n",
    "                \"seed\": seed,\n",
    "                \"amp\": amp,\n",
    "                \"train_indices_len\": len(train_indices),\n",
    "                \"used_precomputed\": bool(use_precomputed),\n",
    "                \"precompute_pack\": dataset_path_for_training if use_precomputed else None,\n",
    "            },\n",
    "            \"rng_state\": rng_state,\n",
    "        }\n",
    "        _atomic_save(ckpt, checkpoint_path)\n",
    "        print(f\"ğŸ’¾ Epoch {epoch}: checkpoint saved â†’ {checkpoint_path}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if bad >= early_stop_patience:\n",
    "            print(f\"â¹ï¸ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"best_val_loss_sum\": float(best_val),\n",
    "        \"history\": history,\n",
    "        \"save_path\": save_path,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"seed\": seed,\n",
    "        \"audio_dir\": audio_dir,\n",
    "        \"train_samples\": len(train_dataset),\n",
    "        \"val_samples\": len(val_dataset),\n",
    "        \"last_checkpoint\": checkpoint_path,\n",
    "        \"last_epoch\": epoch,\n",
    "        \"used_precomputed\": bool(use_precomputed),\n",
    "        \"precompute_pack\": dataset_path_for_training if use_precomputed else None,\n",
    "    }\n",
    "\n",
    "def _preprocess_latents(latents: np.ndarray, *, l2: bool = True, pca_dim: Optional[int] = None, seed: int = 42):\n",
    "    Z = StandardScaler().fit_transform(latents.astype(np.float32))\n",
    "    if l2:\n",
    "        Z = normalize(Z, norm=\"l2\", axis=1)\n",
    "    if pca_dim is not None and Z.shape[1] > pca_dim:\n",
    "        Z = PCA(n_components=int(pca_dim), whiten=True, random_state=seed).fit_transform(Z)\n",
    "    return Z\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "def _hungarian_from_contingency(y_true, y_pred) -> float:\n",
    "    C = contingency_matrix(y_true, y_pred)  # rows=true, cols=pred\n",
    "    r, c = linear_sum_assignment(C.max() - C)\n",
    "    return float(C[r, c].sum() / C.sum())\n",
    "\n",
    "# ======================================================\n",
    "# Runner: INFERENCE / EVAL on FIXED TEST 20%\n",
    "# ======================================================\n",
    "def run_inference(\n",
    "    audio_dir: str,\n",
    "    *,\n",
    "    test_indices: Sequence[int],\n",
    "    checkpoint_path: str = \"vae_model.pth\",\n",
    "    batch_size: int = 256,\n",
    "    kmeans_k: Optional[int] = None,\n",
    "    do_umap: bool = True,\n",
    "    umap_components: int = 3,\n",
    "    umap_html_path: str = \"vae_test_umap.html\",\n",
    "    seed: int = 42,\n",
    "    l2_normalize: bool = True,\n",
    "    pca_dim: Optional[int] = None,   # e.g., 64 to match other pipeline\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load trained VAE, extract Î¼ on FIXED TEST subset, preprocess (Std->L2->PCA optional),\n",
    "    run KMeans with configurable k, and report ARI/AMI/H-Acc + Sil/DB/CH.\n",
    "    \"\"\"\n",
    "    _set_seed(seed)\n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "    # Prefer precomputed pack if it exists\n",
    "    mel_dir = _mel_pack_dir(audio_dir)\n",
    "    if os.path.isdir(mel_dir) and os.path.exists(os.path.join(mel_dir, \"labels.npy\")):\n",
    "        full_dataset = PrecomputedMelDataset(mel_dir, cache=False)\n",
    "        chosen_src = mel_dir\n",
    "    else:\n",
    "        full_dataset = SpectrogramDataset(audio_dir, cache=True)\n",
    "        chosen_src = audio_dir\n",
    "\n",
    "    # DEBUG: print label distribution for sanity\n",
    "    y_all = getattr(full_dataset, \"labels\", None)\n",
    "    if y_all is None:\n",
    "        print(\"â— Dataset has no labels attribute (this would break AMI/ARI).\")\n",
    "    else:\n",
    "        vals, cnts = np.unique(y_all, return_counts=True)\n",
    "        print(f\"[INFO] Inference source dir: {chosen_src}\")\n",
    "        print(\"[INFO] Full label dist:\", dict(zip(vals.tolist(), cnts.tolist())))\n",
    "        tvals, tcnts = np.unique(y_all[np.asarray(test_indices)], return_counts=True)\n",
    "        print(\"[INFO] TEST label dist:\", dict(zip(tvals.tolist(), tcnts.tolist())))\n",
    "\n",
    "    # Build loader over fixed TEST indices\n",
    "    model = ConvVAE(latent_dim=model_latent_dim_from_ckpt(checkpoint_path) or 64)\n",
    "    target_T = model.input_shape[2]\n",
    "    test_subset = Subset(full_dataset, list(test_indices))\n",
    "    num_workers = max(2, (os.cpu_count() or 4) // 2)\n",
    "    test_loader = DataLoader(\n",
    "        test_subset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True, persistent_workers=True,\n",
    "        prefetch_factor=4,\n",
    "        collate_fn=lambda b: _collate_fixed_T(b, target_T)\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model = model.to(device).to(memory_format=torch.channels_last).eval()\n",
    "\n",
    "    # Extract TEST Î¼\n",
    "    latents, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.to(torch.float32)\n",
    "            x = x.contiguous(memory_format=torch.channels_last)\n",
    "            mu, _ = model.encode(x)\n",
    "            latents.append(mu.cpu().numpy())\n",
    "            true_labels.append(y.numpy())\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    # === Preprocess Î¼ like the other report ===\n",
    "    Z = _preprocess_latents(latents, l2=l2_normalize, pca_dim=pca_dim, seed=seed)\n",
    "\n",
    "    # K for clustering\n",
    "    n_clusters = int(kmeans_k) if kmeans_k is not None else int(len(np.unique(true_labels)))\n",
    "\n",
    "    # Cluster on Z (not raw Î¼)\n",
    "    from sklearn.cluster import KMeans\n",
    "    pred_labels = KMeans(n_clusters=n_clusters, n_init=20, random_state=seed).fit_predict(Z)\n",
    "\n",
    "    # Metrics\n",
    "    uniq_pred = np.unique(pred_labels)\n",
    "    valid_internal = (len(uniq_pred) > 1) and (Z.shape[0] > len(uniq_pred))\n",
    "    extra = {\n",
    "        \"silhouette\": float(silhouette_score(Z, pred_labels)) if valid_internal else float(\"nan\"),\n",
    "        \"davies_bouldin\": float(davies_bouldin_score(Z, pred_labels)) if valid_internal else float(\"nan\"),\n",
    "        \"calinski_harabasz\": float(calinski_harabasz_score(Z, pred_labels)) if valid_internal else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "    u_true = np.unique(true_labels).size\n",
    "    u_pred = np.unique(pred_labels).size\n",
    "    if u_true < 2 or u_pred < 2:\n",
    "        ari = float(\"nan\")\n",
    "        ami = float(\"nan\")\n",
    "        # H-Acc still meaningful if true has >= 1 class\n",
    "        hung = _hungarian_from_contingency(true_labels, pred_labels) if u_true >= 1 else float(\"nan\")\n",
    "    else:\n",
    "        ari = float(adjusted_rand_score(true_labels, pred_labels))\n",
    "        ami = float(adjusted_mutual_info_score(true_labels, pred_labels))\n",
    "        hung = _hungarian_from_contingency(true_labels, pred_labels)\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"test_samples\": int(len(true_labels)),\n",
    "        \"latents_shape\": tuple(latents.shape),\n",
    "        \"processed_shape\": tuple(Z.shape),\n",
    "        \"kmeans_k\": int(n_clusters),\n",
    "        \"metrics\": {\n",
    "            \"ari\": float(ari),\n",
    "            \"ami\": float(ami),\n",
    "            \"hungarian_accuracy\": float(hung),\n",
    "            \"silhouette\": float(extra[\"silhouette\"]),\n",
    "            \"davies_bouldin\": float(extra[\"davies_bouldin\"]),\n",
    "            \"calinski_harabasz\": float(extra[\"calinski_harabasz\"]),\n",
    "        },\n",
    "        \"checkpoint_path\": checkpoint_path,\n",
    "        \"preprocess\": {\n",
    "            \"l2_normalize\": bool(l2_normalize),\n",
    "            \"pca_dim\": int(pca_dim) if pca_dim is not None else None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Optional UMAP on processed Z\n",
    "    if do_umap and _HAVE_VIS:\n",
    "        if umap_components not in (2, 3):\n",
    "            raise ValueError(\"umap_components must be 2 or 3\")\n",
    "        reducer = umap.UMAP(n_components=umap_components, metric=\"cosine\", random_state=seed)\n",
    "        z_umap = reducer.fit_transform(Z)\n",
    "        title_bits = [f\"k={n_clusters}\"]\n",
    "        if l2_normalize: title_bits.append(\"L2\")\n",
    "        if pca_dim: title_bits.append(f\"PCA{pca_dim}-whiten\")\n",
    "        title = \"VAE Test Latents (\" + \", \".join(title_bits) + f\") | ARI {ari:.4f}, AMI {ami:.4f}, H-Acc {hung:.4f}\"\n",
    "        if umap_components == 3:\n",
    "            fig = px.scatter_3d(x=z_umap[:,0], y=z_umap[:,1], z=z_umap[:,2],\n",
    "                                color=true_labels.astype(str), title=title)\n",
    "        else:\n",
    "            fig = px.scatter(x=z_umap[:,0], y=z_umap[:,1],\n",
    "                             color=true_labels.astype(str), title=title)\n",
    "        fig.write_html(umap_html_path)\n",
    "        print(f\"Saved UMAP visualization to '{umap_html_path}'\")\n",
    "        results[\"umap_html_path\"] = umap_html_path\n",
    "\n",
    "    return results\n",
    "\n",
    "def model_latent_dim_from_ckpt(_path: str) -> Optional[int]:\n",
    "    \"\"\"(Optional) Heuristic placeholder if you encode latent_dim in filename; otherwise return None.\"\"\"\n",
    "    try:\n",
    "        base = os.path.basename(_path)\n",
    "        if \"lat\" in base:\n",
    "            import re\n",
    "            m = re.search(r\"lat(\\d+)\", base)\n",
    "            if m: return int(m.group(1))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Report helpers (unchanged behavior; safe UMAP usage)\n",
    "# ----------------------------\n",
    "def _ali_spec(x, fs):\n",
    "    Lframe2 = 1000\n",
    "    po = 80\n",
    "    lov = int(np.ceil((po / 100) * Lframe2))\n",
    "    taper = get_window('hann', Lframe2)\n",
    "    Nfft = 2 ** (int(np.floor(np.log2(Lframe2))) + 2)\n",
    "    f, t, s = spectrogram(x, fs=fs, window=taper, noverlap=lov, nfft=Nfft, mode='complex')\n",
    "    as_ = np.abs(s)\n",
    "    as_max = np.max(as_)\n",
    "    sdb = 10 * np.log10(100 * as_ / as_max + 1e-10)\n",
    "    min_inx = np.argmin(np.abs(f - 0))\n",
    "    max_inx = np.argmin(np.abs(f - 800))\n",
    "    return sdb[min_inx:max_inx+1, :], f[min_inx:max_inx+1], t\n",
    "\n",
    "def _generate_spectrogram_base64(audio, fs=10000, title=\"Spectrogram\"):\n",
    "    if not _HAVE_VIS:\n",
    "        return \"<p>Matplotlib not available.</p>\"\n",
    "    spec, f_axis, t_axis = _ali_spec(audio, fs)\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.imshow(spec, aspect='auto', origin='lower',\n",
    "              extent=[t_axis[0], t_axis[-1], f_axis[0], f_axis[-1]],\n",
    "              cmap='hsv')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    fig.tight_layout()\n",
    "    buf = BytesIO()\n",
    "    fig.savefig(buf, format='png', dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    image_base64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return f'<img class=\"d-block w-100\" src=\"data:image/png;base64,{image_base64}\" alt=\"{title}\">'\n",
    "\n",
    "def _make_carousel(cluster_id, class_id, encoded_imgs):\n",
    "    carousel_id = f\"carousel_class{class_id}_cluster{cluster_id}\"\n",
    "    indicators = \"\".join([\n",
    "        f'<button type=\"button\" data-bs-target=\"#{carousel_id}\" data-bs-slide-to=\"{i}\" class=\"{ \"active\" if i==0 else \"\" }\" aria-current=\"{ \"true\" if i==0 else \"false\" }\" aria-label=\"Slide {i+1}\"></button>'\n",
    "        for i in range(len(encoded_imgs))\n",
    "    ])\n",
    "    slides = \"\".join([\n",
    "        f'''<div class=\"carousel-item {'active' if i==0 else ''}\">{img}</div>'''\n",
    "        for i, img in enumerate(encoded_imgs)\n",
    "    ])\n",
    "    return f'''\n",
    "    <div id=\"{carousel_id}\" class=\"carousel slide\" data-bs-ride=\"carousel\">\n",
    "      <div class=\"carousel-indicators\">{indicators}</div>\n",
    "      <div class=\"carousel-inner\">{slides}</div>\n",
    "      <button class=\"carousel-control-prev\" type=\"button\" data-bs-target=\"#{carousel_id}\" data-bs-slide=\"prev\">\n",
    "        <span class=\"carousel-control-prev-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Previous</span>\n",
    "      </button>\n",
    "      <button class=\"carousel-control-next\" type=\"button\" data-bs-target=\"#{carousel_id}\" data-bs-slide=\"next\">\n",
    "        <span class=\"carousel-control-next-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Next</span>\n",
    "      </button>\n",
    "    </div>\n",
    "    '''\n",
    "\n",
    "@torch.no_grad()\n",
    "def _extract_embeddings(vae: ConvVAE, dataloader: DataLoader, device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    vae.eval()\n",
    "    embeddings, labels = [], []\n",
    "    for x, lbl in dataloader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        if x.dtype != torch.float32:\n",
    "            x = x.to(torch.float32)\n",
    "        x = x.contiguous(memory_format=torch.channels_last)\n",
    "        mu, _ = vae.encode(x)\n",
    "        embeddings.append(mu.cpu().numpy())\n",
    "        labels.extend(lbl.numpy())\n",
    "    return np.concatenate(embeddings), np.array(labels)\n",
    "\n",
    "def _analyze_class_with_gmm(vae, class_id, dataset_paths, labels_list, device, gmm_components=10):\n",
    "    if not _HAVE_VIS:\n",
    "        return f\"<div class='section'><h2>Analysis for Class {class_id}</h2><p>Visualization dependencies not available.</p></div>\"\n",
    "    embeddings_all, loc_labels_all, original_paths = [], [], []\n",
    "    target_T = vae.input_shape[2]\n",
    "\n",
    "    for path, loc_label in zip(dataset_paths, labels_list):\n",
    "        # prefer precomputed pack if it exists\n",
    "        mel_dir = _mel_pack_dir(path)\n",
    "        if os.path.isdir(mel_dir) and os.path.exists(os.path.join(mel_dir, \"labels.npy\")):\n",
    "            ds = PrecomputedMelDataset(mel_dir, cache=False)\n",
    "            idxs = np.arange(len(ds))\n",
    "            loader = DataLoader(ds, batch_size=64, shuffle=False,\n",
    "                                num_workers=max(2, (os.cpu_count() or 4)//2),\n",
    "                                pin_memory=True, persistent_workers=True, prefetch_factor=4,\n",
    "                                collate_fn=lambda b: _collate_fixed_T(b, target_T))\n",
    "            emb, labels = _extract_embeddings(vae, loader, device)\n",
    "            # we don't have original raw paths here; skip carousels if needed\n",
    "            raw_paths = np.array([None]*len(ds))\n",
    "        else:\n",
    "            ds = SpectrogramDataset(path, cache=True)\n",
    "            idxs = np.arange(len(ds))\n",
    "            loader = DataLoader(ds, batch_size=64, shuffle=False,\n",
    "                                num_workers=max(2, (os.cpu_count() or 4)//2),\n",
    "                                pin_memory=True, persistent_workers=True, prefetch_factor=4,\n",
    "                                collate_fn=lambda b: _collate_fixed_T(b, target_T))\n",
    "            emb, labels = _extract_embeddings(vae, loader, device)\n",
    "            raw_paths = np.array([os.path.join(path, f\"{i}.npy\") for i in idxs])\n",
    "\n",
    "        mask = labels == class_id\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "        embeddings_all.append(emb[mask])\n",
    "        loc_labels_all.append(np.array([loc_label] * int(np.sum(mask))))\n",
    "        original_paths.append(raw_paths[mask])\n",
    "\n",
    "    if len(embeddings_all) == 0:\n",
    "        return f\"<div class='section'><h2>Analysis for Class {class_id}</h2><p>No samples for this class.</p></div>\"\n",
    "\n",
    "    embeddings = np.vstack(embeddings_all)\n",
    "    location_labels = np.concatenate(loc_labels_all)\n",
    "    original_paths_flat = np.concatenate(original_paths)\n",
    "\n",
    "    reducer = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=42)\n",
    "    proj_3d = reducer.fit_transform(embeddings)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=gmm_components, covariance_type=\"full\", random_state=0)\n",
    "    cluster_labels = gmm.fit_predict(embeddings)\n",
    "    sil = float(silhouette_score(embeddings, cluster_labels))\n",
    "\n",
    "    umap_fig = px.scatter_3d(\n",
    "        x=proj_3d[:, 0], y=proj_3d[:, 1], z=proj_3d[:, 2],\n",
    "        color=[str(int(c)) for c in cluster_labels],\n",
    "        symbol=list(location_labels),\n",
    "        title=f\"Class {class_id} - GMM Clusters (Silhouette={sil:.2f})\",\n",
    "        opacity=0.85, height=800\n",
    "    )\n",
    "    umap_html = pio.to_html(umap_fig, include_plotlyjs=\"cdn\", full_html=False)\n",
    "\n",
    "    cluster_blocks = []\n",
    "    for c in range(gmm_components):\n",
    "        idxs = np.where(cluster_labels == c)[0]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "        cluster_counts = {loc: int(np.sum(location_labels[idxs] == loc)) for loc in set(location_labels)}\n",
    "        total = len(idxs)\n",
    "        cluster_label_html = \"<ul>\" + \"\".join(\n",
    "            [f\"<li>{loc}: {count} ({count/total:.2%})</li>\" for loc, count in cluster_counts.items()]\n",
    "        ) + \"</ul>\"\n",
    "\n",
    "        center = np.mean(embeddings[idxs], axis=0, keepdims=True)\n",
    "        distances = np.linalg.norm(embeddings[idxs] - center, axis=1)\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        sampled_idxs = idxs[sorted_indices[:min(10, len(sorted_indices))]]\n",
    "\n",
    "        imgs = []\n",
    "        for i, chosen_idx in enumerate(sampled_idxs):\n",
    "            try:\n",
    "                if original_paths_flat[chosen_idx] is None:\n",
    "                    imgs.append(f\"<p>Precomputed dataset: raw audio path unavailable.</p>\")\n",
    "                else:\n",
    "                    audio = np.load(original_paths_flat[chosen_idx])\n",
    "                    label = location_labels[chosen_idx]\n",
    "                    imgs.append(_generate_spectrogram_base64(audio, title=f\"#{i+1} ({label})\"))\n",
    "            except Exception as e:\n",
    "                imgs.append(f\"<p>Error loading sample {i+1}: {e}</p>\")\n",
    "\n",
    "        carousel_html = _make_carousel(c, class_id, imgs)\n",
    "        block = f\"<div class='col-md-6 mb-4'><h4>Cluster {c}</h4>{cluster_label_html}{carousel_html}</div>\"\n",
    "        cluster_blocks.append(block)\n",
    "\n",
    "    cluster_html = \"\"\n",
    "    for i in range(0, len(cluster_blocks), 2):\n",
    "        cluster_html += \"<div class='row'>\" + \"\".join(cluster_blocks[i:i+2]) + \"</div>\"\n",
    "\n",
    "    return f\"\"\"<div class='section'>\n",
    "        <h2>Analysis for Class {class_id}</h2>\n",
    "        <div class=\"container\">\n",
    "            <div class=\"row justify-content-center mb-4\">\n",
    "                <div class=\"col-md-12 d-flex justify-content-center\">{umap_html}</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        {cluster_html}\n",
    "    </div>\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Public API: Report (unchanged interface)\n",
    "# ----------------------------\n",
    "def generate_report(\n",
    "    model_path: str,\n",
    "    dataset_paths: List[str],\n",
    "    labels: List[str],\n",
    "    classes: List[int] = (1,2,3,4,5,6),\n",
    "    output_html: str = \"vae_gmm_report.html\",\n",
    "    latent_dim: int = 64,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Load a trained VAE and write an interactive UMAP+GMM HTML report.\n",
    "    Returns the output path.\n",
    "    \"\"\"\n",
    "    if len(dataset_paths) != len(labels):\n",
    "        raise ValueError(\"`dataset_paths` and `labels` must have the same length\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vae = ConvVAE(latent_dim=latent_dim).to(device).to(memory_format=torch.channels_last)\n",
    "    vae.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    vae.eval()\n",
    "\n",
    "    class_names = {\n",
    "        1: 'Red Hind',\n",
    "        2: 'Nassau Grouper',\n",
    "        3: 'Black Grouper',\n",
    "        4: 'Yellow Fin Grouper',\n",
    "        5: 'Squirrel Fish',\n",
    "        6: 'Other Sounds'\n",
    "    }\n",
    "\n",
    "    html = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Latent Space Clustering Report (VAE + GMM)</title>\n",
    "        <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; padding: 20px; background-color: #f5f5f5; }\n",
    "            h1 { color: #2c3e50; }\n",
    "            h2, h4 { color: #34495e; }\n",
    "            .section { margin-bottom: 60px; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1 class='mb-4'>VAE + GMM Clustering Report</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    for class_id in classes:\n",
    "        class_title = class_names.get(class_id, f\"Class {class_id}\")\n",
    "        html += f\"<h2>{class_title} (Class {class_id})</h2>\"\n",
    "        html += _analyze_class_with_gmm(vae, class_id, dataset_paths, labels, device)\n",
    "        html += \"<hr>\"\n",
    "\n",
    "    html += \"</body></html>\"\n",
    "    with open(output_html, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    return output_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0874d4-a21a-4f22-acc7-6cc95016ce5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:55:33.548509Z",
     "iopub.status.busy": "2025-09-23T18:55:33.548132Z",
     "iopub.status.idle": "2025-09-23T18:55:33.590795Z",
     "shell.execute_reply": "2025-09-23T18:55:33.590065Z",
     "shell.execute_reply.started": "2025-09-23T18:55:33.548492Z"
    }
   },
   "outputs": [],
   "source": [
    "# vae_unsup_report.py\n",
    "# Self-contained helpers + class-agnostic, K-controlled report for VAE Î¼-embeddings.\n",
    "\n",
    "import os, math, base64\n",
    "from io import BytesIO\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Optional viz deps\n",
    "try:\n",
    "    import umap.umap_ as umap\n",
    "    import plotly.express as px\n",
    "    import plotly.io as pio\n",
    "    import matplotlib.pyplot as plt\n",
    "    _HAVE_VIZ = True\n",
    "except Exception:\n",
    "    _HAVE_VIZ = False\n",
    "\n",
    "# ----------------------------\n",
    "# Repro & small utils\n",
    "# ----------------------------\n",
    "def _set_seed(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def _mel_pack_dir(audio_dir: str) -> str:\n",
    "    \"\"\"Companion directory name used by your precompute step.\"\"\"\n",
    "    return f\"{audio_dir}__mels64_fft1024_h512\"\n",
    "\n",
    "def _choose_subset(total_n: int, fraction: float = 0.2, seed: int = 42, strategy: str = \"random\"):\n",
    "    \"\"\"\n",
    "    Returns sorted indices for a subset of size ceil(total_n * fraction).\n",
    "    strategy: 'random' | 'head' | 'tail'\n",
    "    \"\"\"\n",
    "    n_sub = max(1, int(math.ceil(total_n * float(fraction))))\n",
    "    idx = np.arange(total_n)\n",
    "    if strategy == \"random\":\n",
    "        rng = np.random.RandomState(seed)\n",
    "        chosen = np.sort(rng.choice(total_n, size=n_sub, replace=False))\n",
    "    elif strategy == \"head\":\n",
    "        chosen = idx[:n_sub]\n",
    "    elif strategy == \"tail\":\n",
    "        chosen = idx[-n_sub:]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown subset strategy: {strategy}\")\n",
    "    return chosen\n",
    "\n",
    "# ----------------------------\n",
    "# Datasets\n",
    "# ----------------------------\n",
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads .npy files (1D waveform or 2D spectrogram-like) and returns log-mels [1,64,T].\n",
    "    If labels.npy exists, it is read but not required (unsupervised mode).\n",
    "    \"\"\"\n",
    "    def __init__(self, audio_dir: str, sr: int = 10000, n_mels: int = 64, cache: bool = True):\n",
    "        self.audio_paths = sorted(\n",
    "            [os.path.join(audio_dir, f) for f in os.listdir(audio_dir)\n",
    "             if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "            key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "        )\n",
    "        self.labels = None\n",
    "        labels_path = os.path.join(audio_dir, \"labels.npy\")\n",
    "        if os.path.exists(labels_path):\n",
    "            try:\n",
    "                self.labels = np.load(labels_path, allow_pickle=True)\n",
    "                if len(self.labels) != len(self.audio_paths):\n",
    "                    self.labels = None  # ignore mismatched labels\n",
    "            except Exception:\n",
    "                self.labels = None\n",
    "\n",
    "        # torchaudio mel\n",
    "        try:\n",
    "            import torchaudio\n",
    "            self._have_ta = True\n",
    "            self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=sr, n_fft=1024, hop_length=512, n_mels=n_mels\n",
    "            )\n",
    "        except Exception:\n",
    "            self._have_ta = False\n",
    "            from scipy.signal import spectrogram, get_window\n",
    "            self._sp_spec = (spectrogram, get_window)\n",
    "            self._sr = sr\n",
    "            self._n_mels = n_mels\n",
    "\n",
    "        self.cache_enabled = cache\n",
    "        self.cache = [None] * len(self.audio_paths) if cache else None\n",
    "\n",
    "    def __len__(self): return len(self.audio_paths)\n",
    "\n",
    "    def _wave_to_logmel(self, y: np.ndarray) -> torch.Tensor:\n",
    "        y = np.nan_to_num(y.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        m = np.max(np.abs(y)); m = 1.0 if (not np.isfinite(m) or m < 1e-8) else m\n",
    "        y_t = torch.from_numpy(y / m).unsqueeze(0)  # [1, T]\n",
    "        if self._have_ta:\n",
    "            mel = self.mel_transform(y_t)\n",
    "            return torch.log(torch.clamp(mel, min=1e-8))\n",
    "        # Fallback: SciPy STFT â†’ log-mag, then linear-bin subsample to n_mels\n",
    "        spectrogram, get_window = self._sp_spec\n",
    "        win = get_window('hann', 1024, fftbins=True)\n",
    "        f, t, Z = spectrogram(y, fs=self._sr, window=win, nperseg=1024,\n",
    "                              noverlap=1024-512, mode='magnitude')\n",
    "        S = np.log(np.maximum(Z, 1e-10)).astype(np.float32)  # (F_lin, T)\n",
    "        if S.shape[0] != self._n_mels:\n",
    "            idx = np.linspace(0, S.shape[0]-1, num=self._n_mels).astype(np.int32)\n",
    "            S = S[idx]\n",
    "        return torch.from_numpy(S).unsqueeze(0)\n",
    "\n",
    "    def _to_logmel(self, arr: np.ndarray) -> torch.Tensor:\n",
    "        if arr.ndim == 1:\n",
    "            return self._wave_to_logmel(arr)\n",
    "        if arr.ndim == 2:\n",
    "            x = torch.from_numpy(arr.astype(np.float32)).unsqueeze(0)\n",
    "            x = torch.clamp(x, min=1e-8).log()\n",
    "            return x\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "            x = torch.from_numpy(arr.astype(np.float32))\n",
    "            x = torch.clamp(x, min=1e-8).log()\n",
    "            return x\n",
    "        raise RuntimeError(f\"Unexpected array shape {arr.shape}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cache_enabled and self.cache[idx] is not None:\n",
    "            x = self.cache[idx]\n",
    "        else:\n",
    "            arr = np.load(self.audio_paths[idx], mmap_mode=\"r\")\n",
    "            x = self._to_logmel(arr).contiguous()\n",
    "            if self.cache_enabled:\n",
    "                self.cache[idx] = x\n",
    "        label = 0 if self.labels is None else self.labels[idx]\n",
    "        return x, label\n",
    "\n",
    "class PrecomputedMelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads precomputed log-mels saved as .pt tensors (shape [1, 64, T]).\n",
    "    labels.npy is optional and ignored for unsupervised clustering.\n",
    "    \"\"\"\n",
    "    def __init__(self, mel_dir: str, cache: bool = False):\n",
    "        self.paths = sorted(\n",
    "            [os.path.join(mel_dir, f) for f in os.listdir(mel_dir) if f.endswith(\".pt\")],\n",
    "            key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "        )\n",
    "        self.cache = [None] * len(self.paths) if cache else None\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cache is not None and self.cache[idx] is not None:\n",
    "            x = self.cache[idx]\n",
    "        else:\n",
    "            x = torch.load(self.paths[idx], map_location=\"cpu\")\n",
    "            if self.cache is not None:\n",
    "                self.cache[idx] = x\n",
    "        return x, 0  # dummy label\n",
    "\n",
    "# ----------------------------\n",
    "# Collate: pad/crop time to target_T\n",
    "# ----------------------------\n",
    "def _pad_or_crop_time(x: torch.Tensor, target_T: int) -> torch.Tensor:\n",
    "    B, C, Freq, T = x.shape\n",
    "    if T == target_T:\n",
    "        return x\n",
    "    if T > target_T:\n",
    "        start = (T - target_T) // 2\n",
    "        return x[..., start:start+target_T]\n",
    "    total_pad = target_T - T\n",
    "    left = total_pad // 2\n",
    "    right = total_pad - left\n",
    "    return F.pad(x, (left, right), mode=\"constant\", value=0.0)\n",
    "\n",
    "def _collate_fixed_T(batch, target_T: int):\n",
    "    xs, ys = zip(*batch)\n",
    "    try:\n",
    "        Ts = {t.shape[-1] for t in xs}\n",
    "        if len(Ts) == 1 and list(Ts)[0] == target_T:\n",
    "            return torch.stack(xs, dim=0), torch.tensor(ys)\n",
    "    except Exception:\n",
    "        pass\n",
    "    max_T = max(t.shape[-1] for t in xs)\n",
    "    stacked = torch.zeros(len(xs), 1, 64, max_T, dtype=xs[0].dtype)\n",
    "    for i, t in enumerate(xs):\n",
    "        T = t.shape[-1]; stacked[i, :, :, :T] = t\n",
    "    fixed = _pad_or_crop_time(stacked, target_T)\n",
    "    return fixed, torch.tensor(ys)\n",
    "\n",
    "# ----------------------------\n",
    "# VAE\n",
    "# ----------------------------\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=64, input_shape=(1, 64, 63)):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *input_shape)\n",
    "            h = self.encoder_cnn(dummy)\n",
    "            self.encoder_output_shape = h.shape[1:]\n",
    "            self.flattened_dim = h.numel()\n",
    "        self.encoder = nn.Sequential(self.encoder_cnn, nn.Flatten())\n",
    "        self.fc_mu = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, self.flattened_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, self.encoder_output_shape),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1), nn.Sigmoid(),\n",
    "            nn.Upsample(size=self.input_shape[1:], mode='bilinear', align_corners=False)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(self.fc_decode(z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# ----------------------------\n",
    "# Embed extraction\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def _extract_embeddings(vae: ConvVAE, dataloader: DataLoader, device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    vae.eval()\n",
    "    emb, labels = [], []\n",
    "    for x, lbl in dataloader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        if x.dtype != torch.float32:\n",
    "            x = x.to(torch.float32)\n",
    "        x = x.contiguous(memory_format=torch.channels_last)\n",
    "        mu, _ = vae.encode(x)\n",
    "        emb.append(mu.cpu().numpy())\n",
    "        labels.extend(np.asarray(lbl))\n",
    "    return np.concatenate(emb), np.array(labels)\n",
    "\n",
    "# ----------------------------\n",
    "# Spectrogram viz helpers\n",
    "# ----------------------------\n",
    "def _ali_spec(x, fs=10000):\n",
    "    from scipy.signal import spectrogram, get_window\n",
    "    Lframe2 = 1000\n",
    "    po = 80\n",
    "    lov = int(np.ceil((po / 100) * Lframe2))\n",
    "    taper = get_window('hann', Lframe2)\n",
    "    Nfft = 2 ** (int(np.floor(np.log2(Lframe2))) + 2)\n",
    "    f, t, s = spectrogram(x, fs=fs, window=taper, noverlap=lov, nfft=Nfft, mode='complex')\n",
    "    as_ = np.abs(s); as_max = np.max(as_) if np.isfinite(np.max(as_)) and np.max(as_) > 0 else 1.0\n",
    "    sdb = 10 * np.log10(100 * as_ / as_max + 1e-10)\n",
    "    min_inx = np.argmin(np.abs(f - 0))\n",
    "    max_inx = np.argmin(np.abs(f - 800))\n",
    "    return sdb[min_inx:max_inx+1, :], f[min_inx:max_inx+1], t\n",
    "\n",
    "def _generate_spectrogram_base64(audio, fs=10000, title=\"Spectrogram\"):\n",
    "    if not _HAVE_VIZ:\n",
    "        return \"<p>Matplotlib not available.</p>\"\n",
    "    spec, f_axis, t_axis = _ali_spec(audio, fs)\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.imshow(spec, aspect='auto', origin='lower',\n",
    "              extent=[t_axis[0], t_axis[-1], f_axis[0], f_axis[-1]], cmap='hsv')\n",
    "    ax.set_title(title); ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    fig.tight_layout()\n",
    "    buf = BytesIO(); fig.savefig(buf, format='png', dpi=150); plt.close(fig); buf.seek(0)\n",
    "    image_base64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return f'<img class=\"d-block w-100\" src=\"data:image/png;base64,{image_base64}\" alt=\"{title}\">'\n",
    "\n",
    "def _make_carousel(cluster_id: int, class_id: int, encoded_imgs: List[str]) -> str:\n",
    "    cid = f\"carousel_class{class_id}_cluster{cluster_id}\"\n",
    "    inds = \"\".join([\n",
    "        f'<button type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide-to=\"{i}\" '\n",
    "        f'class=\"{ \"active\" if i==0 else \"\" }\" aria-current=\"{ \"true\" if i==0 else \"false\" }\" '\n",
    "        f'aria-label=\"Slide {i+1}\"></button>'\n",
    "        for i in range(len(encoded_imgs))\n",
    "    ])\n",
    "    slides = \"\".join([\n",
    "        f'''<div class=\"carousel-item {'active' if i==0 else ''}\">\n",
    "        <div class=\"d-flex justify-content-center\">{img}</div></div>'''\n",
    "        for i, img in enumerate(encoded_imgs)\n",
    "    ])\n",
    "    return f\"\"\"\n",
    "    <div id=\"{cid}\" class=\"carousel slide\" data-bs-interval=\"false\" data-bs-touch=\"false\">\n",
    "      <div class=\"carousel-indicators\">{inds}</div>\n",
    "      <div class=\"carousel-inner\">{slides}</div>\n",
    "      <button class=\"carousel-control-prev\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"prev\">\n",
    "        <span class=\"carousel-control-prev-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Previous</span>\n",
    "      </button>\n",
    "      <button class=\"carousel-control-next\" type=\"button\" data-bs-target=\"#{cid}\" data-bs-slide=\"next\">\n",
    "        <span class=\"carousel-control-next-icon\" aria-hidden=\"true\"></span>\n",
    "        <span class=\"visually-hidden\">Next</span>\n",
    "      </button>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Class-agnostic, K-controlled report\n",
    "# ----------------------------\n",
    "def generate_report(\n",
    "    model_path: str,\n",
    "    dataset_paths: List[str],\n",
    "    labels: List[str],\n",
    "    *,\n",
    "    n_clusters: int = 20,\n",
    "    cluster_method: str = \"gmm\",\n",
    "    output_html: str = \"vae_unsup_report.html\",\n",
    "    latent_dim: int = 64,\n",
    "    seed: int = 42,\n",
    "    examples_per_cluster: int = 6,\n",
    "    subset_fraction: float = 0.2,        # â† NEW: run on 20% by default\n",
    "    subset_strategy: str = \"random\",     # â† 'random' | 'head' | 'tail'\n",
    ") -> str:\n",
    "    assert len(dataset_paths) == len(labels), \"`dataset_paths` and `labels` must match\"\n",
    "    _set_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vae = ConvVAE(latent_dim=latent_dim).to(device).to(memory_format=torch.channels_last).eval()\n",
    "    vae.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    target_T = vae.input_shape[2]\n",
    "\n",
    "    all_Z, all_loc, all_raw = [], [], []\n",
    "\n",
    "    for ds_path, loc_label in zip(dataset_paths, labels):\n",
    "        mel_dir = _mel_pack_dir(ds_path)\n",
    "        if os.path.isdir(mel_dir) and os.path.exists(os.path.join(mel_dir, \"labels.npy\")):\n",
    "            # --- precomputed pack (.pt)\n",
    "            full_ds = PrecomputedMelDataset(mel_dir, cache=False)\n",
    "            idxs = _choose_subset(len(full_ds), fraction=subset_fraction, seed=seed, strategy=subset_strategy)\n",
    "            ds = Subset(full_ds, list(map(int, idxs)))\n",
    "            raw_paths = np.array([None] * len(ds), dtype=object)  # no raw backrefs\n",
    "        else:\n",
    "            # --- raw .npy (wave/spectrogram)\n",
    "            full_ds = SpectrogramDataset(ds_path, cache=True)\n",
    "            # Reconstruct raw .npy list to mirror dataset order\n",
    "            raw_files = sorted(\n",
    "                [f for f in os.listdir(ds_path) if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "                key=lambda x: int(os.path.splitext(x)[0])\n",
    "            )\n",
    "            idxs = _choose_subset(len(full_ds), fraction=subset_fraction, seed=seed, strategy=subset_strategy)\n",
    "            ds = Subset(full_ds, list(map(int, idxs)))\n",
    "            raw_paths = np.array([os.path.join(ds_path, raw_files[i]) for i in idxs], dtype=object)\n",
    "\n",
    "        loader = DataLoader(\n",
    "            ds, batch_size=256, shuffle=False,\n",
    "            num_workers=max(2, (os.cpu_count() or 4)//2), pin_memory=True,\n",
    "            persistent_workers=True, prefetch_factor=4,\n",
    "            collate_fn=lambda b: _collate_fixed_T(b, target_T)\n",
    "        )\n",
    "        Z, _ = _extract_embeddings(vae, loader, device)\n",
    "        all_Z.append(Z)\n",
    "        all_loc.extend([loc_label] * Z.shape[0])\n",
    "        all_raw.append(raw_paths)\n",
    "\n",
    "    # --- (the rest of your function stays the same) ---\n",
    "    embeddings = np.vstack(all_Z).astype(np.float32)\n",
    "    location_labels = np.array(all_loc, dtype=object)\n",
    "    original_paths = np.concatenate(all_raw, axis=0)\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    Zs = StandardScaler().fit_transform(embeddings)\n",
    "\n",
    "    cm = cluster_method.lower()\n",
    "    if cm == \"kmeans\":\n",
    "        from sklearn.cluster import KMeans\n",
    "        clusterer = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed).fit(Zs)\n",
    "        cluster_labels = clusterer.labels_\n",
    "    elif cm == \"agglomerative\":\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n_clusters).fit(Zs)\n",
    "        cluster_labels = clusterer.labels_\n",
    "    elif cm == \"gmm\":\n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        clusterer = GaussianMixture(n_components=n_clusters, covariance_type=\"full\", random_state=seed).fit(Zs)\n",
    "        cluster_labels = clusterer.predict(Zs)\n",
    "    else:\n",
    "        raise ValueError(\"cluster_method must be one of: 'gmm', 'kmeans', 'agglomerative'\")\n",
    "\n",
    "    from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "    def _safe_metric(fn, X, y):\n",
    "        try: return float(fn(X, y))\n",
    "        except Exception: return float(\"nan\")\n",
    "    uniq = np.unique(cluster_labels)\n",
    "    valid = (len(uniq) > 1) and (len(cluster_labels) > len(uniq))\n",
    "    sil = _safe_metric(silhouette_score, Zs, cluster_labels) if valid else float(\"nan\")\n",
    "    dbi = _safe_metric(davies_bouldin_score, Zs, cluster_labels) if valid else float(\"nan\")\n",
    "    ch  = _safe_metric(calinski_harabasz_score, Zs, cluster_labels) if valid else float(\"nan\")\n",
    "\n",
    "    if _HAVE_VIZ:\n",
    "        try:\n",
    "            reducer = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=seed)\n",
    "            proj_3d = reducer.fit_transform(Zs)\n",
    "            title = (f\"Unsupervised {cm.upper()} (k={n_clusters}, subset={int(subset_fraction*100)}%) â€¢ \"\n",
    "                     f\"Sil={sil:.3f} | DBI={dbi:.3f} | CH={ch:.1f}\")\n",
    "            fig = px.scatter_3d(\n",
    "                x=proj_3d[:, 0], y=proj_3d[:, 1], z=proj_3d[:, 2],\n",
    "                color=[str(int(c)) for c in cluster_labels],\n",
    "                symbol=location_labels,\n",
    "                hover_data={\"Cluster\": cluster_labels},\n",
    "                title=title, opacity=0.85, height=800\n",
    "            )\n",
    "            umap_html = pio.to_html(fig, include_plotlyjs=\"cdn\", full_html=False)\n",
    "        except Exception as e:\n",
    "            umap_html = f\"<p class='text-danger'>UMAP/Plotly failed: {e}</p>\"\n",
    "    else:\n",
    "        umap_html = \"<p>UMAP/Plotly not available.</p>\"\n",
    "\n",
    "        # --- Per-cluster sections with exemplar carousels\n",
    "    cluster_blocks = []\n",
    "    for c in sorted(set(cluster_labels)):\n",
    "        idxs = np.where(cluster_labels == c)[0]\n",
    "        if idxs.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Location distribution\n",
    "        from collections import Counter\n",
    "        loc_counts = Counter(location_labels[idxs])\n",
    "        meta_html = \"<p><strong>Location Distribution:</strong></p><ul>\" + \"\".join(\n",
    "            f\"<li><b>{loc}</b>: {cnt} ({cnt/len(idxs):.1%})</li>\" for loc, cnt in loc_counts.items()\n",
    "        ) + \"</ul>\"\n",
    "\n",
    "        # Nearest-to-center exemplars (in standardized latent space)\n",
    "        center = np.mean(Zs[idxs], axis=0, keepdims=True)\n",
    "        dists = np.linalg.norm(Zs[idxs] - center, axis=1)\n",
    "        order = np.argsort(dists)\n",
    "        show = idxs[order[:min(examples_per_cluster, len(order))]]\n",
    "\n",
    "        imgs = []\n",
    "        for i, j in enumerate(show):\n",
    "            p = original_paths[j]\n",
    "            try:\n",
    "                if p is None:\n",
    "                    imgs.append(\"<p>Raw audio not available (precomputed pack).</p>\")\n",
    "                else:\n",
    "                    arr = np.load(p, mmap_mode=\"r\")\n",
    "                    if arr.ndim == 1:\n",
    "                        # waveform â†’ make spectrogram preview\n",
    "                        imgs.append(_generate_spectrogram_base64(\n",
    "                            arr.astype(np.float32),\n",
    "                            title=f\"#{i+1} â€¢ loc={location_labels[j]} â€¢ C={int(c)}\"\n",
    "                        ))\n",
    "                    elif arr.ndim == 2:\n",
    "                        # already a (F,T) spectrogram array\n",
    "                        if not _HAVE_VIZ:\n",
    "                            imgs.append(\"<p>Image backend unavailable.</p>\")\n",
    "                        else:\n",
    "                            import matplotlib.pyplot as plt\n",
    "                            from io import BytesIO\n",
    "                            import base64\n",
    "                            fig, ax = plt.subplots(figsize=(8, 3))\n",
    "                            ax.imshow(arr, aspect='auto', origin='lower', cmap='viridis')\n",
    "                            ax.set_title(f\"#{i+1} â€¢ loc={location_labels[j]} â€¢ C={int(c)}\")\n",
    "                            ax.set_xlabel(\"Frames\"); ax.set_ylabel(\"Mel bins\")\n",
    "                            fig.tight_layout()\n",
    "                            buf = BytesIO(); fig.savefig(buf, format='png', dpi=150); plt.close(fig); buf.seek(0)\n",
    "                            b64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "                            imgs.append(f'<img class=\"d-block w-100\" src=\"data:image/png;base64,{b64}\" alt=\"Spec\">')\n",
    "                    else:\n",
    "                        imgs.append(f\"<p>Unsupported array shape {arr.shape}</p>\")\n",
    "            except Exception as e:\n",
    "                imgs.append(f\"<p class='text-danger'>Error loading sample: {e}</p>\")\n",
    "\n",
    "        carousel_html = _make_carousel(cluster_id=int(c), class_id=-1, encoded_imgs=imgs)\n",
    "        block = f\"<div class='col-md-6 mb-4'><h4>Cluster {c} â€¢ size={len(idxs)}</h4>{meta_html}{carousel_html}</div>\"\n",
    "        cluster_blocks.append(block)\n",
    "\n",
    "    # stitch cluster blocks into rows\n",
    "    cluster_html = \"\"\n",
    "    for i in range(0, len(cluster_blocks), 2):\n",
    "        cluster_html += \"<div class='row'>\" + \"\".join(cluster_blocks[i:i+2]) + \"</div>\"\n",
    "\n",
    "    # --- Final HTML\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Unsupervised Clustering Report (VAE Î¼)</title>\n",
    "        <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; padding: 20px; background-color: #f5f5f5; }}\n",
    "            h1 {{ color: #2c3e50; }}\n",
    "            h2, h4 {{ color: #34495e; }}\n",
    "            .section {{ margin-bottom: 60px; padding: 20px; background: white;\n",
    "                        border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "      <h1 class='mb-4'>Unsupervised Latent Clustering (VAE Î¼)</h1>\n",
    "      <p><b>Method:</b> {cm.upper()} â€¢ <b>k</b>={n_clusters} â€¢ <b>subset</b>={int(subset_fraction*100)}%</p>\n",
    "      <p><b>Silhouette</b>={sil:.3f} â€¢ <b>DBI</b>={dbi:.3f} â€¢ <b>CH</b>={ch:.1f}</p>\n",
    "      <hr>\n",
    "      <div class='section'>\n",
    "        <div class=\"container\">\n",
    "          <div class=\"row justify-content-center mb-4\">\n",
    "            <div class=\"col-md-12 d-flex justify-content-center\">{umap_html}</div>\n",
    "          </div>\n",
    "        </div>\n",
    "        {cluster_html}\n",
    "      </div>\n",
    "    </body></html>\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Save and return\n",
    "    with open(output_html, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    return output_html\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage\n",
    "# ----------------------------\n",
    "# report = generate_report(\n",
    "#     model_path=\"vae_best.pth\",\n",
    "#     dataset_paths=[\"/notebooks/dataset_preprocessed\"],\n",
    "#     labels=[\"PR_U1137\"],\n",
    "#     n_clusters=60,\n",
    "#     cluster_method=\"gmm\",\n",
    "#     subset_fraction=0.20,      # â† 20% of items\n",
    "#     subset_strategy=\"random\",  # or 'head' / 'tail'\n",
    "#     output_html=\"vae_unsup_k60_subset20.html\",\n",
    "#     latent_dim=64,\n",
    "#     examples_per_cluster=5,\n",
    "#     seed=45\n",
    "# )\n",
    "# print(\"Saved:\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dcd6f2-55f1-43c5-826b-67ad3c61ab30",
   "metadata": {},
   "source": [
    "Report and training runner\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6841a49a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T22:31:53.711968Z",
     "iopub.status.busy": "2025-09-23T22:31:53.710403Z",
     "iopub.status.idle": "2025-09-23T22:38:27.726487Z",
     "shell.execute_reply": "2025-09-23T22:38:27.725958Z",
     "shell.execute_reply.started": "2025-09-23T22:31:53.711850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Inference source dir: /notebooks/dataset_preprocessed__mels64_fft1024_h512\n",
      "[INFO] Full label dist: {1: 69829, 2: 70932, 3: 70044, 4: 70779, 5: 70983, 6: 60705}\n",
      "[INFO] TEST label dist: {1: 13966, 2: 14186, 3: 14009, 4: 14156, 5: 14197, 6: 12141}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved UMAP visualization to 'vae_test_umap.html'\n",
      "{'ari': 0.021293397892635138, 'ami': 0.10507615485475799, 'hungarian_accuracy': 0.07834976710422842, 'silhouette': 0.07351772487163544, 'davies_bouldin': 3.337626608126009, 'calinski_harabasz': 914.3685113584048}\n"
     ]
    }
   ],
   "source": [
    "# 1) Make the fixed split once (outside training)\n",
    "train_idx, test_idx = make_fixed_train_test_indices(\"/notebooks/dataset_preprocessed\", test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) Train ONLY on the 80% train indices (a small val is carved from that 80%)\n",
    "# train_res = run_training(\n",
    "#     audio_dir=\"/notebooks/dataset_preprocessed\",\n",
    "#     train_indices=train_idx,\n",
    "#     epochs=50,\n",
    "#     save_path=\"vae_best.pth\",\n",
    "#     checkpoint_path=\"vae_train.ckpt\",   # continue overwriting\n",
    "#     #resume_from=\"vae_train.ckpt\"        # <- resume here\n",
    "# )\n",
    "\n",
    "# 3) Evaluate ONLY on the same held-out 20% with configurable K\n",
    "test_res = run_inference(\n",
    "    audio_dir=\"/notebooks/dataset_preprocessed\",\n",
    "    test_indices=test_idx,\n",
    "    checkpoint_path=\"vae_best.pth\",\n",
    "    kmeans_k=60,\n",
    "    do_umap=True,\n",
    "    umap_components=3,\n",
    "    umap_html_path=\"vae_test_umap.html\",\n",
    "    l2_normalize=True,\n",
    "    pca_dim=64,   # try None vs 64 and compare\n",
    ")\n",
    "print(test_res[\"metrics\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
