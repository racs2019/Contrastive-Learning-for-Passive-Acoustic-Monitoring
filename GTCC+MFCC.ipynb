{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434fa87e-716a-410d-a56d-b71b906558d8",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56e69fc-a7f5-47e7-a572-8cdf31a186c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T14:00:57.341239Z",
     "iopub.status.busy": "2025-10-08T14:00:57.340549Z",
     "iopub.status.idle": "2025-10-08T14:01:12.003374Z",
     "shell.execute_reply": "2025-10-08T14:01:12.002623Z",
     "shell.execute_reply.started": "2025-10-08T14:00:57.341215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gammatone\n",
      "  Downloading Gammatone-1.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from gammatone) (1.26.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gammatone) (1.11.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from gammatone) (3.7.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->gammatone) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gammatone) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->gammatone) (1.16.0)\n",
      "Downloading Gammatone-1.0.3-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: gammatone\n",
      "Successfully installed gammatone-1.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.62.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.3)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.45.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2020.6.20)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading msgpack-1.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.62.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.6/242.6 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.45.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soxr, msgpack, llvmlite, audioread, soundfile, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.11.0 llvmlite-0.45.1 msgpack-1.1.2 numba-0.62.1 pooch-1.8.2 soundfile-0.13.1 soxr-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting plotly\n",
      "  Downloading plotly-6.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (23.2)\n",
      "Downloading plotly-6.3.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.7.0-py3-none-any.whl (412 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m412.8/412.8 kB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-2.7.0 plotly-6.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gammatone\n",
    "!pip install librosa\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd45b98-3973-4c35-ae8c-2fd615c5f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import scipy.stats\n",
    "from scipy.fftpack import dct\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "# CHANGED: correct UMAP import\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from gammatone.filters import centre_freqs, make_erb_filters, erb_filterbank\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "# NEW: internal clustering metrics\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1a8a6-9a71-4f11-8d68-2b7601ce81a8",
   "metadata": {},
   "source": [
    "Runner and report for PAM data\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e421fabf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T15:31:34.375711Z",
     "iopub.status.busy": "2025-08-27T15:31:34.375417Z",
     "iopub.status.idle": "2025-08-27T15:31:44.210244Z",
     "shell.execute_reply": "2025-08-27T15:31:44.209624Z",
     "shell.execute_reply.started": "2025-08-27T15:31:34.375683Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 15:31:41.067668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-27 15:31:41.067735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-27 15:31:41.151712: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-27 15:31:41.325625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-27 15:31:42.820421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "def gtcc_frames(frames, sr, n_bands=64, low_hz=50, high_hz=None, n_ceps=80):\n",
    "    if high_hz is None:\n",
    "        high_hz = sr / 2 - 1\n",
    "    cf = centre_freqs(sr, n_bands, low_hz, high_hz)\n",
    "    fb = make_erb_filters(sr, cf)\n",
    "    energies = []\n",
    "    for f in frames:\n",
    "        y = erb_filterbank(f, fb)\n",
    "        e = np.sum(y**2, axis=1) + 1e-10\n",
    "        energies.append(e)\n",
    "    E = np.vstack(energies)\n",
    "    logE = np.log(E)\n",
    "    C = dct(logE, type=2, axis=1, norm='ortho')[:, :n_ceps]\n",
    "    return C\n",
    "\n",
    "# ----------------------------\n",
    "# Feature & utility functions\n",
    "# ----------------------------\n",
    "def bandpass_filter(signal, sr=16000, lowcut=0, highcut=2000, order=4):\n",
    "    nyquist = 0.5 * sr\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, signal)\n",
    "\n",
    "def compute_deltas(feat_matrix, N=2):\n",
    "    num_frames = feat_matrix.shape[0]\n",
    "    padded = np.pad(feat_matrix, ((N, N), (0, 0)), mode='edge')\n",
    "    deltas = np.zeros_like(feat_matrix)\n",
    "    denom = 2 * sum(i ** 2 for i in range(1, N + 1))\n",
    "    for t in range(num_frames):\n",
    "        for n in range(1, N + 1):\n",
    "            deltas[t] += n * (padded[t + N + n] - padded[t + N - n])\n",
    "        deltas[t] /= denom\n",
    "    return deltas\n",
    "\n",
    "def extract_gtcc_mfcc(audio, sr=16000, n_bands=64, n_coeffs=80):\n",
    "    win_size = int(sr*0.025); hop = int(sr*0.010)\n",
    "    if len(audio) < win_size:\n",
    "        gtcc_mean = np.zeros(n_coeffs, dtype=np.float32)\n",
    "    else:\n",
    "        frames = librosa.util.frame(audio.astype(np.float32), frame_length=win_size, hop_length=hop).T\n",
    "        window = np.hanning(win_size).astype(np.float32)\n",
    "        frames = frames * window\n",
    "        gtcc = gtcc_frames(frames, sr, n_bands=n_bands, n_ceps=n_coeffs)\n",
    "        gtcc_mean = gtcc.mean(axis=0)\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024, hop_length=512, n_mels=64)\n",
    "    logmel = librosa.power_to_db(mel)\n",
    "    mfcc = librosa.feature.mfcc(S=logmel, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "\n",
    "    return np.concatenate([gtcc_mean, mfcc])\n",
    "\n",
    "def _extract_one_feature(index_and_path):\n",
    "    idx, path = index_and_path\n",
    "    audio = np.load(path).astype(np.float32)\n",
    "    maxabs = np.max(np.abs(audio))\n",
    "    if maxabs > 0:\n",
    "        audio = audio / maxabs\n",
    "    feats = extract_gtcc_mfcc(audio)\n",
    "    return idx, feats\n",
    "\n",
    "def load_gtcc_mfcc_features(audio_dir, label_path):\n",
    "    audio_paths = sorted(\n",
    "        [os.path.join(audio_dir, f) for f in os.listdir(audio_dir)\n",
    "         if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "        key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "    )\n",
    "    labels = np.load(label_path)\n",
    "\n",
    "    X = [None] * len(audio_paths)\n",
    "    y = []\n",
    "\n",
    "    with ProcessPoolExecutor() as ex:\n",
    "        futures = {\n",
    "            ex.submit(_extract_one_feature, (i, p)): i\n",
    "            for i, p in enumerate(audio_paths)\n",
    "        }\n",
    "        for fut in as_completed(futures):\n",
    "            i = futures[fut]\n",
    "            try:\n",
    "                idx, feats = fut.result()\n",
    "                X[idx] = feats\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping sample {i} ({audio_paths[i]}) due to error: {e}\")\n",
    "                X[i] = None\n",
    "\n",
    "    X_out, y_out = [], []\n",
    "    for i, feats in enumerate(X):\n",
    "        if feats is not None:\n",
    "            X_out.append(feats)\n",
    "            y_out.append(labels[i])\n",
    "\n",
    "    return np.asarray(X_out), np.asarray(y_out)\n",
    "\n",
    "def hungarian_accuracy(y_true, y_pred):\n",
    "    D = int(max(y_pred.max(), y_true.max())) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    np.add.at(w, (y_pred, y_true), 1)\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return float(sum(w[i, j] for i, j in zip(row_ind, col_ind)) / len(y_pred))\n",
    "\n",
    "# NEW: helper to compute internal indices robustly\n",
    "def _internal_indices(X: np.ndarray, labels: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute Silhouette, DB, CH with guards; returns NaN if invalid.\"\"\"\n",
    "    out = {\"silhouette\": float(\"nan\"), \"davies_bouldin\": float(\"nan\"), \"calinski_harabasz\": float(\"nan\")}\n",
    "    unique = np.unique(labels)\n",
    "    if len(unique) <= 1 or len(unique) >= len(labels):\n",
    "        return out\n",
    "    try:\n",
    "        out[\"silhouette\"] = float(silhouette_score(X, labels))\n",
    "    except Exception as _:\n",
    "        pass\n",
    "    try:\n",
    "        out[\"davies_bouldin\"] = float(davies_bouldin_score(X, labels))\n",
    "    except Exception as _:\n",
    "        pass\n",
    "    try:\n",
    "        out[\"calinski_harabasz\"] = float(calinski_harabasz_score(X, labels))\n",
    "    except Exception as _:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "def run_kmeans_clustering(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    umap_html_path: str = \"kmeans_testset_umap.html\",\n",
    "    *,\n",
    "    # NEW: choose number of clusters; if None, infer from labels; set to 60 for fixed-k option\n",
    "    kmeans_k: Optional[int] = None,\n",
    "    # NEW: toggle computing internal indices on the test set\n",
    "    compute_internal_indices: bool = True,\n",
    ") -> Dict[str, float]:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    n_clusters = int(kmeans_k) if kmeans_k is not None else len(np.unique(y_train))\n",
    "    model = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    model.fit(X_train_scaled)\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    ari = adjusted_rand_score(y_test, y_pred)\n",
    "    ami = adjusted_mutual_info_score(y_test, y_pred)\n",
    "    hacc = hungarian_accuracy(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nğŸ“Š GTCC + MFCC Clustering Evaluation on Test Set (k={n_clusters})\")\n",
    "    print(f\"ARI: {ari:.4f}\")\n",
    "    print(f\"AMI: {ami:.4f}\")\n",
    "    print(f\"Hungarian Accuracy: {hacc:.4f}\")\n",
    "\n",
    "    # NEW: internal indices on X_test\n",
    "    extra = {}\n",
    "    if compute_internal_indices:\n",
    "        extra = _internal_indices(X_test_scaled, y_pred)\n",
    "        print(f\"Silhouette: {extra['silhouette']:.4f} | \"\n",
    "              f\"Daviesâ€“Bouldin: {extra['davies_bouldin']:.4f} | \"\n",
    "              f\"Calinskiâ€“Harabasz: {extra['calinski_harabasz']:.2f}\")\n",
    "\n",
    "    reducer = umap.UMAP(n_components=3, random_state=42)\n",
    "    X_umap = reducer.fit_transform(X_test_scaled.astype(np.float32, copy=False))\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        x=X_umap[:, 0], y=X_umap[:, 1], z=X_umap[:, 2],\n",
    "        color=y_test.astype(str),\n",
    "        title=f\"KMeans (k={n_clusters}) | ARI {ari:.4f}, AMI {ami:.4f}, H-Acc {hacc:.4f}\",\n",
    "        labels={\"x\": \"UMAP-1\", \"y\": \"UMAP-2\", \"z\": \"UMAP-3\"},\n",
    "        opacity=0.7\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.update_layout(legend_title_text='True Label', width=800, height=700)\n",
    "    fig.write_html(umap_html_path)\n",
    "    print(f\"Saved UMAP visualization to '{umap_html_path}'\")\n",
    "\n",
    "    out = {\"ari\": float(ari), \"ami\": float(ami), \"hungarian_accuracy\": float(hacc)}\n",
    "    out.update(extra)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "#  Runner\n",
    "# ----------------------------\n",
    "def run_pipeline(\n",
    "    dataset_path: str,\n",
    "    do_clustering: int = 1,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify: bool = True,\n",
    "    umap_html_path: str = \"kmeans_testset_umap.html\",\n",
    "    # NEW: pass-through options\n",
    "    kmeans_k: Optional[int] = None,\n",
    "    compute_internal_indices: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load features from `dataset_path`, split train/test, and optionally run KMeans clustering.\n",
    "    Expects a 'labels.npy' file inside `dataset_path` and per-utterance audio arrays as .npy files.\n",
    "\n",
    "    NEW:\n",
    "      - kmeans_k: set to 60 to force KMeans with 60 clusters; None infers from labels\n",
    "      - compute_internal_indices: compute Silhouette, DB, CH on test set predictions\n",
    "    \"\"\"\n",
    "    audio_dir = dataset_path\n",
    "    label_path = os.path.join(audio_dir, \"labels.npy\")\n",
    "\n",
    "    if not os.path.isdir(audio_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {audio_dir}\")\n",
    "    if not os.path.isfile(label_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    X, y = load_gtcc_mfcc_features(audio_dir, label_path)\n",
    "    if X.size == 0:\n",
    "        raise ValueError(\"No features were loaded. Check your .npy files and indexing.\")\n",
    "\n",
    "    strat_arg = y if stratify else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat_arg\n",
    "    )\n",
    "\n",
    "    results: Dict[str, Any] = {\n",
    "        \"n_samples\": int(len(X)),\n",
    "        \"n_features\": int(X.shape[1]),\n",
    "        \"train_samples\": int(len(X_train)),\n",
    "        \"test_samples\": int(len(X_test)),\n",
    "        \"did_clustering\": bool(do_clustering),\n",
    "    }\n",
    "\n",
    "    if do_clustering:\n",
    "        metrics = run_kmeans_clustering(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            umap_html_path=umap_html_path,\n",
    "            kmeans_k=kmeans_k,  # NEW\n",
    "            compute_internal_indices=compute_internal_indices,  # NEW\n",
    "        )\n",
    "        results[\"metrics\"] = metrics\n",
    "        results[\"umap_html_path\"] = umap_html_path\n",
    "\n",
    "    results[\"splits\"] = {\n",
    "        \"X_train\": X_train, \"y_train\": y_train,\n",
    "        \"X_test\": X_test, \"y_test\": y_test\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def run_pipeline_test_only(\n",
    "    dataset_path: str,\n",
    "    *,\n",
    "    test_size: float = 0.02,\n",
    "    random_state: int = 42,\n",
    "    stratify: bool = True,\n",
    "    umap_html_path: str = \"kmeans_testset_umap.html\",\n",
    "    # NEW: separate option to force k=60\n",
    "    kmeans_60: bool = False,\n",
    "    # NEW: or specify any k directly; kmeans_60 takes precedence if True\n",
    "    kmeans_k: Optional[int] = None,\n",
    "    compute_internal_indices: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Only process the test split (â‰ˆtest_size of the dataset).\n",
    "    Runs clustering directly on test set only.\n",
    "\n",
    "    NEW:\n",
    "      - kmeans_60=True forces KMeans with 60 clusters (separate option you asked for)\n",
    "      - or set kmeans_k to any int; if both given, kmeans_60 wins\n",
    "      - compute_internal_indices computes Silhouette, DB, CH\n",
    "    \"\"\"\n",
    "    audio_dir = dataset_path\n",
    "    label_path = os.path.join(audio_dir, \"labels.npy\")\n",
    "\n",
    "    if not os.path.isdir(audio_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {audio_dir}\")\n",
    "    if not os.path.isfile(label_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    audio_paths = sorted(\n",
    "        [os.path.join(audio_dir, f) for f in os.listdir(audio_dir)\n",
    "         if f.endswith(\".npy\") and f != \"labels.npy\"],\n",
    "        key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    "    )\n",
    "    labels = np.load(label_path)\n",
    "\n",
    "    strat_arg = labels if stratify else None\n",
    "    _, paths_test, _, y_test = train_test_split(\n",
    "        audio_paths, labels, test_size=test_size,\n",
    "        random_state=random_state, stratify=strat_arg\n",
    "    )\n",
    "\n",
    "    X_test = []\n",
    "    for p in paths_test:\n",
    "        audio = np.load(p).astype(np.float32)\n",
    "        maxabs = np.max(np.abs(audio))\n",
    "        if maxabs > 0:\n",
    "            audio = audio / maxabs\n",
    "        feats = extract_gtcc_mfcc(audio)\n",
    "        X_test.append(feats)\n",
    "\n",
    "    X_test = np.asarray(X_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    # NEW: choose k\n",
    "    if kmeans_60:\n",
    "        n_clusters = 60\n",
    "    elif kmeans_k is not None:\n",
    "        n_clusters = int(kmeans_k)\n",
    "    else:\n",
    "        n_clusters = len(np.unique(y_test))\n",
    "\n",
    "    model = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    y_pred = model.fit_predict(X_test_scaled)\n",
    "\n",
    "    ari = adjusted_rand_score(y_test, y_pred)\n",
    "    ami = adjusted_mutual_info_score(y_test, y_pred)\n",
    "    hacc = hungarian_accuracy(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nğŸ“Š Test-Only KMeans Evaluation (k={n_clusters})\")\n",
    "    print(f\"ARI: {ari:.4f}\")\n",
    "    print(f\"AMI: {ami:.4f}\")\n",
    "    print(f\"Hungarian Accuracy: {hacc:.4f}\")\n",
    "\n",
    "    # NEW: internal indices\n",
    "    extra = {}\n",
    "    if compute_internal_indices:\n",
    "        extra = _internal_indices(X_test_scaled, y_pred)\n",
    "        print(f\"Silhouette: {extra['silhouette']:.4f} | \"\n",
    "              f\"Daviesâ€“Bouldin: {extra['davies_bouldin']:.4f} | \"\n",
    "              f\"Calinskiâ€“Harabasz: {extra['calinski_harabasz']:.2f}\")\n",
    "\n",
    "    reducer = umap.UMAP(n_components=3, random_state=42)\n",
    "    X_umap = reducer.fit_transform(X_test_scaled.astype(np.float32, copy=False))\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        x=X_umap[:, 0], y=X_umap[:, 1], z=X_umap[:, 2],\n",
    "        color=y_test.astype(str),\n",
    "        title=f\"Test-Only KMeans (k={n_clusters}) | ARI {ari:.4f}, AMI {ami:.4f}, H-Acc {hacc:.4f}\",\n",
    "        labels={\"x\": \"UMAP-1\", \"y\": \"UMAP-2\", \"z\": \"UMAP-3\"},\n",
    "        opacity=0.7\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.update_layout(legend_title_text='True Label', width=800, height=700)\n",
    "    fig.write_html(umap_html_path)\n",
    "    print(f\"Saved UMAP visualization to '{umap_html_path}'\")\n",
    "\n",
    "    metrics = {\n",
    "        \"ari\": float(ari),\n",
    "        \"ami\": float(ami),\n",
    "        \"hungarian_accuracy\": float(hacc),\n",
    "    }\n",
    "    metrics.update(extra)\n",
    "\n",
    "    return {\n",
    "        \"test_samples\": int(len(X_test)),\n",
    "        \"n_features\": int(X_test.shape[1]),\n",
    "        \"metrics\": metrics,\n",
    "        \"umap_html_path\": umap_html_path,\n",
    "        \"splits\": {\n",
    "            \"X_test\": X_test, \"y_test\": y_test\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2415c5ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T15:32:10.341719Z",
     "iopub.status.busy": "2025-08-27T15:32:10.341228Z",
     "iopub.status.idle": "2025-08-27T16:12:44.475168Z",
     "shell.execute_reply": "2025-08-27T16:12:44.474740Z",
     "shell.execute_reply.started": "2025-08-27T15:32:10.341696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Test-Only KMeans Evaluation (k=6)\n",
      "ARI: 0.0912\n",
      "AMI: 0.1602\n",
      "Hungarian Accuracy: 0.3380\n",
      "Silhouette: 0.2317 | Daviesâ€“Bouldin: 1.4524 | Calinskiâ€“Harabasz: 1535.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved UMAP visualization to 'kmeans_testset_umap.html'\n",
      "{'ari': 0.09124351671746268, 'ami': 0.16023243740207158, 'hungarian_accuracy': 0.33801112992983307, 'silhouette': 0.23171464457016527, 'davies_bouldin': 1.45241012006733, 'calinski_harabasz': 1535.8141682878859}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Example usage\n",
    "# ----------------------------\n",
    "res = run_pipeline_test_only(\"/notebooks/dataset_preprocessed\", kmeans_60=False, kmeans_k=6)\n",
    "print(res[\"metrics\"])  # includes silhouette, DB, CH\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00d4cc1-47ab-45fc-a326-cae916dc71a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T14:01:28.401655Z",
     "iopub.status.busy": "2025-10-08T14:01:28.401052Z",
     "iopub.status.idle": "2025-10-08T14:01:35.623346Z",
     "shell.execute_reply": "2025-10-08T14:01:35.622547Z",
     "shell.execute_reply.started": "2025-10-08T14:01:28.401634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundata\n",
      "  Downloading soundata-1.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from soundata) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from soundata) (1.26.3)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.11/dist-packages (from soundata) (2.2.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from soundata) (4.66.1)\n",
      "Collecting jams>=0.3.4 (from soundata)\n",
      "  Downloading jams-0.3.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting py7zr>=0.16.0 (from soundata)\n",
      "  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: jsonschema>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from jams>=0.3.4->soundata) (4.21.1)\n",
      "Collecting mir_eval>=0.8.2 (from jams>=0.3.4->soundata)\n",
      "  Downloading mir_eval-0.8.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sortedcontainers>=2.1.0 (from jams>=0.3.4->soundata)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from jams>=0.3.4->soundata) (1.16.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from jams>=0.3.4->soundata) (5.1.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (0.62.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (1.3.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (4.9.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->soundata) (1.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.5->soundata) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.3.5->soundata) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.5->soundata) (2023.4)\n",
      "Collecting texttable (from py7zr>=0.16.0->soundata)\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pycryptodomex>=3.20.0 (from py7zr>=0.16.0->soundata)\n",
      "  Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting brotli>=1.1.0 (from py7zr>=0.16.0->soundata)\n",
      "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.16.0->soundata) (5.9.8)\n",
      "Collecting pyzstd>=0.16.1 (from py7zr>=0.16.0->soundata)\n",
      "  Downloading pyzstd-0.18.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting pyppmd<1.3.0,>=1.1.0 (from py7zr>=0.16.0->soundata)\n",
      "  Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr>=0.16.0->soundata)\n",
      "  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr>=0.16.0->soundata)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr>=0.16.0->soundata)\n",
      "  Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.0.1->jams>=0.3.4->soundata) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.0.1->jams>=0.3.4->soundata) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.0.1->jams>=0.3.4->soundata) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.0.1->jams>=0.3.4->soundata) (0.17.1)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa>=0.10.0->soundata) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.10.0->soundata) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.10.0->soundata) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.10.0->soundata) (2.31.0)\n",
      "Collecting typing_extensions>=4.1.1 (from librosa>=0.10.0->soundata)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa>=0.10.0->soundata) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa>=0.10.0->soundata) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.10.0->soundata) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.10.0->soundata) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.10.0->soundata) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.10.0->soundata) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.10.0->soundata) (2020.6.20)\n",
      "Downloading soundata-1.0.1-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.0/162.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jams-0.3.5-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mir_eval-0.8.2-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.8/102.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyzstd-0.18.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (428 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: texttable, sortedcontainers, brotli, typing_extensions, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, pyzstd, mir_eval, py7zr, jams, soundata\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "Successfully installed brotli-1.1.0 inflate64-1.0.3 jams-0.3.5 mir_eval-0.8.2 multivolumefile-0.2.3 py7zr-1.0.0 pybcj-1.0.6 pycryptodomex-3.23.0 pyppmd-1.2.0 pyzstd-0.18.0 sortedcontainers-2.4.0 soundata-1.0.1 texttable-1.7.0 typing_extensions-4.15.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install soundata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1194641-b200-402d-ac45-4f08290e026e",
   "metadata": {},
   "source": [
    "Runner and report for urbansound8k dataset\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0421cdd-ed5a-4e2d-898a-da9eeb6c0b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T14:01:37.585485Z",
     "iopub.status.busy": "2025-10-08T14:01:37.585235Z",
     "iopub.status.idle": "2025-10-08T15:09:06.763032Z",
     "shell.execute_reply": "2025-10-08T15:09:06.762324Z",
     "shell.execute_reply.started": "2025-10-08T14:01:37.585465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring unknown args: ['-f', '/root/.local/share/jupyter/runtime/kernel-859d66c2-3c5d-4f58-ba0f-3a3069499661.json']\n",
      "Initializing UrbanSound8K dataset with soundata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Downloading ['all', 'index']. Index is being stored in /usr/local/lib/python3.11/dist-packages/soundata/datasets/indexes, and the rest of files in /root/sound_datasets/urbansound8k\n",
      "INFO: [all] downloading UrbanSound8K.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring dataset is present (this is idempotent)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.61GB [06:15, 16.0MB/s]                                \n",
      "INFO: [index] downloading urbansound8k_index_1.0.json\n",
      "1.15MB [00:06, 197kB/s]                             \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 756.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8732/8732 [00:13<00:00, 636.67it/s]\n",
      "INFO: Success: the dataset is complete and all files are valid.\n",
      "INFO: --------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] Validation issues â€” missing: 2, invalid: 2\n",
      "Using audio root: /root/sound_datasets/urbansound8k/audio\n",
      "[subset] Reducing 8732 â†’ 5000\n",
      "Clips to process: 5000\n",
      "[feat] Extracting features (GTCC+MFCC)...\n",
      "Features: (5000, 77) (dim=77)\n",
      "Saved metrics â†’ ./us8k_out/report_test_only_unsup.txt\n"
     ]
    }
   ],
   "source": [
    "# urbansound8k_metrics_mfcc_gtcc_AUTODOWNLOAD.py\n",
    "# -----------------------------------------------------------\n",
    "# TEST-only UrbanSound8K clustering with auto-download (soundata)\n",
    "# - Downloads & validates UrbanSound8K (idempotent)\n",
    "# - Extracts MFCC + GTCC from WAVs\n",
    "# - KMeans clustering on TEST features only\n",
    "# - UNSUPERVISED METRICS ONLY:\n",
    "#     Silhouette (cosine), Daviesâ€“Bouldin, Calinskiâ€“Harabasz,\n",
    "#     Intra-cluster cosine (mean)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import glob\n",
    "import soundata\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import librosa\n",
    "from gammatone.filters import centre_freqs, make_erb_filters, erb_filterbank\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "\n",
    "# ---------- Feature extraction (GTCC + MFCC) ----------\n",
    "def gtcc_frames(frames, sr, n_bands=64, low_hz=50, high_hz=None, n_ceps=80):\n",
    "    if high_hz is None:\n",
    "        high_hz = sr / 2 - 1\n",
    "    cf = centre_freqs(sr, n_bands, low_hz, high_hz)\n",
    "    fb = make_erb_filters(sr, cf)\n",
    "    energies = []\n",
    "    for f in frames:\n",
    "        y = erb_filterbank(f, fb)\n",
    "        e = np.sum(y ** 2, axis=1) + 1e-10\n",
    "        energies.append(e)\n",
    "    E = np.vstack(energies)\n",
    "    logE = np.log(E)\n",
    "    C = dct(logE, type=2, axis=1, norm=\"ortho\")[:, :n_ceps]\n",
    "    return C\n",
    "\n",
    "def extract_gtcc_mfcc(audio, sr=32000, n_bands=64, n_ceps=80, n_mfcc=13):\n",
    "    \"\"\"Returns a single vector: [gtcc_mean (n_ceps) ; mfcc_mean (n_mfcc)].\"\"\"\n",
    "    audio = np.asarray(audio, dtype=np.float32)\n",
    "    if audio.size:\n",
    "        maxabs = float(np.max(np.abs(audio)))\n",
    "        if maxabs > 0:\n",
    "            audio = audio / maxabs\n",
    "\n",
    "    # 25 ms window / 10 ms hop @ 32 kHz\n",
    "    win_size = int(sr * 0.025)\n",
    "    hop = int(sr * 0.010)\n",
    "\n",
    "    if len(audio) < max(2, win_size):\n",
    "        gtcc_mean = np.zeros(n_ceps, dtype=np.float32)\n",
    "    else:\n",
    "        frames = librosa.util.frame(audio, frame_length=win_size, hop_length=hop).T\n",
    "        frames = frames * np.hanning(win_size).astype(np.float32)\n",
    "        gtcc = gtcc_frames(frames, sr, n_bands=n_bands, n_ceps=n_ceps)\n",
    "        gtcc_mean = gtcc.mean(axis=0).astype(np.float32)\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024, hop_length=512, n_mels=64)\n",
    "    logmel = librosa.power_to_db(mel, ref=np.max)\n",
    "    mfcc = librosa.feature.mfcc(S=logmel, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_mean = (mfcc.mean(axis=1) if mfcc.size else np.zeros(n_mfcc, dtype=np.float32)).astype(np.float32)\n",
    "\n",
    "    return np.concatenate([gtcc_mean, mfcc_mean], dtype=np.float32)\n",
    "\n",
    "def extract_from_wav(path, target_sr=32000):\n",
    "    y, _ = librosa.load(path, sr=target_sr, mono=True)\n",
    "    return extract_gtcc_mfcc(y, sr=target_sr)\n",
    "\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def l2n(A):\n",
    "    return A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def intra_cluster_cosine(X, labels):\n",
    "    \"\"\"Mean pairwise cosine similarity within clusters (excluding diagonal).\"\"\"\n",
    "    Xn = l2n(X)\n",
    "    per_cluster = []\n",
    "    for c in np.unique(labels):\n",
    "        idxs = np.where(labels == c)[0]\n",
    "        if idxs.size < 2:\n",
    "            continue\n",
    "        S = cosine_similarity(Xn[idxs])\n",
    "        m = (S.sum() - len(idxs)) / (len(idxs) * len(idxs) - 1)\n",
    "        per_cluster.append(m)\n",
    "    return (float(np.mean(per_cluster)) if per_cluster else 0.0), np.array(per_cluster)\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main(args):\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Initializing UrbanSound8K dataset with soundata...\")\n",
    "    dataset = soundata.initialize(\"urbansound8k\")\n",
    "\n",
    "    # Download first so the dataset index exists (idempotent if already present)\n",
    "    try:\n",
    "        print(\"Ensuring dataset is present (this is idempotent)...\")\n",
    "        dataset.download()\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] soundata.download() raised: {e}\")\n",
    "\n",
    "    # Validate (optional info only)\n",
    "    try:\n",
    "        missing, invalid = dataset.validate()\n",
    "        if missing or invalid:\n",
    "            print(f\"[warn] Validation issues â€” missing: {len(missing)}, invalid: {len(invalid)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"[warn] Could not load dataset index for validation. Continuing.\")\n",
    "\n",
    "    # Resolve dataset root (handle nested UrbanSound8K/)\n",
    "    base = dataset.data_home\n",
    "    # Prefer <data_home>/UrbanSound8K/audio if it exists; else <data_home>/audio\n",
    "    audio_root = os.path.join(base, \"UrbanSound8K\", \"audio\")\n",
    "    if not os.path.isdir(audio_root):\n",
    "        audio_root = os.path.join(base, \"audio\")\n",
    "    if not os.path.isdir(audio_root):\n",
    "        raise SystemExit(\n",
    "            f\"Could not find audio directory under {base}. \"\n",
    "            \"Check your soundata cache or extractor.\"\n",
    "        )\n",
    "    print(f\"Using audio root: {audio_root}\")\n",
    "\n",
    "    # List WAVs (all folds)\n",
    "    wavs = sorted(glob.glob(os.path.join(audio_root, \"fold*\", \"*.wav\")))\n",
    "    if not wavs:\n",
    "        # fallback: any wavs under audio_root\n",
    "        wavs = sorted(glob.glob(os.path.join(audio_root, \"**\", \"*.wav\"), recursive=True))\n",
    "    if not wavs:\n",
    "        raise SystemExit(\"No WAV files found after download/validate.\")\n",
    "\n",
    "    # Optional random subset for speed\n",
    "    MAX_TEST = 5000 if args.max_test is None else int(args.max_test)\n",
    "    if MAX_TEST > 0 and len(wavs) > MAX_TEST:\n",
    "        print(f\"[subset] Reducing {len(wavs)} â†’ {MAX_TEST}\")\n",
    "        rng = np.random.default_rng(42)\n",
    "        sel = np.sort(rng.choice(len(wavs), size=MAX_TEST, replace=False))\n",
    "        wavs = [wavs[i] for i in sel]\n",
    "\n",
    "    print(f\"Clips to process: {len(wavs)}\")\n",
    "\n",
    "    # Feature extraction\n",
    "    feats = []\n",
    "    print(\"[feat] Extracting features (GTCC+MFCC)...\")\n",
    "    for p in wavs:\n",
    "        try:\n",
    "            v = extract_from_wav(p, target_sr=args.sample_rate)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed on {p}: {e}\")\n",
    "            continue\n",
    "        feats.append(v)\n",
    "\n",
    "    if not feats:\n",
    "        raise SystemExit(\"No features extracted (all files failed?).\")\n",
    "\n",
    "    X = np.asarray(feats, dtype=np.float32)\n",
    "    print(f\"Features: {X.shape} (dim={X.shape[1]})\")\n",
    "\n",
    "    # Standardize + L2 normalize\n",
    "    Xs = StandardScaler().fit_transform(X).astype(np.float32, copy=False)\n",
    "    Xn = l2n(Xs)\n",
    "\n",
    "    # KMeans clustering\n",
    "    km = KMeans(n_clusters=args.k_clusters, n_init=10, random_state=42)\n",
    "    y_cluster = km.fit_predict(Xn)\n",
    "\n",
    "    # Unsupervised metrics (guard for degenerate cases)\n",
    "    def _safe(metric_fn, *a, **kw):\n",
    "        try:\n",
    "            return float(metric_fn(*a, **kw))\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] {metric_fn.__name__} not computed: {e}\")\n",
    "            return float(\"nan\")\n",
    "\n",
    "    sil = _safe(silhouette_score, Xn, y_cluster, metric=\"cosine\")\n",
    "    dbi = _safe(davies_bouldin_score, Xn, y_cluster)\n",
    "    ch  = _safe(calinski_harabasz_score, Xn, y_cluster)\n",
    "    cos_mean, per_cluster = intra_cluster_cosine(Xn, y_cluster)\n",
    "\n",
    "    # Save report\n",
    "    rep = os.path.join(args.out_dir, \"report_test_only_unsup.txt\")\n",
    "    with open(rep, \"w\") as f:\n",
    "        f.write(\"UrbanSound8K metrics (TEST-only; UNSUPERVISED ONLY)\\n\")\n",
    "        f.write(f\"Sample rate: {args.sample_rate}\\n\")\n",
    "        f.write(f\"KMeans k={args.k_clusters}\\n\")\n",
    "        f.write(f\"Silhouette (cosine): {sil:.6f}\\n\" if np.isfinite(sil) else \"Silhouette (cosine): N/A\\n\")\n",
    "        f.write(f\"Daviesâ€“Bouldin:      {dbi:.6f}\\n\" if np.isfinite(dbi) else \"Daviesâ€“Bouldin:      N/A\\n\")\n",
    "        f.write(f\"Calinskiâ€“Harabasz:   {ch:.6f}\\n\" if np.isfinite(ch) else \"Calinskiâ€“Harabasz:   N/A\\n\")\n",
    "        f.write(f\"Intra-cluster cosine mean: {cos_mean:.6f}\\n\")\n",
    "        f.write(f\"Clusters >=2 items: {len(per_cluster)}\\n\")\n",
    "    print(\"Saved metrics â†’\", rep)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--out_dir\", type=str, default=\"./us8k_out\", help=\"Output directory.\")\n",
    "    ap.add_argument(\"--sample_rate\", type=int, default=32000)\n",
    "    ap.add_argument(\"--k_clusters\", type=int, default=10)\n",
    "    ap.add_argument(\"--max_test\", type=int, default=5000, help=\"Cap #clips; 0=use all.\")\n",
    "    args, unknown = ap.parse_known_args()\n",
    "    if unknown:\n",
    "        print(\"Ignoring unknown args:\", unknown)\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159eb95-7282-4d58-a1c7-b44e357ade62",
   "metadata": {},
   "source": [
    "Runner and report for birdset dataset\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e103a0-ed28-4df2-8149-d29b2061c7eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T15:02:40.917617Z",
     "iopub.status.busy": "2025-10-07T15:02:40.917276Z",
     "iopub.status.idle": "2025-10-07T16:26:21.423160Z",
     "shell.execute_reply": "2025-10-07T16:26:21.422401Z",
     "shell.execute_reply.started": "2025-10-07T15:02:40.917589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[subset] Reducing test set: 24480 â†’ 5000\n",
      "Reusing prepared TEST audio: 5000 npy\n",
      "[feat] Extracting TEST features (MFCC+GTCC)...\n",
      "Features (test): (5000, 77)  |  dim=77\n",
      "[cluster] KMeans(k=20) on TEST features...\n",
      "Saved metrics -> ./birdset_bridge_out_32k/report_test_only.txt\n"
     ]
    }
   ],
   "source": [
    "# birdset_metrics_mfcc_gtcc_TESTONLY.py\n",
    "# -----------------------------------------------------------\n",
    "# Use prepared TEST audio (.npy) to:\n",
    "#  - extract MFCC + GTCC features (no SimCLR)\n",
    "#  - KMeans clustering (k=20) on TEST features only\n",
    "#  - cluster metrics: silhouette, DBI, CH, intra-cluster cosine\n",
    "#  - (optional, if test_labels.npy exists):\n",
    "#       Hungarian hit@1 (clusterâ†’class), NMI, ARI\n",
    "#\n",
    "# No train features or labels are touched. No labels regeneration.\n",
    "#\n",
    "# Dependencies: numpy, scipy, scikit-learn, librosa, gammatone\n",
    "# -----------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    "    normalized_mutual_info_score,\n",
    "    adjusted_rand_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import librosa\n",
    "from gammatone.filters import centre_freqs, make_erb_filters, erb_filterbank\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "\n",
    "# ---------- Feature extraction (GTCC + MFCC) ----------\n",
    "def gtcc_frames(frames, sr, n_bands=64, low_hz=50, high_hz=None, n_ceps=80):\n",
    "    if high_hz is None:\n",
    "        high_hz = sr / 2 - 1\n",
    "    cf = centre_freqs(sr, n_bands, low_hz, high_hz)\n",
    "    fb = make_erb_filters(sr, cf)\n",
    "    energies = []\n",
    "    for f in frames:\n",
    "        y = erb_filterbank(f, fb)\n",
    "        e = np.sum(y ** 2, axis=1) + 1e-10\n",
    "        energies.append(e)\n",
    "    E = np.vstack(energies)\n",
    "    logE = np.log(E)\n",
    "    C = dct(logE, type=2, axis=1, norm=\"ortho\")[:, :n_ceps]\n",
    "    return C\n",
    "\n",
    "def extract_gtcc_mfcc(audio, sr=32000, n_bands=64, n_ceps=80, n_mfcc=13):\n",
    "    \"\"\"Returns a single vector: [gtcc_mean (n_ceps) ; mfcc_mean (n_mfcc)].\"\"\"\n",
    "    audio = np.asarray(audio, dtype=np.float32)\n",
    "    maxabs = float(np.max(np.abs(audio))) if audio.size else 0.0\n",
    "    if maxabs > 0:\n",
    "        audio = audio / maxabs\n",
    "\n",
    "    # 25 ms window / 10 ms hop at 32 kHz\n",
    "    win_size = int(sr * 0.025)\n",
    "    hop = int(sr * 0.010)\n",
    "\n",
    "    if len(audio) < max(2, win_size):\n",
    "        gtcc_mean = np.zeros(n_ceps, dtype=np.float32)\n",
    "    else:\n",
    "        frames = librosa.util.frame(audio, frame_length=win_size, hop_length=hop).T\n",
    "        window = np.hanning(win_size).astype(np.float32)\n",
    "        frames = frames * window\n",
    "        gtcc = gtcc_frames(frames, sr, n_bands=n_bands, n_ceps=n_ceps)\n",
    "        gtcc_mean = gtcc.mean(axis=0).astype(np.float32)\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024, hop_length=512, n_mels=64)\n",
    "    logmel = librosa.power_to_db(mel, ref=np.max)\n",
    "    mfcc = librosa.feature.mfcc(S=logmel, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_mean = (mfcc.mean(axis=1) if mfcc.size else np.zeros(n_mfcc, dtype=np.float32)).astype(np.float32)\n",
    "\n",
    "    return np.concatenate([gtcc_mean, mfcc_mean], dtype=np.float32)\n",
    "\n",
    "def _extract_one_feature(path, sr=32000):\n",
    "    audio = np.load(path).astype(np.float32, copy=False)\n",
    "    return extract_gtcc_mfcc(audio, sr=sr)\n",
    "\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def l2n(A):\n",
    "    return A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def intra_cluster_cosine(X, labels):\n",
    "    Xn = l2n(X)\n",
    "    per_cluster = []\n",
    "    for c in np.unique(labels):\n",
    "        idxs = np.where(labels == c)[0]\n",
    "        if idxs.size < 2:\n",
    "            continue\n",
    "        S = cosine_similarity(Xn[idxs])\n",
    "        m = (S.sum() - len(idxs)) / (len(idxs) * len(idxs) - 1)\n",
    "        per_cluster.append(m)\n",
    "    return (float(np.mean(per_cluster)) if per_cluster else 0.0), np.array(per_cluster)\n",
    "\n",
    "def _list_npy_ordered(dirpath):\n",
    "    paths = sorted(\n",
    "        [p for p in glob(os.path.join(dirpath, \"*.npy\")) if os.path.basename(p) != \"labels.npy\"],\n",
    "        key=lambda p: int(os.path.splitext(os.path.basename(p))[0])\n",
    "    )\n",
    "    return paths\n",
    "\n",
    "\n",
    "# ---------- Main (TEST ONLY) ----------\n",
    "def main(args):\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    test_npy = os.path.join(args.out_dir, \"test5s_npy\")\n",
    "\n",
    "    # Reuse prepared TEST .npy (collect all, then subset to cap)\n",
    "    test_paths_all = _list_npy_ordered(test_npy)\n",
    "    N_total = len(test_paths_all)\n",
    "    if N_total == 0:\n",
    "        raise SystemExit(\n",
    "            f\"Missing test npys. Found 0 in {test_npy}. \"\n",
    "            \"Point --out_dir to your previous run with test5s_npy/.\"\n",
    "        )\n",
    "\n",
    "    # ---- Limit test set size (hard cap 5000, random but reproducible)\n",
    "    MAX_TEST = 5000\n",
    "    if N_total > MAX_TEST:\n",
    "        print(f\"[subset] Reducing test set: {N_total} â†’ {MAX_TEST}\")\n",
    "        rng = np.random.default_rng(42)\n",
    "        sel_idx = np.sort(rng.choice(N_total, size=MAX_TEST, replace=False))\n",
    "        test_paths = [test_paths_all[i] for i in sel_idx]\n",
    "    else:\n",
    "        sel_idx = np.arange(N_total, dtype=int)\n",
    "        test_paths = test_paths_all\n",
    "\n",
    "    print(f\"Reusing prepared TEST audio: {len(test_paths)} npy\")\n",
    "\n",
    "    # ---- Feature extraction (MFCC+GTCC) for TEST only\n",
    "    print(\"[feat] Extracting TEST features (MFCC+GTCC)...\")\n",
    "    X_test = np.asarray([_extract_one_feature(p, sr=32000) for p in test_paths], dtype=np.float32)\n",
    "    print(f\"Features (test): {X_test.shape}  |  dim={X_test.shape[1]}\")\n",
    "\n",
    "    # ---- Standardize then L2-normalize (fit on TEST, since no train)\n",
    "    scaler = StandardScaler()\n",
    "    X_test_s = scaler.fit_transform(X_test).astype(np.float32, copy=False)\n",
    "    X_test_n = l2n(X_test_s)\n",
    "\n",
    "    # ---- KMeans clustering (on TEST)\n",
    "    print(f\"[cluster] KMeans(k={args.k_clusters}) on TEST features...\")\n",
    "    km = KMeans(n_clusters=args.k_clusters, n_init=10, random_state=42)\n",
    "    y_cluster = km.fit_predict(X_test_n)\n",
    "\n",
    "    # ---- Cluster metrics\n",
    "    sil_cos = silhouette_score(X_test_n, y_cluster, metric=\"cosine\")\n",
    "    dbi     = davies_bouldin_score(X_test_n, y_cluster)\n",
    "    ch      = calinski_harabasz_score(X_test_n, y_cluster)\n",
    "    mean_cos, per_cluster = intra_cluster_cosine(X_test_n, y_cluster)\n",
    "\n",
    "    # ---- Optional label-based metrics (requires test_labels.npy)\n",
    "    labels_path = os.path.join(args.out_dir, \"test_labels.npy\")\n",
    "    have_labels = os.path.isfile(labels_path)\n",
    "    hungarian_hit1 = nmi = ari = None\n",
    "    valid_map_mask = None\n",
    "    N_labels_used = 0\n",
    "\n",
    "    if have_labels:\n",
    "        Y_test_all = np.load(labels_path)\n",
    "\n",
    "        if Y_test_all.shape[0] == N_total:\n",
    "            # Proper alignment with the selected subset\n",
    "            Y_test = Y_test_all[sel_idx]\n",
    "        else:\n",
    "            # Fallback: warn and truncate to whatever we ended up using\n",
    "            print(\"[warn] test_labels.npy length doesn't match all test npys; \"\n",
    "                  \"falling back to naive truncation to match current subset.\")\n",
    "            Y_test = Y_test_all[:X_test.shape[0]]\n",
    "\n",
    "        if Y_test.shape[0] != X_test.shape[0]:\n",
    "            N = min(Y_test.shape[0], X_test.shape[0])\n",
    "            print(f\"[align] Truncating to {N} to match features/labels.\")\n",
    "            Y_test = Y_test[:N]\n",
    "            X_test_n = X_test_n[:N]\n",
    "            y_cluster = y_cluster[:N]\n",
    "\n",
    "        # Classes present in TEST (ignore all-zero columns)\n",
    "        classes = np.where(Y_test.sum(axis=0) > 0)[0]\n",
    "        K = y_cluster.max() + 1\n",
    "        C = len(classes)\n",
    "\n",
    "        if C > 0 and K > 0:\n",
    "            M = np.zeros((K, C), dtype=np.int64)\n",
    "            for i in range(Y_test.shape[0]):\n",
    "                pos = np.where(Y_test[i, classes] == 1)[0]\n",
    "                if pos.size == 0:\n",
    "                    continue\n",
    "                M[y_cluster[i], pos] += 1\n",
    "\n",
    "            sz = max(K, C)\n",
    "            pad = np.zeros((sz, sz), dtype=np.int64)\n",
    "            pad[:K, :C] = M\n",
    "            cost = pad.max() - pad\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "            cluster_to_class = {r: classes[c] for r, c in zip(row_ind, col_ind) if r < K and c < C}\n",
    "            mapped_pred = np.array([cluster_to_class.get(k, -1) for k in y_cluster])\n",
    "            valid_map_mask = (mapped_pred != -1)\n",
    "            N_labels_used = int(valid_map_mask.sum())\n",
    "\n",
    "            # hit@1 for multilabel: success if any true label == mapped class\n",
    "            hits = (Y_test[np.arange(Y_test.shape[0]), np.clip(mapped_pred, 0, Y_test.shape[1]-1)] == 1)\n",
    "            hungarian_hit1 = float(np.mean(hits[valid_map_mask])) if N_labels_used > 0 else None\n",
    "\n",
    "            # NMI / ARI via single-label reduction (dominant test label)\n",
    "            dom_gt = Y_test[:, classes].argmax(axis=1)\n",
    "            # mask items with at least one positive among 'classes'\n",
    "            valid_single = (Y_test[:, classes].sum(axis=1) > 0)\n",
    "            if valid_single.any():\n",
    "                nmi = normalized_mutual_info_score(dom_gt[valid_single], y_cluster[valid_single])\n",
    "                ari = adjusted_rand_score(dom_gt[valid_single], y_cluster[valid_single])\n",
    "\n",
    "    # ---- Save report\n",
    "    report_path = os.path.join(args.out_dir, \"report_test_only.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"Mode: TEST-ONLY\\n\")\n",
    "        f.write(f\"Test5s dir: {test_npy}\\n\")\n",
    "        f.write(f\"Features (test): {X_test.shape}\\n\")\n",
    "        f.write(f\"Feature dim: {X_test.shape[1]}\\n\")\n",
    "        f.write(f\"KMeans k={args.k_clusters}\\n\")\n",
    "        f.write(f\"Silhouette (cosine): {sil_cos:.6f}\\n\")\n",
    "        f.write(f\"Daviesâ€“Bouldin:      {dbi:.6f}\\n\")\n",
    "        f.write(f\"Calinskiâ€“Harabasz:   {ch:.6f}\\n\")\n",
    "        f.write(f\"Intra-cluster cosine mean: {mean_cos:.6f}\\n\")\n",
    "        f.write(f\"Clusters >=2 items: {len(per_cluster)}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        if have_labels:\n",
    "            if hungarian_hit1 is not None:\n",
    "                f.write(f\"Hungarian hit@1 (clusterâ†’class) [valid mapped items {N_labels_used}]: {hungarian_hit1:.6f}\\n\")\n",
    "            else:\n",
    "                f.write(\"Hungarian hit@1 (clusterâ†’class): N/A\\n\")\n",
    "            if nmi is not None:\n",
    "                f.write(f\"NMI (dominant test label, labeled N={int((Y_test.sum(axis=1)>0).sum())}): {nmi:.6f}\\n\")\n",
    "            else:\n",
    "                f.write(\"NMI: N/A\\n\")\n",
    "            if ari is not None:\n",
    "                f.write(f\"ARI (dominant test label, labeled N={int((Y_test.sum(axis=1)>0).sum())}): {ari:.6f}\\n\")\n",
    "            else:\n",
    "                f.write(\"ARI: N/A\\n\")\n",
    "        else:\n",
    "            f.write(\"Label-based metrics: N/A (test_labels.npy not found)\\n\")\n",
    "\n",
    "    print(\"Saved metrics ->\", report_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-f\", \"--fff\", default=None, help=\"(ignored; Jupyter arg)\")\n",
    "    ap.add_argument(\"--out_dir\", type=str, default=\"./birdset_bridge_out_32k\",\n",
    "                    help=\"Dir from your previous run (must contain test5s_npy/ and optionally test_labels.npy)\")\n",
    "    ap.add_argument(\"--k_clusters\", type=int, default=20)\n",
    "    args, unknown = ap.parse_known_args()\n",
    "    if unknown:\n",
    "        print(\"Ignoring unknown args:\", unknown)\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
